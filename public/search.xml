<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ANSI-KMP算法</title>
    <url>/2024/08/28/ANSI-KMP/</url>
    <content><![CDATA[<p>在C语言程序设计这本书的第四章中提到了strindex 函数的编写;
其针对的问题是如何在主串(文本串)中找到某种类型的子串,
也就是我们常说的字符串匹配问题. 显然这个方法是存在朴素求解算法的,
在此我们将介绍除朴素求解算法以外的, 另一种优化的匹配算法, 即 KMP 算法和
KMP 算法的两种不同的实现思路.</p>
<span id="more"></span>
<h2 id="暴力搜索法-brute-force">暴力搜索法 (Brute Force)</h2>
<h3 id="算法思-想">算法思 想</h3>
<p>对于文本串 T 和模式串 P,
从文本串的第一个字符与模式串的第一个字符开始匹配; 如果相等,
那么比较位置同时后移, 比较第二个字符是否匹配, 以此类推;
直到出现模式串结束, 那么输出文本串的位置; 或者出现有一个不匹配的字符,
那么将文本串回退到最早比较的下一个字符, 模式串回退到开始, 重新循环.</p>
<h3 id="算法实现">算法实现</h3>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">strindex</span><span class="params">(<span class="type">char</span>* text,<span class="type">char</span>* pattern)</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="type">int</span> i,j,k;</span><br><span class="line">  <span class="keyword">for</span>(i=<span class="number">0</span>;s[i]!=<span class="string">&#x27;\0&#x27;</span>;i++)<span class="comment">//文本串的循环</span></span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">for</span>(k=i,j=<span class="number">0</span>;pattern[j]!=<span class="string">&#x27;\0&#x27;</span>&amp;&amp;pattern[j]==text[k];j++,k++)</span><br><span class="line">          <span class="comment">//模式串的循环</span></span><br><span class="line">	;</span><br><span class="line">    <span class="keyword">if</span>(j&gt;<span class="number">0</span>&amp;&amp;pattern[j]==<span class="string">&#x27;\0&#x27;</span>)<span class="comment">//判断是否为真正匹配结束</span></span><br><span class="line">      <span class="keyword">return</span> i;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="算法分析">算法分析</h3>
<p>我们假设文本串的长度为 <span class="math inline">\(n\)</span>,
模式串的长度为 <span class="math inline">\(m\)</span>.</p>
<p>最坏的情况是类似于文本串为AAAAAAAB, 模式串为AAAB,这样的话,
需要匹配<span class="math inline">\((n-m+1)m\)</span> 次, 时间复杂度为
<span class="math inline">\(O(nm)\)</span>.</p>
<p>最好的情况则是第一次匹配就成功, 那么时间复杂度就是 <span
class="math inline">\(O(m)\)</span>.</p>
<p>如果我们考虑每种情况以等概率的情况出现, 我们可知 BF
算法的平均时间复杂度为 <span class="math inline">\(O(nm)\)</span>.</p>
<h2 id="kmp-算法">KMP 算法</h2>
<p>我们在上面发现对于文本串为AAAAAAAB, 模式串为AAAB的匹配,
每次都是在最后一个字母匹配出错, 然后就要将模式串和文本串同时回退,
重新比较; 这一步会消耗大量的时间, 并且完全没有利用到前面匹配的结果;
因此KMP算法的提出就是为了解决上面提到的问题:
其利用了匹配失败以前的匹配成功的结果, 从而将时间复杂度降到了 <span
class="math inline">\(O(n+m)\)</span>.</p>
<h3 id="kmp-算法的基本思想">KMP 算法的基本思想</h3>
<p>其核心思想就是当出现不匹配的情况,
利用已知匹配的结果避免回退文本串的指针并且尽可能少的回退模式串的指针.</p>
<p>例如, 如果我们匹配到第 j 个字符串出现了字符不匹配现象,
那么我们已知的是 P[0:j-1]=T[i:i+j-1], BF
算法对这个的处理是直接回退文本指针和模式指针, 并不利用这一良好性质. 而
KMP 算法则是需要寻找 P[0:j-1] 的最长相等前后缀,因为已知
P[0:j-1]=T[i:i+j-1]; 假设 P[0:j-1] 的最长相等前后缀长度为
k,那么你就可以通过一个连等式快速得到需要回退的位置, 即
P[0:k-1]=P[j-k:j-1]=T[j-k:j-1]; 如此可知实际上,
即使我们用暴力搜索重新去计算, 只不过是不断地失败,
直到达到上面所说的情况而已.</p>
<h3 id="基于-pmt-的-kmp-算法">基于 PMT 的 KMP 算法</h3>
<p>上面介绍了 KMP 算法的基本思想, 接下来我们就用不同的实现思路去实现 KMP
算法的思想. 首先我们引入 PMT (Partial Match Table, 部分匹配表) 的概念,
其实这个表就是将要介绍的前缀表.</p>
<h4 id="前缀表的定义">前缀表的定义</h4>
<p>前缀表在 KMP 算法中的作用是用来控制模式指针的回退,
其记录的是字符串的最长相等前后缀的长度,
其用于寻找当出现模式串与文本串不匹配的时候,
模式串应该从哪里开始重新匹配.</p>
<p>例如对于文本串 T=aabaacaabaaf, 模式串为 P=aabaaf;
在第一次字符串匹配时, P[5]和T[5]不匹配了, 如果用 BF 算法,
那么我们下一次匹配就是 P[0] 和 T[1] 的比较; 但显然我们通过朴素的思考,
因为 P[0:4]=aabaa 的最长相同前后缀为 aa, 所以利用 KMP
算法的下一次匹配实际上是考虑 P[2] 和 T[5] 的匹配, 显然如此,
匹配速度有很大的提升.</p>
<p>前缀表是记录下标 i 之前 (包括 i)
的字符串中的最长相同前后缀的长度.</p>
<p>对于一个字符串的前缀指的是包含首字符, 但不包含尾字符的字符串; 例如
aabaaf 的前缀集合为 {a,aa,aab,aaba,aabaa}</p>
<p>对于一个字符串的后缀指的是包含尾字符, 但不包含首字符的字符串; 例如
aabaaf的后缀集合为 {f,af,aaf,baaf,abaaf}</p>
<h4 id="前缀表的计算思路及代码实现">前缀表的计算思路及代码实现</h4>
<ol type="1">
<li>引入两个指针 i 和 j; j 用来指示前缀的末尾, i 用来指示后缀的末尾;
换言之, i 会从 0 开始遍历模式串的每一个下标, j 则是用于记录 P[0:i-1]
的最长相等前后缀的长度.</li>
<li>如果模式串的第 i 位和模式串的第 j 位相等, 那么 j++, i++即可</li>
<li>如果模式串的第 i 位和模式串的第 j 位不相等, 那么就要做一个持续回退;
因为已知 P[j]!=P[i], 但是 P[0:j-1]=P[i-j:i-1]; 那么我们可以用 j=pmt[j-1]
来回退到 P[0:j-1] 的最长相等前后缀的后一位; 这一步的依据是首先
P[0:j-1]=P[i-j:i-1], 并且我们假设 k 为 P[0:j-1] 的最长相等前后缀长度,
那么 P[0:k-1]=P[j-k:j-1]=P[i-k:i-1], 如此就可以去比较 P[k] 和 P[i].</li>
<li>更新 pmt 数组</li>
</ol>
<p>代码实现如下:</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">generate_PMT</span><span class="params">(<span class="type">const</span> <span class="built_in">string</span> &amp;pattern,<span class="type">int</span>* pmt)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> j=<span class="number">0</span>;</span><br><span class="line">    pmt[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;pattern.size();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(j&gt;<span class="number">0</span>&amp;&amp;pattern[i]!=pattern[j])</span><br><span class="line">            j=pmt[j<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">if</span> (pattern[i]==pattern[j])</span><br><span class="line">            j++;</span><br><span class="line">        pmt[i]=j;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>利用前缀表进行 KMP 算法匹配思路</p>
<ol type="1">
<li>设置两个指针 i 和 j; i 用来控制在文本串的移动; j
用来控制在模式串的移动</li>
<li>如果匹配没出错, 那么返回匹配的起点即可</li>
<li>如果匹配出错了, 那么分情况讨论,如果 j=0,
即模式串的第一个字符就与其不匹配, 那么就需要将文本串向后移动一位; 如果 j
不为0, 那么就用 j=pmt[j-1] 去更新匹配成功部分的最长相等前后缀.</li>
<li>输出结果</li>
</ol>
<h4 id="利用前缀表进行-kmp-算法代码实现">利用前缀表进行 KMP
算法代码实现</h4>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">generate_PMT</span><span class="params">(<span class="type">const</span> string &amp;pattern,<span class="type">int</span>* pmt)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> j=<span class="number">0</span>;</span><br><span class="line">    pmt[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>;i&lt;pattern.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(j&gt;<span class="number">0</span>&amp;&amp;pattern[i]!=pattern[j])</span><br><span class="line">            j=pmt[j<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">if</span> (pattern[i]==pattern[j])</span><br><span class="line">            j++;</span><br><span class="line">        pmt[i]=j;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">strindex</span><span class="params">(string text,string pattern)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> j=<span class="number">0</span>;</span><br><span class="line">  	<span class="keyword">if</span> (text.<span class="built_in">size</span>()==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">if</span> (pattern.<span class="built_in">size</span>()==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">pmt</span><span class="params">(pattern.size())</span></span>;</span><br><span class="line">    <span class="built_in">generate_PMT</span>(pattern,&amp;pmt[<span class="number">0</span>]);<span class="comment">//&amp;取地址</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;text.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(j&gt;<span class="number">0</span>&amp;&amp;text[i]!=pattern[j])</span><br><span class="line">            j=pmt[j<span class="number">-1</span>];</span><br><span class="line">        <span class="keyword">if</span>(text[i]==pattern[j])</span><br><span class="line">            j++;</span><br><span class="line">        <span class="keyword">if</span>(j==pattern.<span class="built_in">size</span>())</span><br><span class="line">            <span class="keyword">return</span> (i-pattern.<span class="built_in">size</span>()+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="用-next-数组实现-kmp-算法">用 next 数组实现 KMP 算法</h4>
<p>我们在此额外介绍前缀表的另一种表示形式, 也就是 next 数组,
在市面上很多算法书更多采用这种. 我们只介绍其中一种 next 数组生成方法,
也就是将前缀表整体右移以后, 在第一个位置补上 -1. 我们给出代码实现</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">generate_NEXT</span><span class="params">(<span class="type">const</span> string&amp; pattern, <span class="type">int</span>* next)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> j = <span class="number">-1</span>;</span><br><span class="line">    next[<span class="number">0</span>] = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; pattern.<span class="built_in">size</span>(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(j&gt;=<span class="number">0</span>&amp;&amp;pattern[i<span class="number">-1</span>]!=pattern[j])</span><br><span class="line">			j=next[j];</span><br><span class="line">        <span class="keyword">if</span> (j == <span class="number">-1</span> || pattern[i<span class="number">-1</span>] == pattern[j])</span><br><span class="line">            next[i] = ++j;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">strindex</span><span class="params">(string text,string pattern)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> j=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (text.<span class="built_in">size</span>()==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">if</span> (pattern.<span class="built_in">size</span>()==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">next</span><span class="params">(pattern.size())</span></span>;</span><br><span class="line">    <span class="built_in">generate_NEXT</span>(pattern,&amp;next[<span class="number">0</span>]);<span class="comment">//&amp;取地址</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;text.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(j&gt;<span class="number">0</span>&amp;&amp;text[i]!=pattern[j])</span><br><span class="line">            j=next[j];</span><br><span class="line">        <span class="keyword">if</span>(j==<span class="number">-1</span>||text[i]==pattern[j])</span><br><span class="line">            j++;</span><br><span class="line">        <span class="keyword">if</span>(j==pattern.<span class="built_in">size</span>())</span><br><span class="line">            <span class="keyword">return</span> (i-pattern.<span class="built_in">size</span>()+<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="基于-dfa-的-kmp-算法">基于 DFA 的 KMP 算法</h3>
<h4 id="dfa-的定义">DFA 的定义</h4>
<p>自动机分为两类: DFA (确定性有限状态自动机) 和 NFA
(非确定性有限状态自动机), 二者均可用于字符串匹配问题, 我们在此仅使用 DFA
来完成 KMP 算法. 而 NFA 主要用于正则表达式的匹配问题.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/LYD122504/picx-images-hosting@master/微信图片_20240829160304.6png3qwz50.png"
alt="微信图片_20240829160304" />
<figcaption aria-hidden="true">微信图片_20240829160304</figcaption>
</figure>
<p>由上图知, DFA 由状态 (带数字的圆圈) 和转换 (带模式串字母的箭头) 组成.
从上图知, 由于模式串为 ABABAC 六个字符, 因此 DFA 一共由七个状态,
其中状态 0 表示起始状态, 也就是未开始匹配的状态, 模式 6
为匹配成功的状态. 值得注意的是, 构建的 DFA 是基于模式串进行的,
文本串是一系列输入该 DFA 的参数; 从某种角度来说, 这与我们前面提到的 next
数组类似, 实际上这两个只是不同维度的同一内容而已.</p>
<p>进一步, 我们给出图中涉及的一些专业术语, 在字符串匹配问题上,
每个状态都表示模式串中各个字符串的索引值,
该图中给出了在每个状态下输入不同字符的状态的转换操作,
但实际上这些转换中只有 0-1-2-3-4-5-6 这一条是匹配转换(其实就是从第 i
个状态转换到第 i+1 个状态), 其余的转换操作都需要回退指针,
称之为非匹配转换.</p>
<h4 id="构建-dfa-的思路与算法实现">构建 DFA 的思路与算法实现</h4>
<p>在计算机内部, 我们用二维数组 DFA[char][state] 来表示上面介绍的 DFA,
其中 DFA[i][j] 具体含义是在状态 j 的情况下输入字符 i 之后,
状态转换的结果.</p>
<p>从构建DFA的流程中, 我们知道 DFA 的状态转换只有两种类型:
匹配转换或者非匹配转换.对此分类讨论即可得到构建 DFA 的算法</p>
<p>如果状态转换是匹配转换, 也就是在第 j 个状态下输入的字符是 pattern[j],
那么匹配成功, 我们可以转换到第 j+1 个状态, DFA[pattern[j]][j]=j+1;</p>
<p>假如状态转换是非匹配转换, 也就是在第 j 个状态下输入的字符并不是
pattern[j]. 为了后续讨论简便, 我们用 i 来表示输入的文本串字符位置, 但是
text[i] 不等于 pattern[j]. 此时, 因为出现了匹配失败的情况,
我们需要回退状态, 但是我们尽量要避免和暴力算法一样直接回退到开始,
而是尽可能少的回退, 并且不排除任何可能情况, 这就是 KMP 算法本身的思想.
虽然我们在第 j 个状态下匹配失败, 但是我们已知 Pattern[0~j-1] 和
text[i-j~i-1] 已经完成了匹配. 所以无论如何从 text[i-j]
开始去寻找模式串已经是不可能成立的了, 所以此时我们就可以从
text[i-j+1]开始分析, 如果我们输入的新字符串是 text[i-j+1~i-1],
那么会出现在什么状态? 由于这个新字符串依赖的不仅仅是文本串的内容,
实际上由于 Pattern[0~j-1]=text[i-j~i-1],
我们可以认为输入的新字符串实际上是 Pattern[1~j-1].
所以我们就得到了一系列等价表达, 即如果在第 j 个状态下输入的 text[i]
出现了非匹配转换, 其最终状态等价于输入text[i-j+1~i-1]以后再输入 text[i]
的状态,等价于输入 Pattern[1~j-1] 以后再输入 text[i] 的状态.</p>
<p>此时会发现如果我们要知道第 j 个状态下的非匹配转换结果,
其实我们只需要知道 输入 Pattern[1~j-1] 之后的状态; 由于我们的 DFA
构造是递推的, 所以这个状态的转换结果是已经构造了的,
我们可以直接得到这个结果, 换言之其实是一个 DP 过程. 为了讲解方便,
我们引入一个重启状态的概念, 重启状态就是如果我们在第 j
个状态下出现了非匹配转换, 那么由我们前面的讨论得知, 输入 Pattern[1~j-1]
得到的状态就是第 j 个状态的重启状态, 第 j
个状态的非匹配转换和重启状态的对应字符转换状态一致.</p>
<p>我们给出如下的例子来解释重启状态, 在上面的 DFA 的图中, 如果我们用
ABABAA 来输入这个 DFA 中, 在状态 5 中, 最后输入的字符是 A, 与 C 不匹配,
那么我们就需要去找这个状态的重启装态, 我们已知 Pattern[0~4]=ABABA,
寻找重启状态的话, 我们就需要输入 Pattern[1~4]=BABA, 通过查图,
我们可以发现状态 5 的非匹配转换和状态 3 的一致.</p>
<p>为了解释方便, 我们用重启状态数组来加以解释,
虽然我们在代码中是以数的形式存在, 但这并不影响. X[j+1] 表示输入
Pattern[1~j] 之后再输入 Pattern[j+1] 得到的结果, 但是我们知道输入
Pattern[1~j] 得到的状态其实是 X[j], 所以我们要求的 X[j+1] 实际上是在状态
X[j] 的基础上输入 Pattern[j+1] 的状态, 也就是
X[j+1]=DFA[Pattern[j]][X[j]]. 我们初始化 X[0]=0.</p>
<p>下面给出构建 DFA 的代码,</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">generate_DFA</span><span class="params">(<span class="type">const</span> string&amp; pattern, vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; &amp;DFA)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    DFA[pattern[<span class="number">0</span>]][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> X;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>, X = <span class="number">0</span>; j &lt; pattern.<span class="built_in">size</span>(); j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; i++)<span class="comment">// 256是asc码的长度,需要将字符串内出现的所有可能字符都列入</span></span><br><span class="line">            DFA[i][j] = DFA[i][X];</span><br><span class="line">        DFA[pattern[j]][j] = j + <span class="number">1</span>;</span><br><span class="line">        X = DFA[pattern[j]][X];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其实, 在 DFA 中寻找重启状态, 我们用的是 Pattern[1~j-1] 重新输入,
这个其实就是我们在 PMT 中寻找最长相等前后缀的过程, 只是 DFA
将其可视化了而已, 或者说, 我们在前面构造的重启状态数组实际上就是 PMT.
因此这两个办法理论上没有任何区别, 不过 DFA 在空间存储方面消耗更大,
他是一个二维数组.</p>
<h4 id="用-dfa-实现-kmp-的算法实现">用 DFA 实现 KMP 的算法实现</h4>
<p>如果我们已经构造了 DFA 数组, 我们只需要存储对应状态和依次输入文本串到
DFA 即可. 值得注意的是, 循环结束的原因是</p>
<ol type="1">
<li>达到匹配成功状态, 返回文本串匹配成功起点</li>
<li>文本串达到末尾, 也就是匹配失败, 返回 -1</li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">generate_DFA</span><span class="params">(<span class="type">const</span> string&amp; pattern, vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; &amp;DFA)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    DFA[pattern[<span class="number">0</span>]][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> X;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>, X = <span class="number">0</span>; j &lt; pattern.<span class="built_in">size</span>(); j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">256</span>; i++)</span><br><span class="line">            DFA[i][j] = DFA[i][X];</span><br><span class="line">        DFA[pattern[j]][j] = j + <span class="number">1</span>;</span><br><span class="line">        X = DFA[pattern[j]][X];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">strindex</span><span class="params">(string text, string pattern)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> R = <span class="number">256</span>;</span><br><span class="line">    vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">DFA</span>(R, <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt;(pattern.<span class="built_in">size</span>(), <span class="number">0</span>));</span><br><span class="line">    <span class="built_in">generate_DFA</span>(pattern, DFA);</span><br><span class="line">    <span class="type">int</span> i, j;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>, j = <span class="number">0</span>; i &lt; text.<span class="built_in">size</span>() &amp;&amp; j &lt; pattern.<span class="built_in">size</span>(); i++)</span><br><span class="line">    &#123;</span><br><span class="line">        j = DFA[text[i]][j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (j == pattern.<span class="built_in">size</span>())</span><br><span class="line">        <span class="keyword">return</span> i - pattern.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="kmp-算法分析">KMP 算法分析</h3>
<p>如果模式串长度为 <span class="math inline">\(m\)</span>, 文本串长度为
<span class="math inline">\(n\)</span>.</p>
<p>空间复杂度显然是 <span class="math inline">\(O(m)\)</span></p>
<p>对于时间复杂度分析, 我们只考虑 PMT 的情况, DFA 完全类似. 由于 PMT
需要求解 PMT 数组/Next 数组和 KMP 搜索两部分代码, 所以我们逐步分析.</p>
<ol type="1">
<li>求解 PMT 数组/ Next 数组; 外层的 for 循环需要迭代 <span
class="math inline">\(m-1\)</span> 次, 但是里面嵌套了一个 while 循环,
如果我们试图直接寻找 while 的循环次数, 虽然可以得到结果,
但是显然不太现实. 我们利用 j 的值的变化来近似分析时间复杂度. j
的变化只有两种情况, 要么加一, 要么 j=pmt[j-1] 减少.
但是减少的程度其实依赖于 j++ 的程度, 而 j++ 和 i++ 都是同时发生的,
所以最多发生 <span class="math inline">\(m-1\)</span> 次,
因此这一步的时间复杂度是 <span class="math inline">\(O(m)\)</span></li>
<li>搜索: for 循环走的次数是 <span class="math inline">\(n-1\)</span>,
while 的回退其实总体来说最多只可能走 <span
class="math inline">\(n-m\)</span> 因此一共复杂度 <span
class="math inline">\(O(n+m)\)</span></li>
</ol>
]]></content>
      <categories>
        <category>C Language</category>
      </categories>
      <tags>
        <tag>-Computer Science -C Language</tag>
      </tags>
  </entry>
  <entry>
    <title>Lecture series on graph theory 1</title>
    <url>/2022/03/28/Graph1/</url>
    <content><![CDATA[<p>Def. A graph is a <font color=red>discrete structure/combinational
structure</font> consisting of vertices and edges which connect to a
pair of vertices.</p>
<p>Usually, a vertex is represented by a dot and an edge is represented
by a line (straight or curved) joining two vertices.</p>
<span id="more"></span>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-02-26_212559.jpg" /></p>
<center>
<font color=red size=4>The green points mean crossing of the
edges,rather than vertices  </font>
</center>
<p>A vertex is incident with an edge if the edge join the vertex to
another vertex, which is also called the end of the edge.</p>
<p>Two vertices are adjacent to each other if there is an edge joinging
them.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-02-26_214218.jpg" /></p>
<center>
<font color=red size=4>We say that the vertex v is incident with the
edge e<br>The vertex u is adjacent to the vertex v because uv is an edge
of G</font>
</center>
<p>A graph is simple if there is at most one edge between any two
vertices and there is no edge joining a vertex itself.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-28_185112.jpg" /></p>
<p>In short, a graph is simple if and only if it has no multi-edges and
no loops.Next, unless otherwise specified, we will discuss simple graphs
by default.</p>
<p>For any given simple graph <span
class="math inline">\(G(V,E)\)</span>，V is the vertex set and E is the
edge set. <span class="math display">\[
if\ \lvert V\rvert=n,\lvert E\rvert\leq\binom{n}{2}
\]</span> A graph is complete if the graph has an edge between every
pair of vertices.A complete graph of n vertices is usually denoted by
<span class="math inline">\(K_n\)</span></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-28_185902.jpg" /></p>
<p>The degree of a vertex v of G, denoted by <span
class="math inline">\(d_G(v)\)</span>, is the number of edges incident
with it.</p>
<p>The maximum degree of G is defined as <span
class="math inline">\(\Delta(G):=\max\{d_G(x)|v\in V(G)\}\)</span></p>
<p>The minimum degree of G is defined as <span
class="math inline">\(\delta G:=\min\{d_G(v)|v\in V(G)\}\)</span></p>
<p>The average degree of G is defined as <span
class="math inline">\(d(G):=\frac{\lvert E(G)\rvert}{\lvert
V(G)\rvert}\)</span>, this index is used to measure the density of the
graph.According to this index, graph can be divided into dense graph and
sparse graph</p>
<p>A neighbor of v in G is a vertex joined to v by an edge of G.The
neighborhood of a vertex v in G is the set of all neighbors of v in G,
denoted by <span class="math inline">\(N_G(v)=\{u|u\in V(G),uv\in
E(G)\}\)</span>.We have <span class="math inline">\(d_G(v)=\lvert
N_G(v)\rvert\)</span> easily.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-28_185933.jpg" /></p>
<p><font color=red>This picture shows the multi-level neighbors of v. If
u is both a second-level neighbor and a third-level neighbor of v, it is
classified as a second-level neighbor. <br>Here we give the set
representation of neighbors: <br><span
class="math inline">\(N_1(v)=N_G(v),N_2(v)=N_G(N_1(v))-N_1(v)\cup\{v\},\
N_3(v)=N_G(N_2(v))-N_1(v)\)</span><br>and if <span
class="math inline">\(w\in N_3(v),\ distance(w,v)=3\)</span></font></p>
<p>A graph is even if every vertex of G has an even degree.</p>
<p><img src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-28_185942.jpg" style="zoom:50%;" /></p>
<p><strong>Proposition(Handshaking Lemma)<br>Every graph has an even
number of vertices with odd degree.</strong></p>
<p>Proof.</p>
<p>Let <span class="math display">\[
\left\{\begin{aligned}X&amp;=\{x|x\in V(G)\ and\
d_G(x)\equiv0(mod\ 2)\}\\Y&amp;=\{y|y\in V(G)\ and\
d_G(y)\equiv1(mod\ 2)\}\end{aligned}\right.
\]</span> Then,<span class="math inline">\(\sum_{x\in
X}d_G(x)+\sum_{y\in Y}d_G(y)=2\lvert E(G)\rvert\)</span></p>
<p>So, <span class="math inline">\(\sum_{y\in Y}d_G(y)\equiv0(mod\
2)\)</span>.It follows immediately that <span
class="math inline">\(\lvert Y\rvert\equiv0(mod\ 2)\)</span></p>
<p>So the proposition holds.</p>
<p>Question:Let G be an n-vertex graph.How many edges will force G to
have a triangle?(Mantel's Theorem)</p>
<p>Def A walk of G is <font color=red>a sequence of vertices and edges
of G</font>such that (i)both the first element and the last element of
the sequence are vertices and (ii) any two consecutive elements in the
sequence contains one vertex and one edge which are incident with each
other.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-28_200038.jpg" /></p>
<p><font color=red>We give two different walks: <span
class="math inline">\(w_{1}=v_1e_1v_2e_2v_3e_2v_2e_1v_1,\ 
w_2=v_2e_2v_3e_3v_4e_4v_2\)</span><br>For walk,vertex can be repeated,
edge can be repeated.</font></p>
<p>A walk is open if the first vertex is not equal to the last vertex.
<font color=red><span
class="math inline">\(w_0=v_2e_2v_3e_3v_4e_4v_2e_1v_1\)</span></font></p>
<p>A walk is a trail if it does not have repeated edges.
<font color=red><span class="math inline">\(w_0\)</span> is a
trail</font></p>
<p>A trail is a path if it does not have repeated vertices.</p>
<p><font color=red><span
class="math inline">\(w_3=v_1e_1v_2e_2v_3e_3v_4=v_1v_2v_3v_4\)</span> is
a path</font></p>
<p>A cycle is a closed trail( or path) without repeated vertices
(vertices and edges) <font color =red>except the first vertex and the
last vertex. <span
class="math inline">\(w_4=v_2e_2v_3e_3v_4e_4v_2\)</span></font></p>
<p>A graph is connected if for any two vertices x and y, there is a path
joining x and y.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-28_200053.75zqcj50zpw0.jpg" /></p>
<p>A subgraph H of G is a graph with <span
class="math inline">\(V(H)\subset V(G),E(H)\subset E(G)\)</span></p>
<p>A maximal connected subgraph of G is called a connected component.
<font color=red>Maximal means anything bigger than it is not
connected</font></p>
<p>An edge e of G is bridge or cut-edge if #connected components of
G-e&gt;#connected components of G</p>
<p><img src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-28_200107.7d9xw8946f80.jpg" style="zoom:67%;" /></p>
<p><font color=red>#connected components of G-<span
class="math inline">\(e_1\)</span>=3&gt;2=#connected components of
G<br>Both <span class="math inline">\(e_1\)</span> and<span
class="math inline">\(e_2\)</span> are bridges/cut-edges</font></p>
<p><strong>Thm(BJJ,Fan)<br>Every graph without bridges has a family of
cycles which covers every edge exactly 2k times for any integer <span
class="math inline">\(k\ge2\)</span></strong></p>
<p>A subgraph H is spanning if <span
class="math inline">\(V(H)=V(G)\)</span></p>
<p>A subgraph H is induced by <font color=red> <span
class="math inline">\(S\subset V(G)\)</span></font>, if <span
class="math inline">\(V(H)=S\)</span> and for any two vertices <span
class="math inline">\(x,y\in S\)</span>, <span
class="math inline">\(xy\in E(H)\Leftrightarrow xy\in E(G)\)</span></p>
<p>A subgraph H is induced by <font color=red> <span
class="math inline">\(S\subset E(G)\)</span></font>, if <span
class="math inline">\(E(H)=S\)</span> and <span
class="math inline">\(v\in V(H)\)</span>a if and only if v is incident
with an edge in S.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-28_213047.4nctxwr5lzc0.jpg" /></p>
<p>Let <span class="math inline">\(x,y\in V(G)\)</span>, the distance
between x and y in G is the length of the shortest path joining x and y,
denoted by <span class="math inline">\(dist_G(x,y)\)</span>. In other
words, <span class="math display">\[
dist_G(x,y)=\min\{\lvert E(P)\rvert\big|P\ is\ a\ path\ joining\ x\ and\
y\}
\]</span>
<img src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-28_215149.3o033gismx60.jpg" style="zoom:67%;" /></p>
<p><font color=red><span class="math inline">\(dist_G(x,y)=k\)</span>,
this means finding a shortest path between any two vertices x and y is
easy</font></p>
<p><font color=red>However, finding a longest path between any two
vertices x and y.Given an integer k, is there an (x,y)-path with length
k?They are NPC problems</font></p>
]]></content>
      <categories>
        <category>Graph Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Operation Research</tag>
      </tags>
  </entry>
  <entry>
    <title>ANSI-Extern, Const和Static 的用法</title>
    <url>/2024/09/11/ANSI-extern/</url>
    <content><![CDATA[<h3 id="extern-的用法">Extern 的用法</h3>
<p>在 C 语言中, 修饰符 extern 用在变量或者函数声明之前,
用来标识变量或者函数的定义在别的文件中已经给出,
告知编译器运行到此变量或函数时, 在其他位置寻找其定义.
其常见的用法如下:</p>
<span id="more"></span>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="type">int</span> a;</span><br><span class="line"><span class="keyword">extern</span> <span class="type">int</span> b;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">max</span><span class="params">(<span class="type">int</span> l,<span class="type">int</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> l &gt; r ? l : r;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">max</span>(a, b) &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> a = <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> b = <span class="number">20</span>;</span><br></pre></td></tr></table></figure>
<p>此处如果不在开头加上 extern int a,那么main函数是无法访问变量 a,b 的,
即使变量 a,b 是一个外部变量, 但其作用域是定义以后至程序结束, 而修饰符
extern 引导的变量声明延拓了该外部变量的存在域. 实际上, extern
修饰符主要还是在多源文件的情况下使用.</p>
<h4 id="extern-修饰变量的声明">Extern 修饰变量的声明</h4>
<p>　extern关键字可以用来修饰变量，表示该变量在别的文件中已有声明。例如：我们在文件file1.c中声明了变量int
var，然后我们又需要再file2.c中使用该变量，则可在文件file2.c中声明extern
int var，就可在文件file2.c中使用该变量了。</p>
<p>值得强调的是 extern 修饰的是变量的声明,
变量的声明仅仅向编译器传递了变量的信息, 但并不会分配内存;
分配内存是通过变量定义来实现的. 因此我们如果使用 extern 修饰变量声明,
是不可以同时在其后初始化, 因为声明并不会对变量, 对内存操作的. 其次, 使用
extern 修饰声明的变量一定要是全局变量, 这是因为如果声明的是局部变量,
那么他的存活域只在某个函数体内部,
在其他文件中引用这个变量也是没有意义的.</p>
<p>进一步, 我们可以讨论由 extern 修饰的变量声明对应的存活域. 首先,
由前面的介绍知道, extern 声明的变量只能是全局变量, 但是如果 extern
修饰的变量声明位于某个函数体内, 那么其存活域也只在这个函数内部,
与在函数内部设置的局部变量一致.</p>
<h4 id="extern-修饰函数声明">Extern 修饰函数声明</h4>
<p>从本质来看, extern 修饰函数声明与修饰变量声明是等价的.
但我们更为常用的引用其他文件的函数是通过头文件的方式来引用的.
在程序实现层面, 这两种方式是有所不同的,
引用头文件的编译处理是一种预处理方式, 而extern
修饰的声明则是代码链接层面的实现. 因此一般而言,
如果需要大批量引用函数声明, 那么就用头文件更为便利,
而如果只是引用少数函数, 那么用 extern 修饰声明即可.</p>
<h4 id="extern-用来实现链接指定">Extern 用来实现链接指定</h4>
<p>extern用来进行链接指定一般来说是用来实现混合编程.
例如我们如果要在C++程序中调用C代码, 那么我们就需要用extern
"C"来声明.</p>
<p>extern "C" 可用于单一语句</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">extern</span> “C” <span class="function"><span class="type">double</span> <span class="title">sqrt</span><span class="params">(<span class="type">double</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<p>也可以是复合语句, 相当于复合语句中的声明都加了 extern “C”</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">extern</span> “C”</span><br><span class="line">&#123;</span><br><span class="line">      <span class="function"><span class="type">double</span> <span class="title">sqrt</span><span class="params">(<span class="type">double</span>)</span></span>;</span><br><span class="line">      <span class="function"><span class="type">int</span> <span class="title">min</span><span class="params">(<span class="type">int</span>, <span class="type">int</span>)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样可以包含头文件，相当于头文件中的声明都加了extern “C”
(不建议这样写，会有嵌套可能)</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">extern</span> “C”</span><br><span class="line">&#123;</span><br><span class="line">      ＃include &lt;cmath&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>值得注意的是, extern "C" 是不可以用在函数内部的,
且如果函数有多个声明, 可以全都加上 extern "C", 也可以只加一个,
后续声明会遵循第一个链接指示符的规则.</p>
<h3 id="static-的用法">Static 的用法</h3>
<p>static 的用法其实只有三个: 隐藏, 保持变量持久, 默认初始化为0</p>
<h4 id="static-的主要功能隐藏">Static 的主要功能:隐藏</h4>
<p>当我们同时编译多个源文件时, 普通的全局变量和函数 (即未加 static 的)
都是全局可访问的.</p>
<p>例如, 对于多源文件main.cpp和test.cpp如下所示</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//main source</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">extern</span> <span class="type">char</span> a;    <span class="comment">// extern variable must be declared before use</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%c &quot;</span>, a);</span><br><span class="line">    <span class="function"><span class="keyword">extern</span> <span class="type">void</span> <span class="title">msg</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="built_in">msg</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>test.cpp结构如下</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="type">char</span> a = <span class="string">&#x27;A&#x27;</span>; <span class="comment">// global variable</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">msg</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello\n&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述可以运行是因为在test.cpp文件中的全局变量并没有加static,因此其余文件可以随意的利用修饰符extern来调用全局变量和函数.
如果加上static, 那么他就会对其余的源文件隐藏, 只能在本文件中使用.
因此利用这一特性我们可以在不同的源文件定义同名变量与同名函数,
不会出现命名冲突, 也不会出现数据混用的情况. static
可用于修饰函数也可以用来修饰变量; 如果针对函数, 那么 static
的作用仅限于隐藏; 而对于变量, 那么 static 就有接下来的两个作用.</p>
<h4 id="保持变量内容的持久">保持变量内容的持久</h4>
<p>与自动变量存储在栈区不同, static变量会存储在静态存储区.
虽然全局变量也会存储在静态存储区, 但是与全局变量相比,
static变量主要的特性是对变量的隐藏.</p>
<p>如果static用于声明内部变量,
那么static类型的内部变量和函数内部的自动变量一样,只能在函数内部使用,
但不同的一点是, 函数内部的自动变量会在函数运行结束自动释放内存,
下一次运行需要重新声明, 重新分配内存; 而static类型变量则不会,
她只在第一次声明时分配内存且完成初始化, 不会随着函数的返回而释放空间,
一直等到重新调用函数时,其值为上次结束时的值.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">fun</span><span class="params">(<span class="type">void</span>)</span></span>&#123;</span><br><span class="line">    <span class="type">static</span> <span class="type">int</span> count = <span class="number">10</span>;    <span class="comment">// 首次运行时,会将静态变量完成初始化,而后不会再运行,下一次的值是由上次存储的数据来保证.</span></span><br><span class="line">    <span class="keyword">return</span> count--;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> count = <span class="number">1</span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;    </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;global\t\tlocal static\n&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span>(; count &lt;= <span class="number">10</span>; ++count)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d\t\t%d\n&quot;</span>, count, <span class="built_in">fun</span>());    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="默认初始化为-0">默认初始化为 0</h4>
<p>实际上, 全局变量也有这个特性,
这是因为全局变量和静态变量都存储在静态存储区, 静态存储区的字节默认为
0x00. 善用这个特性, 可以极大简便程序的编写复杂度,
如果我们考虑一个稀疏矩阵, 如果我们设置局部变量,
那么我们需要先将变量全部置为0, 而后一个个添加非零元素;
实际上我们可以直接用静态变量, 这样的话, 我们只需要添加非零元素即可.
这同样对字符串有相同的操作.</p>
<h3 id="const-的用法">Const 的用法</h3>
<p>const 用来修饰一个变量, 那么他的值从某种角度来说就是不能被改变的.
这其实很类似与C语言中的#define的宏命令, 这两个之间还是有较大的差异:</p>
<ol type="1">
<li>#define 是预处理指令, 其在预处理阶段就会完成文本替换.
而const则是对变量的修饰,因此他需要在后续的语法编译的时期,
程序还要检查其类型是否正确, 对比更加安全.</li>
<li>const 可以保护被修饰的内容, 防止意外修改,
从而来增强程序的鲁棒性.</li>
<li>const 常量是不通过编译器分配内存空间的, 而是将其纳入符号表;
这样使得他在运行过程中不需要存储和读取内存的时间, 提高了运行效率.</li>
</ol>
<h4 id="修饰局部变量">修饰局部变量</h4>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> a=<span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> <span class="type">const</span> a=<span class="number">1</span>;</span><br></pre></td></tr></table></figure>
<p>这两个语句表示的是同一条命令, 即定义整型const常量a并初始化为1.
由于const常量后续将不再允许被修改,
因此我们在定义const常量的同时一定要对其进行初始化.
其实对于这种基本类型的const修饰十分简单,
我们考虑const字符串,可以知道他的一些优势.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">char</span>* ptr=<span class="string">&quot;Hello world!&quot;</span>;</span><br></pre></td></tr></table></figure>
<p>如果这里我们不加上const限定符, 那在程序中可能出现 ptr[3]='L' 的命令,
他会因为对只读区域的写入而报错, 也就是在程序运行时报错; 而如果我们用
const 限制符, 那么在程序编译期间, 就会因为试图修改const常量而报错.</p>
<h4 id="常量指针和指针常量">常量指针和指针常量</h4>
<p>值得注意的是 const 的使用在此处将会十分灵活, 因为他涉及到指针的内容,
因此比较复杂.</p>
<p>首先, 我们先考虑常量指针, 其声明方式如下所示,</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span>* ptr;</span><br><span class="line"><span class="type">int</span> <span class="type">const</span> * ptr;</span><br></pre></td></tr></table></figure>
<p>常量指针的含义为指针指向的内容是常量.
不可以通过常量指针来修改对应的值. 需要注意的是,</p>
<ol type="1">
<li><p>这里的常量指针不能修改对应的值,只是不能通过常量指针修改;
但允许通过其他的引用来修改, 例如</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> a=<span class="number">10</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span>* pn=&amp;a;</span><br><span class="line">*pn=<span class="number">10</span>; <span class="comment">//报错,因为pn是常量指针</span></span><br><span class="line">a=<span class="number">10</span>; <span class="comment">//允许运行, 因为这是其他的引用并没有常量的限制</span></span><br></pre></td></tr></table></figure></li>
<li><p>常量指针指向的值不能修改, 但是可以修改常量指针指向的对象,
也就是可以将常量指针指向不同的位置.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> x=<span class="number">5</span>;</span><br><span class="line"><span class="type">int</span> y=<span class="number">6</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span>* pn=&amp;x;</span><br><span class="line">pn=&amp;y;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>其次, 我们讨论指针常量, 其定义声明如下所示</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span>* <span class="type">const</span> pn;</span><br></pre></td></tr></table></figure>
<p>指针常量指的是这个指针本身就是一个常量,
我们在运行过程中不可以重新给该指针常量重新赋值,
但是我们可以用这个指针修改他指向的内容. 例如</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> a = <span class="number">5</span>;</span><br><span class="line"><span class="type">int</span>* <span class="type">const</span> n = &amp;a;</span><br><span class="line">*n = <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<p>这两种const常量的定义的区别仅在于 const 限定符与 * 的位置关系,
如果const在* 左边, 那么定义的就是常量指针; 如果const在* 右边,
那么定义的就是指针常量. 特殊的实际上, 我们可以将上面的两种定义合并,
得到指向常量的常指针的定义</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span>* <span class="type">const</span> a=<span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<p>指针指向的位置不能改变并且也不能通过这个指针改变变量的值，但是依然可以通过其他的普通指针改变变量的值.</p>
<h4 id="修饰函数的参数">修饰函数的参数</h4>
<p>根据前面讨论的常量指针和指针常量, const修饰函数的参数也为两种</p>
<ol type="1">
<li><p>防止修改指针指向的内容</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">StringCopy</span><span class="params">(<span class="type">char</span> *strDestination, <span class="type">const</span> <span class="type">char</span> *strSource)</span></span>;</span><br></pre></td></tr></table></figure>
<p>其中函数内部是不能随意修改strSource指向的内容,
在编译期间就会报错.</p></li>
<li><p>防止修改指针的地址</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">swap</span><span class="params">(<span class="type">int</span>* <span class="type">const</span> p1, <span class="type">int</span>* <span class="type">const</span> p2)</span></span>;</span><br></pre></td></tr></table></figure>
<p>不能随意修改p1和p2的地址.</p></li>
</ol>
<h4 id="修饰函数的返回值">修饰函数的返回值</h4>
<p>如果给以“指针传递”方式的函数返回值加 const
修饰，那么函数返回值（即指针）的内容不能被修改，该返回值只能被赋给加const
修饰的同类型指针.</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span>* <span class="title">Getstring</span><span class="params">()</span></span>;</span><br><span class="line"><span class="type">const</span> <span class="type">char</span>* str=<span class="built_in">Getstring</span>();</span><br></pre></td></tr></table></figure>
<h4 id="修饰全局变量">修饰全局变量</h4>
<p>全局变量的作用域是整个文件，我们应该尽量避免使用全局变量，因为一旦有一个函数改变了全局变量的值，它也会影响到其他引用这个变量的函数，导致除了bug后很难发现，如果一定要用全局变量，我们应该尽量的使用const修饰符进行修饰，这样防止不必要的人为修改，使用的方法与局部变量是相同的。</p>
<h3 id="代码运行的内存分配">代码运行的内存分配</h3>
<p>C/C++程序经过编译器完成编译链接等等处理工作以后得到的二进制文件,
其包含栈(stack), 堆(heap), 数据段, BSS段, 代码段. 其中数据段,
BSS段和代码段是程序编译完成就已经分配完成了, 无需等代码运行;
而堆区和栈区需要程序被加载到内存开始运行时才会分配.</p>
<ol type="1">
<li><p>栈区(stack): 由编译器自动分配和释放内存.
该区一般用于存储函数参数, 局部变量等值.
其操作方式类似于数据结构中提到的栈.</p></li>
<li><p>堆区(heap): 程序员通过 new 命令和 malloc 命令,
可以动态申请某个大小的内存. 注意的是, 此区的内存和栈区的不同,
并不能由编译器自动释放, 而是由程序员通过 delete 命令和 free
命令来手动释放内存, 如果不释放, 长期使用下, 申请内存超过了堆区大小,
那么就会发生内存泄漏现象.</p></li>
<li><p>数据段:
数据段属于静态内存分配,所有有初值的全局变量和用static修饰的静态变量，常量数据都在数据段中。实际上可以认为其分为两块数据段:1.只读数据段
2. 读写数据段.</p>
<p>在只读数据段中,
一般用来存储程序使用时不会发生变化的数据.一般是用const修饰的变量或者程序中使用的文字常量存储在此处.
特殊的还有常量存储区（特殊的常量存储区，属于静态存储区）</p>
<p>　　1) 常量占用内存,只读状态,决不可修改 　　2)
常量字符串就是放在这里的，程序结束后由系统释放</p>
<p>读写数据段:用来存储那些已经完成初始化的全局变量或者初始化的静态变量.
已初始化数据是在程序中声明，并且具有初值的变量，这些变量需要占用存储器的空间，在程序执行时它们需要位于可读写的内存区域内，并且有初值，以供程序运行时读写</p></li>
<li><p>BSS段: 存储未初始化的全局变量或者静态（全局）变量. 其是可读写的,
编译器给处理成0. 未初始化数据是在程序中声明, 但是没有初始化的变量,
这些变量在程序运行之前不需要占用存储器的空间. 与读写数据段类似,
它也属于静态数据区. 但是该段中数据没有经过初始化.
未初始化数据段只有在运行的初始化阶段才会产生,
因此它的大小不会影响目标文件的大小.</p></li>
<li><p>代码段:
存放函数体的二进制代码,所有语句经过编译后产生的CPU指令都会存放在此处.</p></li>
</ol>
]]></content>
      <categories>
        <category>C Language</category>
      </categories>
      <tags>
        <tag>-Computer Science -C Language</tag>
      </tags>
  </entry>
  <entry>
    <title>Lecture series on graph theory 2</title>
    <url>/2022/03/28/Graph2/</url>
    <content><![CDATA[<p>First, we try to prove the Mantel's Theorem, which was presented as a
question in the last blog post.</p>
<p><strong>Thm(Mantel's Theorem)<br>Let G be a graph with n vertices. If
G does not contain a triangle, then</strong> <span
class="math display">\[
\lvert E(G)\rvert\leq\frac{n^2}{4}
\]</span> Before the proof begin, we introduce what triangle means.</p>
<span id="more"></span>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-29_220946.jpg" /></p>
<p><font color=red><span
class="math inline">\(C=v_1v_2v_3v_4v_5v_1\)</span> is a
5-cycle.Generally, k-cylce has k vertices.Specially, we call a 3-cycle
as a triangle</font></p>
<p>Now, let's take a few examples to find potential patterns which may
help to prove this theorem.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/63DEEBC1B5B34ABA2048219295C11EF6.jpg" /></p>
<p><font color=red>In general, we can divide n vertices into two groups,
then we add all edges between two groups.Because there is no edge
joining the vertices from the same side, if there is a cycle in graph,
it must be an even cycle, such as alternating left-hand and right-hand
sides.<br>If n is even, #edges=<span
class="math inline">\(\frac{n}{2}\cdot\frac{n}{2}=\frac{n^2}{4}\)</span>,
if n is odd, #edges=<span
class="math inline">\(\frac{n+1}{2}\cdot\frac{n-1}{2}=\frac{n^2-1}{4}\leq\frac{n^2}{4}\)</span></font></p>
<hr />
<p><strong>Proof of the Mantel's Theorem</strong></p>
<p>Let G be an n-vertex graph without a triangle.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-29_225536.jpg" /></p>
<p><span class="math inline">\(\forall x,y\in V(G)\)</span>, if <span
class="math inline">\(xy\in E(G)\)</span>, <span
class="math inline">\(N_G(x)\cap N_G(y)=\emptyset\)</span> because G has
no triangle.</p>
<p>Because <span class="math inline">\(N_G(x)\cup N_G(y)\subset
V(G)\)</span>, <span class="math inline">\(d_G(x)+d_G(y)\leq
n\)</span></p>
<p>From the above formula, we have <span class="math display">\[
\sum_{xy\in E(G)}(d_G(x)+d_G(y))\leq\sum_{xy\in E(G)}n=n\cdot\lvert
E(G)\rvert \tag{1}
\]</span> <img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-03-29_225550.4v4nzok1fwc0.jpg" /></p>
<p>We find that <span class="math inline">\(d_G(x)\)</span> appears in
<span
class="math inline">\(d_G(x)+d_G(y_1),d_G(x)+d_G(y_2),\dots,d_G(x)+d_G(y_{d_G(x)})\)</span>
<span class="math display">\[
So\ \sum_{xy\in E(G)}(d_G(x)+d_G(y))=\sum_{x\in V(G)}d_G(x)\cdot
d_G(x)=\sum_{x\in V(G)}d_G(x)^2\tag{2}
\]</span> From (1) &amp; (2), we have <span class="math display">\[
\sum_{x\in V(G)}d_G(x)^2\leq n\cdot\lvert E(G)\rvert\tag{3}
\]</span> According to the relationship between degree and edge: <span
class="math inline">\(\sum_{x\in V(G)}d_G(x)=2\lvert E(G)\rvert\)</span>
and the Cauchy-Schwarz inequality:<span
class="math inline">\((\sum_{k=1}^n a_k^2)(\sum_{k=1}^n
b_k^2)\geq(\sum_{k=1}^na_kb_k)^2\)</span>, we have <span
class="math display">\[
\sum_{x\in V(G)}d_G(x)^2\geq \frac{(\sum_{x\in
V(G)}d_G(x))^2}{n}=\frac{4\lvert E(G)\rvert^2}{n}\tag{4}
\]</span> From (3) &amp; (4), <span class="math inline">\(\frac{4\lvert
E(G)\rvert^2}{n}\leq \sum_{x\in V(G)}d_G(x)^2\leq n\cdot \lvert
E(G)\rvert\)</span> which implies <span class="math inline">\(\lvert
E(G)\rvert \leq\frac{n^2}{4}\)</span></p>
<hr />
<p>If a graph G has more than <span
class="math inline">\(\frac{n^2}{4}\)</span> edges, then G has a
triangle.</p>
<p><font color=red>Furthermore, we can extend the triangle to k-cycle
and obtain the Erdős-Gallai's Theorem <br>We can also extend the
triangle to a complete graph with k vetices, which are called k-clique
and obtain the Turán's Theorem</font></p>
<p>Tree: A tree is a connected graph without cycles.</p>
<p><strong>Proposition: Every tree with at least two vetices has at
least two vertices of degree one</strong></p>
<hr />
<p><strong>Proof</strong></p>
<p>Let P be the longest path of G. Then both end vertices of P have
degree one in G.If not, we suppose that <span
class="math inline">\(u,v\in V(P),\ uv\in E(P)\)</span> and u is the end
vertex of P, but <span class="math inline">\(d_G(u)&gt;1\)</span>.</p>
<p>Case 1:</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-04-04_204919.4qpcnrekreg0.jpg" /></p>
<p><span class="math inline">\(w\not\in V(P)\)</span> and <span
class="math inline">\(wu\in E(G)\)</span>,then <span
class="math inline">\(\tilde{P}=P+wu\)</span> is longer than P. This
contradicts the fact that P is the longest path.</p>
<p>Case 2:</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-04-04_204927.tn63688yr68.jpg" /></p>
<p><span class="math inline">\(w\in V(P),w\not=v\)</span> and <span
class="math inline">\(wu\in E(G)\)</span>, then we can find that there
is a cycle. This contradicts the fact that G is a tree.</p>
<hr />
<p>A vertex of a tree is a leaf if it has degree one.</p>
<p><strong>Proposition: A connected graph with n vertices is a tree if
and only if it has n-1 edges. </strong></p>
<hr />
<p><strong>Proof</strong></p>
<p><span class="math inline">\(\Rightarrow\)</span> n=1, G has no
edges.</p>
<p>Now, we assume that it is correct for a tree with n-1 vertices to
have n-2 edges.From this assumption, we want to deduce that a tree with
n vertices has n-1 edges.</p>
<p>Since <span class="math inline">\(n\ge2\)</span>, G has a vertex v of
degree one. Let u be the only neighbor of v in G. Because G is a tree
with n vertices, G-v is a tree with n-1 vertices. According to the
asssumption,<span class="math inline">\(\lvert
E(G-v)\rvert=(n-1)-1\)</span>. <span class="math display">\[
\lvert E(G)\rvert=\lvert E(G-v)\rvert+1=(n-1)-1+1=n-1
\]</span> <span class="math inline">\(\Leftarrow\)</span> Let G be a
connected graph with n vertices and n-1 edges. We claim that G has a
vertex of degree one.</p>
<p>We prove this claim first.If not, <span class="math inline">\(\forall
x\in V(G)\)</span>, assume <span
class="math inline">\(d_G(x)\ge2\)</span></p>
<p>Then we have <span class="math inline">\(\lvert
E(G)\rvert=\frac{1}{2}\sum_{x\in V(G)}d_G(x)\ge\frac{1}{2}\cdot2\lvert
V(G)\rvert=n&gt;n-1\)</span> which contradicts that <span
class="math inline">\(\lvert E(G)\rvert=n-1\)</span>. The contradiction
implies the claim holds.</p>
<p>From this claim, G has a vertex of degree one. Use the induction on
G-v. Since G-v has n-1 vertices and <span
class="math inline">\((n-1)-1\)</span> edges. By inductive hypothsis,
G-v is a tree and has no cycles.Since v is a degree one vertex, G has no
cycles. Therefore, G is a tree.</p>
<hr />
<p><strong>Corollary: Every graph with minimum degree at least two
contains a cycle.</strong></p>
<p>A spanning tree of G is a connected spanning subgraph of G without
cycles.</p>
<p><font color=red>Problem: counting the number of edge-disjoint
spanning trees in a graph.<br>If the number of edge-disjoint spanning
trees is large, the graph is well-connected. (Related to
edge-connectivity of the graph)</font></p>
<p>Now we introduce two classic search algorithms in graph theory.</p>
<p>A breadth-first-search tree of a graph (BFS-tree)</p>
<p><img src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-04-05_111757.6pnsx44gbus0.jpg" style="zoom: 50%;" /></p>
<ol type="i">
<li><p>Start at <span class="math inline">\(x=x_0\)</span> and join x to
all neighbors of x, Let <span
class="math inline">\(N_1=N_G(x)\)</span></p></li>
<li><p>For each <span class="math inline">\(y\in N_i\)</span>, join y to
all neighbors of y without creating cycles and let <span
class="math inline">\(N_{i+1}=\cup_{y\in N_i}N_G(y)\)</span></p></li>
</ol>
<p>The algorithm will take <span class="math inline">\(O (\lvert
V\rvert+\lvert E\rvert)\)</span> steps to find a BFS-tree rooted at
<span class="math inline">\(x_0\)</span>.</p>
<p>A deepth-first-search tree of a graph (DFS-tree)</p>
<p><img src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/微信图片_20220405111924.6g6tlxqwfto0.jpg" style="zoom:15%;" /></p>
<ol type="i">
<li><p>Start at <span class="math inline">\(x=x_0\)</span> and <span
class="math inline">\(T_0=\{x_0\}\)</span></p></li>
<li><p>Join x to one of its neighbor <span class="math inline">\(x_1\in
N_G(x_0)\)</span> and<span
class="math inline">\(T_1=\{x_0,x_1\}\)</span></p></li>
<li><p>If <span class="math inline">\(N_i=N_G(x_i)\backslash
T_i\not=\emptyset\)</span>, then join <span
class="math inline">\(x_i\)</span> to a vertex <span
class="math inline">\(x_{i+1}\in N_i\)</span> and let <span
class="math inline">\(T_{i+1}=T_i\cup\{x_{i+1}\}\)</span> and <span
class="math inline">\(x_{i+1}\to x_i\)</span></p></li>
</ol>
<p>If <span class="math inline">\(N_i=N_G(x_i)\backslash
T_i=\emptyset\)</span>, then set <span class="math inline">\(x_{i-1}\to
x_{i}\)</span> and <span
class="math inline">\(N_{i-1}=N_G(x_{i-1})\backslash T_i\to N_i\)</span>
and continue.</p>
<p>Eulerian graph:</p>
<p><font color=red>Problem: Let G be a graph.When does G have a closed
trail which contains all edges of G?</font></p>
<p>A closed trail is Eulerian if the trail go through all edges of G. A
graph is called Eulerian if it has an Eulerian trail.</p>
<p><strong>Proposition: Every Eulerian graph is a connected graph which
has only even-degree vertices</strong></p>
<hr />
<p><strong>Proof</strong> Let G be an Eulerian graph. Then G has an
Eulerian trail, denoted by <span
class="math inline">\(T=v_1e_1v_2e_2\cdots v_me_mv_1\)</span>. So G is
connected.</p>
<p>Now, orient the edges of G along the trail T such that, for an edge
<span class="math inline">\(e_i\)</span> in T, orient <span
class="math inline">\(e_i\)</span> from <span
class="math inline">\(v_i\)</span> to <span
class="math inline">\(v_{i+1}\)</span>. Since T has no repeated edges,
every edge receives exactly one orientation. For each vertex <span
class="math inline">\(v_i\)</span>, T visits <span
class="math inline">\(v_i\)</span> and then leaves <span
class="math inline">\(v_i\)</span>, which implies <span
class="math display">\[
\# edges\ oriented\ towards\ v_i=\#edges\ oriented\ away\ from\ v_i
\]</span> But the degree of <span class="math inline">\(v_i\)</span>
satisfies <span class="math display">\[
\begin{aligned}
d_G(v_i)&amp;=\# edges\ oriented\ towards\ v_i+\#edges\ oriented\ away\
from\ v_i\\
&amp;\equiv0(mod\ 2)
\end{aligned}
\]</span> So every vertex of G has an even degree.</p>
<hr />
<p>A cycle decomposition D of a graph G is a set of egde-disjoint cycles
<span class="math inline">\(D=\{C_1,C_2,\dots,C_k\}\)</span> such
that<br />
<span class="math display">\[
\cup_{C_i\in D}E(C_i)=E(G)
\]</span> <strong>Proposition: Every even graph has a cycle
decomposition.</strong></p>
<hr />
<p><strong>Proof</strong> Let G be an even graph. Suppose to the
contrary that G does not have a cycle decomposition.</p>
<p>Assume G is a minimal counterexample.If G has a vertex v of degress
zero, the G-v is a subgraph of G and hence is not a counterexample. So
G-v has a cycle decomposition. So does G, a contradiction to that G is a
counterexample.</p>
<p>So assume that<span class="math inline">\(\delta(G)\ge2\)</span>.
Then by corollary, G has a cycle C. Then, G-E(C) is an even subgraph of
G. So G-E(C) has a cycle decomposition D. Then, <span
class="math inline">\(D\cup\{C\}\)</span> is a cycle-decomposition of G,
which contradicts that G is a counterexample. So prop. holds.</p>
<hr />
<p><font color='red'>Corollary: Every Eulerian graph has a cycle
decomposition.</font></p>
<p><font color='red'>（conjecture,Hajos,1968）All Eulerian graph with n
vertex. How many cycles in a cycle decomposition D?<span
class="math inline">\(\lvert
D\rvert\le\frac{n-1}{2}\)</span>?</font></p>
]]></content>
      <categories>
        <category>Graph Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Operation Research</tag>
      </tags>
  </entry>
  <entry>
    <title>Lecture series on graph theory 3</title>
    <url>/2022/05/20/Graph3/</url>
    <content><![CDATA[<p>Given a graph G, <span class="math inline">\(T=\{v|d_G(v)\equiv1\
mod\ 2\}\)</span>, a subgraph P is called a parity subgraph if <span
class="math display">\[
d_P(v)\equiv\left\{
\begin{aligned}
&amp;0\ mod\ 2,\ v\not\in T\\
&amp;1\ mod\ 2,\ v\in T
\end{aligned}
\right.\qquad i.e.\ d_P(v)\equiv d_G(v)
\]</span> <span class="math inline">\(G-E(P)\)</span> is an even
subgraph <font color='red'>( i.e. every vertex of G-E(P) has even
degree)<br>Problem: To find a parity subgraph with the minimum number of
edges.</font></p>
<span id="more"></span>
<p>More general, let <span class="math inline">\(T\subset V(G)\)</span>,
a set of edges <span class="math inline">\(J\)</span> is called a <span
class="math inline">\(T\)</span>-join if all the edges of <span
class="math inline">\(J\)</span> induce a subgraph, s.t. <span
class="math inline">\(d_J(v)\equiv1\ mod\ 2\Leftrightarrow v\in
T\)</span>, <font color='red'>A parity subgraph of G is a <span
class="math inline">\(T\)</span>-join of G where <span
class="math inline">\(T=\{v|d_G(v)\equiv1\ mod\ 2\}\)</span><br>Problem:
To find a <span class="math inline">\(T\)</span>-join with the minimum
number of edges.</font></p>
<p><strong>Bipartite Graph</strong></p>
<p>A graph is bipartite if its vertex set can be partitioned into two
sets X and Y such that every edge of G joins a vertex of X and a vertex
of Y.</p>
<p><font color='red'>For example.</font></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-22_185717.5b3liqdtj4g0.jpg"
alt="2022-05-21_121856" />
<figcaption aria-hidden="true">2022-05-21_121856</figcaption>
</figure>
<p><strong>Proposition. A graph is bipartite <span
class="math inline">\(\Leftrightarrow\)</span> it has no odd
cycles</strong></p>
<hr />
<p><strong>Proof</strong></p>
<p><span class="math inline">\(\Rightarrow\)</span> Let <span
class="math inline">\(G\)</span> be a bipartite graph and let <span
class="math inline">\((X,Y)\)</span> be a bipartition of <span
class="math inline">\(G\)</span>. Color vertices in X by red and
vertices in Y by blue. Let <span class="math inline">\(C\)</span> be a
cycle. Then the red vertices and blue vertices appear alternatively on
<span class="math inline">\(C\)</span>. So <span
class="math inline">\(C\)</span> has an even number of vertices.
Therefore, <span class="math inline">\(G\)</span> has no odd cycles.</p>
<p><span class="math inline">\(\Leftarrow\)</span> Let v be a vertex of
<span class="math inline">\(G\)</span>. Let <span
class="math inline">\(X=\{x|d_G(x,v)\equiv0(mod\ 2)\}\)</span> and <span
class="math inline">\(Y=\{y|d_G(y,v)\equiv 1(mod\ 2)\}\)</span></p>
<p>Since <span class="math inline">\(G\)</span> has no odd cycle, there
is no edges joining two vertices from the same part. So G is
bipartite.</p>
<hr />
<p><font color='red'>Every part in the bipartition induce the graph
without edges.</font></p>
<p>A vertex subset <span class="math inline">\(I\)</span> of <span
class="math inline">\(V(G)\)</span> is independent if <span
class="math inline">\(G\)</span> has no edges joining any two vertices
of <span class="math inline">\(I\)</span>.</p>
<p>The maximum cardinality of independent sets is called the independent
number of <span class="math inline">\(G\)</span>.</p>
<p><font color='red'>Problem: Determine the independent number of a
given graph <span class="math inline">\(G\)</span> (NPC)</font></p>
<p>If G is bipartite, the its independent number <span
class="math inline">\(\alpha(G)\geq\frac{n}{2}\)</span>.</p>
<p><font color='red'><strong>(The Four color problem) Every graph drawn
on the plane without crossing edges has a vertex partition into four
independent sets.<br>According to the above, if G is a plane graph,
<span class="math inline">\(\alpha(G)\geq\frac{\lvert
V(G)\rvert}{4}\)</span></strong></font></p>
<hr />
<p><strong>Second Proof of the Mantel's Theorem</strong></p>
<p><font color='red'>Recall: Let G be an n-vertex graph without
triangle, then <span class="math inline">\(\lvert
E(G)\rvert\leq\frac{n^2}{4}\)</span></font></p>
<p>Let x be a vertex of G such that <span
class="math inline">\(d_G(x)=\Delta(G)\)</span>. So <span
class="math inline">\(N_G(x)\)</span> is an independent set of G. <span
class="math display">\[
\lvert E(G)\rvert\leq\sum_{y\not\in N_G(x)}d_G(y)\leq(n-d_G(x))\Delta
(G)=(n-d_G(x))d_G(x)\leq\frac{n^2}{4}
\]</span> and equality holds <span class="math inline">\(\Leftrightarrow
n-d_G(x)=d_G(x),\ i.e.\ d_G(x)=\frac{n}{2}\)</span></p>
<hr />
<p><strong>Degenerated Graph</strong></p>
<p>A graph G is k-degenerate if every subgraph of G has a vertex of
degree at most k. <font color='red'>A tree is 1-degenerate</font></p>
<p>A graph G is k-colorable if the vertex of G can be colored by k
different colors such that any vertex subset with same color is an
independent set. i.e. the vertex set of G can be partitioned into k
independent subsets.</p>
<p><strong>Proposition: A k-degenerated graph is
(k+1)-colorable</strong></p>
<hr />
<p><strong>Proof.</strong> Let G be k-degenerate graph. Then G has a
vertex v of degree at most k.</p>
<p>Use induction on the number of vertex.</p>
<p>The inductive hypothesis implies that <span
class="math inline">\(G-v\)</span> is (k+1)-colorable. All vertices in
<span class="math inline">\(N_G(x)\)</span> have been colored by at most
k different colors in any (k+1)-coloring of G-v. Then color v by the
(k+1)th color. So G is (k+1)-colorable.</p>
<hr />
<p><font color='red'>1-factor is a spanning subgraph in which every
vertex has degree one.</font></p>
<p><font color='red'>1-degenerate graph is a tree(or a
forest)</font></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-21_190614.g013sfif35c.jpg"
alt="2022-05-21_190614" />
<figcaption aria-hidden="true">2022-05-21_190614</figcaption>
</figure>
<p><strong>Corollary. Every graph G is <span
class="math inline">\((\Delta(G)+1)\)</span>-degenerate</strong></p>
<p><strong>Def. A Planar graph is a graph with a drawing on the plane
<span class="math inline">\(\mathbb{R}^2\)</span></strong> . A connected
region of <span class="math inline">\(\mathbb{R}^2-G\)</span> is a face
of G.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-21_191223.3jzn0lp4vyg0.jpg"
alt="2022-05-21_191223" />
<figcaption aria-hidden="true">2022-05-21_191223</figcaption>
</figure>
<p><strong>Thm(Euler Formula) Let G be a planar graph, and let n,m and f
be the numbers of vertex,edges and faces of G, Then </strong> <span
class="math display">\[
n-m+f=2
\]</span> <strong>Proposition. Every planar graph is
5-degenerate</strong></p>
<hr />
<p><strong>Proof.</strong> Let G be a planar graph. It suffices to show
that G has a vertex of degree at most five. Without loss of generality,
assume that G is a maximal planar graph (<font color='red'>i.e. every
face is bounded by a triangle</font>). Otherwise, we can add edges to G
to keep it to be a planar graph.</p>
<p>Since every edge appears on the boundaries of two faces and every
face contains exactly three edges, it follows that <span
class="math display">\[
3f=2m\Rightarrow f=\frac{2}{3}m
\]</span> Let <span class="math inline">\(\delta=\delta(G)\)</span>, the
minimum degree of G, then <span class="math display">\[
\delta n\leq\sum_{x\in V(G)}d_G(x)=2m\Rightarrow n\leq\frac{2}{\delta}m
\]</span> By Euler's Formula: <span
class="math inline">\(n-m+f=2\)</span> <span class="math display">\[
2=n-m+\frac{2}{3}m\leq(\frac{2}{\delta}-\frac{1}{3})m
\]</span> Therefore, <span
class="math inline">\(\frac{2}{\delta}-\frac{1}{3}&gt;0\Rightarrow
\delta&lt;6\Leftrightarrow \delta\leq5\)</span></p>
<p>Every planar graph has a vertex of degree at most 5, which implies a
planar graph is 5-degenerate.</p>
<hr />
<p><strong>Corollary. Every planar graph is 6-colorable.</strong></p>
<p><font color='red'>Actually we can use Kempe chain to prove the
proposition that every planar graph is 5-colorable.<br>Problem:What kind
of Eulerian graphs having an even-cycle
decomposition?<br>Conjecture(Akiyama,1980s) Every planar graph with n
vertices has an induced 2-degenerate graph with at least n/2
vertices.<br>Conjecture(Albertson&amp;Berman,1970s) Every planar graph
with n vertices has an induced 1-degenerate graph with at least n/2
vertices.</font></p>
<p><strong>Matrices and Graphs</strong></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-21_235307.fn17ct60tvc.jpg" /></p>
<p><font color='red'>A is a symmetric matrix and because we only
consider simple graphs, its diagonal elements are all zero.</font></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-21_235516.2htdkwuxlpy0.jpg" /></p>
<p>A square matrix A is symmetric if <span
class="math inline">\(A^T=A\)</span></p>
<p>A scalar <span class="math inline">\(\lambda\)</span> is eigenvalue
of A if there exists a non-zero vector <span
class="math inline">\(\vec{X}\)</span>, s.t. <span
class="math inline">\(A\vec{X}=\lambda\vec{X}\)</span> and <span
class="math inline">\(\vec{X}\)</span> is called an eigenvector of A
with respect to <span class="math inline">\(\lambda\)</span>.</p>
<p>A basis <span class="math inline">\(B\)</span> of a vector space is
orthonormal if <span class="math inline">\(||\vec{X}||=1\)</span> for
any <span class="math inline">\(\vec{X}\in B\)</span>, and <span
class="math inline">\(&lt;\vec{X_1},\vec{X_2}&gt;=0\)</span> for any
<span class="math inline">\(\vec{X_1},\vec{X_2}\in B\)</span></p>
<p><strong>Thm. Let A be a real symmetric <span
class="math inline">\((n\times n)\)</span>-matrix. Then A is a
diagonizable and <span class="math inline">\(\mathbb{R}^n\)</span> has
orthonormal basis of eigenvectos of A</strong> <span
class="math display">\[
A\sim\left(\begin{matrix}
   \lambda_1 &amp; \dots &amp;0\\
   \vdots &amp; \ddots &amp;\vdots\\
   0 &amp; \dots &amp;\lambda_n\\
  \end{matrix}\right)
\]</span></p>
<p><strong>Proposition. All eigenvalues of a real symmetric <span
class="math inline">\((n\times n)\)</span>-matrix is real</strong></p>
<hr />
<p><strong>Proof</strong> We assume that A is a <span
class="math inline">\(n\times n\)</span> matrix, <span
class="math inline">\(\lambda\)</span> is an eignevalue of A.</p>
<p>Assume that <span class="math inline">\(\vec{X}\)</span> is an
eigenvector of <span class="math inline">\(\lambda\)</span>, Then <span
class="math display">\[
\begin{aligned}
\lambda||\vec{X}||&amp;=&lt;\lambda\vec{X},\vec{X}&gt;=&lt;A\vec{X},\vec{X}&gt;=\vec{X^T}A\vec{X}\\&amp;=\vec{X^T}A^T\vec{X}=(A\vec{X})^T\vec{X}=&lt;\vec{X},A\vec{X}&gt;=\bar{\lambda}||\vec{X}||
\end{aligned}
\]</span> So <span
class="math inline">\(\lambda=\bar{\lambda}\Rightarrow\lambda\)</span>
is real.</p>
<hr />
<p>A real symmetric matrix A is positive semi-definite if ,for all <span
class="math inline">\(\vec{X}\in\mathbb{R}^n\)</span>, <span
class="math inline">\(\vec{X}^TA\vec{X}\geq0\)</span> and A is positive
definite if <span
class="math inline">\(\vec{X}^TA\vec{X}&gt;0\)</span></p>
<p><strong>Proposition. All eigenvalues of a positive semi-definite
<span class="math inline">\(\Leftrightarrow\ \exists\)</span> a matrix
B, s.t. <span class="math inline">\(A=B^TB\)</span></strong></p>
<hr />
<p><strong>Proof.</strong><span
class="math inline">\(\Rightarrow\)</span> We assume that A is a
positive semi-definite matrix.</p>
<p>Then A is diagonaizable and assume that <span
class="math inline">\(A=Q^TDQ\)</span> where D is a diagonable matrix
with eigenalues on its diagonal. Note that all egienvalues of A are
non-negative. <span class="math display">\[
A=Q^TDQ=Q^TD^{\frac{1}{2}}D^\frac{1}{2}Q=(\sqrt{D}Q)^T(\sqrt{D}Q)=B^TB,\quad
where\ B=\sqrt{D}Q
\]</span> <span class="math inline">\(\Leftarrow\)</span> Let <span
class="math inline">\(A=B^TB\)</span>.</p>
<p>Then for any <span
class="math inline">\(\vec{X}\in\mathbb{R}^n\)</span>, <span
class="math inline">\(\vec{X}^TA\vec{X}=\vec{X}^TB^TB\vec{X}=(B\vec{X})^TB\vec{X}=||B\vec{X}||\geq0\)</span></p>
<p>So A is positive semi-definite.</p>
]]></content>
      <categories>
        <category>Graph Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Operation Research</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 Notes for Lecture 2</title>
    <url>/2024/07/30/CS229%20Notes%20for%20Lecture%202/</url>
    <content><![CDATA[<h1 id="lecture-1">Lecture 1</h1>
<p><a id="org8b14315"></a></p>
<h2 id="introduction-of-supervised-learning">Introduction of supervised
learning</h2>
<p>在介绍监督学习(Supervised
learning)之前,我们先罗列一些常用的符号,以便于后续的讨论:</p>
<ul>
<li><p><span class="math inline">\(m\)</span>: 训练样本的数量</p></li>
<li><p><span class="math inline">\(n\)</span>: 特征的数量</p></li>
<li><p><span class="math inline">\(x\)</span>: 输入变量(input variable),
或称为输入特征(input feature/attribute)</p></li>
<li><p><span class="math inline">\(y\)</span>: 输出变量(output
variable), 或称为目标变量(target variable)</p></li>
<li><p><span class="math inline">\((x,y)\)</span>: 表示训练样本; <span
class="math inline">\((x^{(i)}, y^{(i)})\)</span>: 第 i 个训练样本(<span
class="math inline">\(i^{th}\)</span> training example)</p></li>
<li><p><span class="math inline">\(\mathcal{X}\)</span>:
输入变量的空间(input space)</p></li>
<li><p><span class="math inline">\(\mathcal{Y}\)</span>:
输出变量的空间(output space)</p>
<span id="more"></span></li>
</ul>
<p>监督学习的目标是基于已知的训练样本, 学习出一个假设(hypothesis) <span
class="math inline">\(h: \mathcal{X} \rightarrow \mathcal{Y}\)</span>,
使得 <span class="math inline">\(h(x)\)</span>
能够尽可能地接近真实的输出 <span class="math inline">\(y\)</span>.
根据输出变量的类型,我们可以将监督学习分成两类问题:</p>
<ol type="1">
<li>回归问题(Regression): 输出变量的取值是连续的</li>
<li>分类问题(Classification): 输出变量的取值是离散的</li>
</ol>
<p>我们可以从下图来直观理解监督学习的过程:</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picx-images-hosting@master/2024-07-30_16-29-16_screenshot.lz8nr081.png" />
因此,监督学习的关键就是利用学习算法(learning algorithm)从训练集(training
set)中学习出一个符合条件的假设 <span class="math inline">\(h\)</span>,
进一步其实监督学习的关键就是去构造一个合适的学习算法.</p>
<p>而在构造学习算法的过程中, 我们需要考虑以下几个问题:</p>
<ol type="1">
<li>训练集样本是什么样的?</li>
<li>假设函数是什么样的?</li>
<li>如何表示假设函数?</li>
</ol>
<p><a id="org172f874"></a></p>
<h2 id="linear-regression">Linear regression</h2>
<p>在本节中, 我们将假设函数 <span class="math inline">\(h\)</span>
限制在线性函数形式下考虑回归问题, 这一问题也被称之为线性回归问题(Linear
Regression). 在线性回归问题中, 我们试图去构造一个线性的假设函数,
因此我们可以直接形式的写出假设函数的结构如下: <span
class="math display">\[h(x)=\theta_0+\theta_1x_1+\cdots+\theta_nx_n\]</span>
其中, <span class="math inline">\(\theta_i(i=0,\cdots,n)\)</span>
是参量(parameter), 如果我们将其确定下来,
那么我们就可以得到一个确定的假设函数 <span
class="math inline">\(h\)</span>. 但为了形式上的简洁,
我们引入一个虚特征(dummy feature) <span
class="math inline">\(x_0=1\)</span>,
这样我们就可以将上面的假设函数写成更为紧凑的形式: <span
class="math display">\[h(x)=\sum_{i=0}^n\theta_ix_i=\theta^Tx\]</span>
其中, <span
class="math inline">\(\theta=(\theta_0,\cdots,\theta_n)^T\)</span>
是学习算法的参数向量(parameter vector), <span
class="math inline">\(x=(x_0,\cdots,x_n)^T\)</span>. 在此值得注意的是,
虽然我们只考虑 <span class="math inline">\(n\)</span> 个输入特征,
但由于我们为了方便引入了虚特征, 因此实际上我们考虑的 <span
class="math inline">\(\theta\)</span> 和 <span
class="math inline">\(x\)</span> 是一个 <span
class="math inline">\(n+1\)</span> 维的向量. 其次,
虽然我们在此给出的假设函数符号是 <span
class="math inline">\(h(x)\)</span>, 但实际上更为明确的写法应该是 <span
class="math inline">\(h_{\theta}(x)\)</span>, 这样可以表示假设函数 <span
class="math inline">\(h\)</span> 不仅依赖于输入特征 <span
class="math inline">\(x\)</span>, 还依赖于参数向量 <span
class="math inline">\(\theta\)</span>; 只是在不引起混淆的情况下,
我们可以省略 <span class="math inline">\(\theta\)</span> 的表示.</p>
<p>根据上面的假设函数的结构,
我们可以知道学习算法的工作是选择一个合适的参数向量,
这样就可以导出一个假设, 因此如何选择合适的参数向量就是学习算法的关键.
从直观上来说, 对于训练集样本, 我们希望选择出来的参数向量能够使得 <span
class="math inline">\(h(x)\)</span> 尽可能地接近真实的输出 <span
class="math inline">\(y\)</span>. 因此我们需要先定义一个函数用来度量
<span class="math inline">\(h(x)\)</span> 和 <span
class="math inline">\(y\)</span> 之间的差距, 我们称之为成本函数(cost
function): <span
class="math display">\[J(\theta)=\frac{1}{2}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2\]</span>
此处的 <span class="math inline">\(\frac{1}{2}\)</span>
是因为可以消去因求导得到的<span class="math inline">\(2\)</span>,
并不影响最后结果. 根据我们前面介绍的选择参数向量 <span
class="math inline">\(\theta\)</span> 的目标,
我们将其转化为一个优化问题, 即找到一个 <span
class="math inline">\(\theta\)</span> 使得 <span
class="math inline">\(J(\theta)\)</span> 最小, 其数学形式如下: <span
class="math display">\[\min_{\theta}J(\theta)=\min_{\theta}\frac{1}{2}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2\]</span></p>
<p><a id="org5e1d335"></a></p>
<h2 id="gradient-descentgd">Gradient Descent(GD)</h2>
<p>此节中, 我们将用梯度下降算法(Gradient Descent)来求解上面的优化问题.
值得注意的是, 梯度下降算法是一种迭代算法,
其基本思想是通过不断更新迭代参数向量 <span
class="math inline">\(\theta\)</span> 的值, 使得成本函数 <span
class="math inline">\(J(\theta)\)</span> 不断减小, 直到收敛到最小值.</p>
<p>我们可以给出梯度下降算法的框架如下:</p>
<ol type="1">
<li>Start with some <span class="math inline">\(\theta\)</span>. For
example, <span class="math inline">\(\theta=0\)</span>.</li>
<li>Keep changing <span class="math inline">\(\theta\)</span> to reduce
<span class="math inline">\(J(\theta)\)</span>.</li>
<li>Repeat 2. until convergence.</li>
</ol>
<p>从上面的框架中我们可以看出, 梯度下降算法的关键就是如何更新参数向量
<span class="math inline">\(\theta\)</span> 的值, 其更新的规则如下:
<span
class="math display">\[\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\
\text{for each value of }j\]</span> 这里的 <span
class="math inline">\(\alpha\)</span> 是学习率(learning rate),
用来控制参数向量 <span class="math inline">\(\theta\)</span> 的更新步长;
但这个值的选取是一个极具经验性的问题,
因为学习率太小则会导致收敛速度太慢, 而学习率太大则会导致算法无法收敛.
因此实际中, 应该尝试多组学习率, 从而选择一个最为合适的学习率.</p>
<p>由于我们已知成本函数 <span class="math inline">\(J(\theta)\)</span>
的具体形式, 因此我们可以直接求出其关于参数向量 <span
class="math inline">\(\theta\)</span> 的偏导数的具体形式为: <span
class="math display">\[\begin{aligned}\frac{\partial}{\partial\theta_j}J(\theta)&amp;=\frac{\partial}{\partial\theta_j}\frac{1}{2}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2\\
&amp;=\sum_{i=0}^m(h(x^{(i)})-y^{(i)})\frac{\partial}{\partial
\theta_j}(h(x^{(i)})-y^{(i)})\\
&amp;=\sum_{i=0}^m(h(x^{(i)})-y^{(i)})\frac{\partial}{\partial
\theta_j}(\sum_{k=0}^n\theta_kx_k^{(i)}-y^{(i)})\\
&amp;=\sum_{i=0}^m(h(x^{(i)})-y^{(i)})x_j^{(i)}\end{aligned}\]</span>
根据上面的偏导数的具体形式,
我们可以将梯度下降算法的更新规则写成更为具体的形式: <span
class="math display">\[\theta_j:=\theta_j-\alpha\sum_{i=0}^m(h(x^{(i)})-y^{(i)})x_j^{(i)},\
j=0,1,\dots,n\]</span> 从这个更新规则中,
我们可以发现每更新一次需要用全部训练样本的数据,
因此我们将这种梯度下降算法称之为批量梯度下降算法(Batch Gradient
Descent).其的主要优点就是计算稳定, 他一定可以朝着极小值的方向前进;
但是缺点也很明显, 因为他每次更新参数都需要用到全部的训练样本,
因此如果训练样本数据量很大的情况,
那么他的计算量就会花费大量时间在调用全部训练样本上.</p>
<p>因此, 为了避免每次更新参数都需要用到全部训练样本的数据,
我们可以引入随机梯度下降算法(Stochastic Gradient Descent, SGD),
其的更新规则如下: <span class="math display">\[
\theta_j:=\theta_j-\alpha(h(x^{(i)})-y^{(i)})x_j^{(i)},\ j=0,1,\dots,n\
(\text{for }i=1\ \text{to }m)\]</span> 此时,
每次更新我们都只用了一个训练样本的数据,
因此这样会大幅度减少更新计算量;操作过程是在第一次更新中,
我们选用第一个训练样本更新参数; 在第二次更新中,
我们选用第二个训练样本更新参数, 以此类推. 但值得注意的一点是,
随机梯度下降算法由于每次都是用一个训练样本的数据,
因此他的更新方向是不稳定的, 因此他可能会在极小值附近不断震荡;
但他优点就是每次迭代的计算量小, 尤其是在训练样本数据量很大的情况下,
他的优势就会更加明显. 不仅如此, 如果我们考虑较为复杂的优化问题,
随机梯度下降算法是有可能帮助我们跳出局部极小值,
因此随机算法对于机器学习是很重要的优化算法.</p>
<p>虽然SGD可能达不到最小值, 而是在最小值附近震荡,
但我们可以在某个容忍精度下停止迭代,这样得到的参数也是一个比较好的参数.
但实际上, 我们一般是在使用SGD的过程中, 随时间增长慢慢地减小学习率的值,
这样即使参数会在最小值附近震荡, 但随着学习率 <span
class="math inline">\(\alpha\)</span> 的减小, 参数的震荡范围越来越小,
近似认为达到了最小值.</p>
<p>最后, 我们给出另一个梯度下降算法的idea,
他结合了批量梯度下降算法和随机梯度下降算法的优点,
这就是小批量梯度下降算法(Mini-batch Gradient Descent).
他并不是每次迭代都只用一个训练样本的数据(会使得下降方向不稳定),也不用全部训练样本的数据(会使得计算量过大),
而是每次迭代用一小部分训练样本的数据, 他每次迭代都用 <span
class="math inline">\(k\)</span> 个训练样本的数据, 其既降低了计算量,
又保持了下降方向的稳定性. 至于 <span class="math inline">\(k\)</span>
的选取, 其一般是在算法运行之前人为选取的.</p>
<p>不过事实上, 如果成本函数 <span
class="math inline">\(J(\theta)\)</span> 的结构并不是很好的情况下,
那么会出现以下几种情况:</p>
<ol type="1">
<li>收敛到函数的局部最小值(local minimum), 而不是全局最小值(global
minimum).</li>
<li>收敛到函数的驻点(saddle point).</li>
<li>微分值接近于 <span class="math inline">\(0\)</span>
就停下来，但这里只是比较平缓，并不是极值点。</li>
</ol>
<p>但值得注意的是,我们在这里考虑线性回归问题,
其成本函数其实是一个二次函数, 因此他是不存在局部最小值和驻点的,
因此只要他能收敛, 那么就一定是收敛到全局最小值.</p>
<p><a id="org0d1c713"></a></p>
<h2 id="normal-equation">Normal Equation</h2>
<p>上面我们介绍的GD是一种迭代算法, 但实际上对于线性回归问题,
我们可以直接用代数的方法求解出最优参数向量 <span
class="math inline">\(\theta\)</span>, 这种方法被称之为正规方程(Normal
Equation). 但需要声明的是, 正规方程只适用于线性回归问题.
在推导正规方程以前, 我们先引入矩阵导数的记号: <span
class="math display">\[\begin{aligned}\nabla_{\theta}J(\theta)&amp;=\begin{pmatrix}\frac{\partial
J}{\partial \theta_0}\cdots \frac{\partial J}{\partial
\theta_n}\end{pmatrix}^T,\ \theta\in\mathbb{R}^{n+1}\\
\nabla_Af(A)&amp;=\begin{pmatrix}\frac{\partial f}{\partial
A_{11}}&amp;\cdots&amp;\frac{\partial f}{\partial A_{1q}}\\\vdots&amp;
&amp;\vdots\\\frac{\partial f}{\partial
A_{p1}}&amp;\cdots&amp;\frac{\partial f}{\partial A_{pq}}\end{pmatrix},\
A\in\mathbb{R}^{p\times q}\end{aligned}\]</span>
因此我们希望能够将成本函数 <span
class="math inline">\(J(\theta)\)</span> 写成紧凑的矩阵向量形式,
在此之前, 我们先引入一些矩阵符号来辅助推导成本函数的矩阵形式: <span
class="math display">\[\begin{aligned}X&amp;=\begin{pmatrix}\cdots&amp;(x^{(1)})^T&amp;\cdots\\
&amp;\vdots&amp; \\\cdots&amp;(x^{(m)})^T&amp;\cdots\end{pmatrix}
\text{design matrix}\\
\vec{y}&amp;=\begin{pmatrix}y^{(i)}&amp;\cdots&amp;y^{(m)}\end{pmatrix}^T
\text{label vector}\end{aligned}\]</span> 这里值得注意的是,
我们构造的这个 design matrix 的行向量在记号上是训练样本的转置,
但是实际上, 我们考虑的是训练样本的特征再加上虚特征 <span
class="math inline">\(x_0\)</span>, 所以这个 design matrix 的维度为
<span class="math inline">\(m\times (n+1)\)</span>. 依据这些记号,
我们给出成本函数的矩阵形式为: <span
class="math display">\[J(\theta)=\frac{1}{2}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2=\frac{1}{2}(X\theta-\vec{y})^T(X\theta-\vec{y})\]</span>
由于我们知道线性回归问题的核心问题就是最小化成本函数,
这个从导数的角度来看, 就是成本函数对参数向量的导数为0; 其数学形式如下:
<span
class="math display">\[\nabla_{\theta}J(\theta)=\nabla_{\theta}\frac{1}{2}(X\theta-\vec{y})^T(X\theta-y)=X^TX\theta-X^T\vec{y}=0\]</span>
所以线性回归问题对应的正规问题为: <span
class="math display">\[X^TX\theta=X^T\vec{y}\Rightarrow
\theta=(X^TX)^{-1}X^T\vec{y}\]</span> 这里有一个问题就是如果 <span
class="math inline">\(X^{T}X\)</span> 不可逆, 那么 <span
class="math inline">\((X^TX)^{-1}\)</span> 并不存在, 那我们可以用 <span
class="math inline">\((X^TX)^{-1}\)</span> 的伪逆来代替.</p>
]]></content>
      <categories>
        <category>CS229</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Lecture series on graph theory 5</title>
    <url>/2022/05/22/Graph5/</url>
    <content><![CDATA[<p>Let G be a finite graph. We consider a random walk on the vertices of
G of the following type. Start at a vertex <span
class="math inline">\(v\)</span> .<font color='red'>(v could be chosen
randomly according to some probably distribution or could be specified
in advance).</font> Among all edges incident to v, choose one uniformly
at random<font color='red'>( i.e. if <span
class="math inline">\(d_G(v)=d\)</span>, each of these edges is chosen
with probability 1/d)</font></p>
<p><strong>Problem: determine the probability of being at a given vertex
after a given number steps.</strong></p>
<span id="more"></span>
<p>Let M(G) be the matrix whose rows and columns are indexed by vertices
<span class="math inline">\(v_1,\dots,v_n\)</span>, we assume that <span
class="math inline">\(m_{ij}\)</span> is the <span
class="math inline">\((i,j)\)</span>-entry of M(G), which satisfies the
following formula <span class="math display">\[
m_{ij}=\frac{a_{ij}}{d_i}
\]</span> where <span class="math inline">\(a_{ij}\)</span> is the
number of edges between <span class="math inline">\(v_i\)</span> and
<span class="math inline">\(v_j\)</span>. The <span
class="math inline">\(m_{ij}\)</span> is probability that if one starts
at <span class="math inline">\(v_i\)</span> and then the next step will
be at <span class="math inline">\(v_j\)</span>.</p>
<p><font color='red'>Recall: the number of k-walks from <span
class="math inline">\(v_i\)</span> to <span
class="math inline">\(v_j\)</span>= the <span
class="math inline">\((i,j)\)</span>-entry of <span
class="math inline">\(A(G)^k\)</span></font></p>
<p><span class="math inline">\(\exists v_ie_1v_{i_1}e_2v_{i_2}\cdots
e_kv_j,\ P(v_ie_1v_{i_1}e_2v_{i_2}\cdots
e_kv_j)=\frac{a_{ii_1}}{d_i}\frac{a_{i_1i_2}}{d_{i_1}}\cdots\frac{a_{i_{k-1}i_j}}{d_{i_{k-1}}}=m_{ii_1}m_{i_1i_2}\cdots
m_{i_{k-1}j}\)</span> <span class="math display">\[
\begin{aligned}
P(v_i\to v_j\ in\ k\ steps)&amp;=\sum_{k-walk\ from\ v_i\to v_j}
P(v_ie_1v_{i_1}e_2v_{i_2}\cdots e_kv_j)\\
&amp;=\sum m_{ii_1}m_{i_1i_2}\cdots m_{i_{k-1}j}=(i,j)-entry\ of\ M(G)^k
\end{aligned}
\]</span> Assume the probability to pick <span
class="math inline">\(v_i\)</span> is <span
class="math inline">\(p(v_i)\)</span> for <span
class="math inline">\(i\in\{1,\dots,n\}\)</span> s.t. <span
class="math inline">\(\sum_{i=1}^n p(v_i)=1\)</span></p>
<p>Let <span
class="math inline">\(P=[\begin{matrix}p(v_1)&amp;p(v_2)&amp;\cdots&amp;p(v_n)\end{matrix}]\)</span>.
Then the probability of a walk ending up at <span
class="math inline">\(v_i\)</span> in k steps is <span
class="math display">\[
P(there\ is\ a\ walk\ ending\ up\ at\ v_i\ in\ k\ steps)=[PM(G)^k]_i
\]</span> If G is k regular <font color='red'>( every vertex has degree
k)</font> then <span
class="math inline">\(M(G)=\frac{1}{d}A(G)\)</span></p>
<p>Then the eigenvalue <span class="math inline">\(\beta_i\)</span> of
<span class="math inline">\(M(G)\)</span> satisfies <span
class="math inline">\(\beta_1=\frac{1}{d}\lambda_i\)</span> where <span
class="math inline">\(\lambda_i\)</span> is an eigenvalue of G. <span
class="math display">\[
P(\exists\ a\ closed\ k-walk\ starting\
v)=\beta_1^k+\cdots+\beta_n^k=\frac{1}{d^k}(\lambda_1^k+\cdots+\lambda_n^k)
\]</span> <strong>Weighted graphs and digraphs</strong></p>
<p>Let G be a graph. A weighted graph is a graph G associated with a
weight function: <span
class="math inline">\(w:E(G)\to\mathbb{F}\)</span> where <span
class="math inline">\(\mathbb{F}\)</span> is a field, denote by <span
class="math inline">\((G,w)\)</span>. Most of the time, we consider the
case that <span
class="math inline">\(\mathbb{F}=\mathbb{R}\)</span>.</p>
<p>The adjacency matrix of a weighted graph is a real symmetric matrix
<span class="math inline">\(A(G,w)=(a_{ij})_{n\times n}\)</span> with
<span class="math inline">\(a_{ij}=w(v_iv_j)\)</span>, the weight of the
edge of <span class="math inline">\(v_iv_j\)</span>.</p>
<p>Let H be a subgraph of <span class="math inline">\((G,w)\)</span>.
The weight of H is defined as <span
class="math inline">\(w(H)=\sum_{e\in E(H)}w(e)\)</span></p>
<p>For example, how to find a minimum weight path between two given
vertices? How to find a minimum spanning tree or how to find a cycle or
walk with large weight<font color='red'>( so-called heavy cycles or
walks)</font></p>
<p>A digraph is a graph with an orientation which assigns an orientation
to each edge.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-23_002508.6gvjbxprb7g0.jpg" /></p>
<p><font color='red'>An orientation edge (or arc) from <span
class="math inline">\(v_i\)</span> to <span
class="math inline">\(v_j\)</span> is an ordered pair of vertices ,
denoted by <span class="math inline">\((v_i,v_j)\)</span> where <span
class="math inline">\(v_i\)</span> is called the tail and <span
class="math inline">\(v_j\)</span> is called the head of arc.</font>
<span class="math display">\[
A(D,w)=[a_{ij}]_{n\times n}\  where\ a_{ij}=\left\{\begin{aligned}&amp;
w(v_iv_j),\quad v_i\to v_j\\
&amp;-w(v_iv_j),\quad v_j\to v_i\\
&amp;0,\quad otherwise\end{aligned}\right.
\]</span> Then <span class="math inline">\(A(D,w)\)</span> is no longer
symmetric <font color='red'>but skew-symmetric:<span
class="math inline">\(A+A^T=O\)</span></font></p>
<p>For a given graph G, let D be an orientation of G, <span
class="math inline">\(\vec{B}\)</span> is an oriented incidence matrix
<span class="math display">\[
\vec{B}\cdot \vec{f}=\vec{b}\ where\ \vec{b}\in\mathbb{R}^{|V(G)|},\
\vec{f}\in\mathbb{R}^{|E(G)|}
\]</span> A solution to <span class="math inline">\(\vec{B}\cdot
\vec{f}=\vec{b}\)</span> is called a network flow. <span
class="math inline">\(\vec{b}\)</span> is called a boundary
condition.</p>
<p>If <span class="math inline">\(\vec{b}=\vec{0},\ \vec{B}\cdot
\vec{f}=\vec{0}.\qquad(*)\)</span></p>
<p>A vector <span class="math inline">\(\vec{f}\)</span> satisfies <span
class="math inline">\((*)\)</span> is called a circulation or real flow
if <span
class="math inline">\(\vec{f}\in\mathbb{R}^{|E(G)|}\)</span>.</p>
<p>If a solution to <span class="math inline">\((*)\)</span> satisfies
<span class="math inline">\(\vec{f}\in\mathbb{Z}^{|E(G)|}\)</span>, then
<span class="math inline">\(\vec{f}\)</span> is called an integer
flow.</p>
<p><font color='red'>Tutte's five-flow conjecture: For a graph without
bridge, it has a nowhere zero 5-flow i.e. <span
class="math inline">\(\exists \vec{f}\in\mathbb{Z}^{|E(G)|}\)</span> and
every component of f is not zero and absolute value of every component
is at most 4. <span class="math inline">\(\vec{f}=[f_1,
f_2,\cdots,f_m]\)</span> where <span class="math inline">\(m=|E(G)|,\
f_i\in\{-4,-3,-2,-1,0,1,2,3,4\}\)</span></font></p>
<p><strong>Eigenvalues and subgraphs </strong></p>
<p>Let G be a graph with n vertices, <span
class="math inline">\(A(G)\)</span> be the adjacency matrix.</p>
<p>All n eigenvalues of <span class="math inline">\(A(G)\)</span> are
real because <span class="math inline">\(A(G)\)</span> is real and
symmertic <span class="math display">\[
\lambda_1(G)\geq\lambda_2(G)\geq\cdots\geq\lambda_{\lceil
\frac{n+1}{2}\rceil}\geq\lambda_{\lfloor
\frac{n+1}{2}\rfloor}\geq\lambda_n(G)
\]</span> where <span class="math inline">\(\lambda_{\lceil
\frac{n+1}{2}\rceil},\lambda_{\lfloor \frac{n+1}{2}\rfloor}\)</span> are
median eigenvalues.</p>
<p>The largest eigenvalues <span
class="math inline">\(\lambda_1(G)\)</span> is usually called the
spectral radius of G.</p>
<p><strong>Proposition. Let G be an n-vertex graph and A be its
adjacency matrix.Let <span
class="math inline">\(\lambda_1\geq\cdots\geq\lambda_n\)</span> be the
eigenvalues of A<br>Then 1. <span class="math inline">\(\lambda_1
=\max_{\vec{x}\not=\vec{0}}\frac{\vec{x}^TA\vec{x}}{\vec{x}^T\vec{x}}\)</span>,
2. <span
class="math inline">\(\lambda_1\geq|\lambda_n|\)</span></strong></p>
<hr />
<p><strong>Proof</strong></p>
<p>1.The eigenvalues of A (say <span
class="math inline">\(\vec{u}_1,\vec{u}_2,\dots,\vec{u}_n\)</span>) can
be chosen as a standard orthogonal basis of <span
class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>Assume that array (<span
class="math inline">\(x_1,x_2,\dots,x_n\)</span>) is the coordinate of
<span class="math inline">\(\vec{x}\)</span> under this basis <span
class="math display">\[
\frac{\vec{x}^TA\vec{x}}{\vec{x}^T\vec{x}}=\frac{\lambda_1x_1^2+\cdots+\lambda_nx_n^2}{x_1^2+\cdots+x_n^2}\leq\lambda_1
\]</span> with equality holds if and only if <span
class="math inline">\(x_1\not= 0\)</span> and <span
class="math inline">\(x_i=0\)</span> for <span
class="math inline">\(i=\{2,\cdots,n\}\)</span>, i.e. <span
class="math inline">\(\vec{x}\)</span> is exactly an eigenvector of
<span class="math inline">\(\lambda_1\)</span>.</p>
<ol start="2" type="1">
<li><p>Let <span class="math inline">\(\vec{v}\)</span> be get from
<span class="math inline">\(\vec{u}_n\)</span> by convert each component
into its absolute value (i.e. <span
class="math inline">\(\vec{v}=(|\vec{u}_n(1)|,|\vec{u}_n(2)|,\cdots,|\vec{u}_n(n)|)\)</span>)</p>
<p>From proof of 1. we can write <span
class="math inline">\(\lambda_n\)</span> as <span
class="math display">\[
|\lambda_n|=|\frac{\vec{u}^T_nA\vec{u}_n}{\vec{u}_n^T\vec{u}_n}|=|\vec{u}^T_nA\vec{u}_n|\leq\sum_{i,j\in[n]}A(i,j)|\vec{u}_n(i)\vec{u}_n(j)|=|\frac{\vec{v}^TA\vec{v}}{\vec{v}^T\vec{v}}|\leq\lambda_1
\]</span> We have used triangle inequality at the first inequality and
the result of 1. at the second inequality.</p></li>
</ol>
<hr />
<p><strong>Propsition. Let G be a graph, and <span
class="math inline">\(\Delta (G)\)</span> be the maximum degree and
<span class="math inline">\(d(G)\)</span> be the average degree. Then we
have </strong> <span class="math display">\[
d(G)\leq\lambda_1\leq\Delta (G)
\]</span></p>
<hr />
<p><strong>Proof</strong></p>
<p>Note that, there exist <span
class="math inline">\(\vec{x}\not=\vec{0}\)</span>, s.t. <span
class="math inline">\(A\vec{x}=\lambda\vec{x}\)</span> where A is the
adjacency matrix. <span class="math display">\[
\lambda_1=\sup\{\vec{x}^TA\vec{x}\Big|||\vec{x}||=1\ and\
\vec{x}\in\mathbb{R}^n\}
\]</span> Let <span
class="math inline">\(\vec{x}_1=\frac{1}{\sqrt{n}}[\begin{matrix}1&amp;1&amp;\dots&amp;1\end{matrix}]^T\)</span>,
then <span class="math display">\[
\begin{aligned}\lambda_1&amp;\geq
\vec{x}_1^TA\vec{x}_1=\frac{1}{\sqrt{n}}[\begin{matrix}1&amp;1&amp;\dots&amp;1\end{matrix}]A\frac{1}{\sqrt{n}}[\begin{matrix}1&amp;1&amp;\dots&amp;1\end{matrix}]^T\\
&amp;=\frac{1}{n}[\begin{matrix}1&amp;1&amp;\dots&amp;1\end{matrix}][\begin{matrix}d(v_1)&amp;d(v_2)&amp;\cdots&amp;d(v_n)\end{matrix}]^T\\
&amp;=\frac{1}{n}\sum_{i=1}^nd(v_i)=d(G)\end{aligned}
\]</span> For the upper bound, let <span
class="math inline">\(\vec{x}\)</span> s.t. <span
class="math inline">\(A\vec{x}=\lambda_1\vec{x}\)</span> where <span
class="math inline">\(\vec{x}=[\begin{matrix}x_1&amp;x_2&amp;\cdots&amp;x_n\end{matrix}]^T\)</span></p>
<p>Let <span class="math inline">\(x_i\)</span> be the largest value
among all components of <span class="math inline">\(\vec{x}\)</span>.
Then, <span class="math display">\[
\lambda_1 x_i=\sum_{v_iv_j\in E(G)}x_j\leq\Delta(G)x_j
\]</span> So <span class="math inline">\(\lambda_1\leq\Delta
(G)\)</span> follows.</p>
<hr />
<p><strong>Corollary Let G be a k-regular graph. Then <span
class="math inline">\(\lambda_1(G)=k\)</span></strong></p>
<p>A signed graph <span class="math inline">\((G,\sigma)\)</span> is a
weighted graph s.t. <span class="math inline">\(\sigma:\
E(G)\to\{-1,1\}\)</span> <span class="math display">\[
A(G,\sigma)=[a_{ij}]_{n\times n}\ where\
a_{ij}=\left\{\begin{aligned}&amp;\sigma(v_iv_j),\quad v_iv_j\in
E(G)\\&amp;0,\quad otherwise\end{aligned}\right.
\]</span> So <span class="math inline">\(A(G,\sigma)\)</span> is a
symmetric real matrix.</p>
<p>We can assume that <span
class="math inline">\(\lambda_1(G,\sigma)\geq\lambda_2(G,\sigma)\geq\cdots\geq\lambda_n(G,\sigma)\)</span>,
<font color='red'>but <span
class="math inline">\(\lambda_1(G,\sigma)\geq|\lambda_n(G,\sigma)|\)</span> may
not hold.</font></p>
<p><strong>Prop. Let <span class="math inline">\((G,\sigma)\)</span> be
a signed graph with maximum degree <span class="math inline">\(\Delta
(G)\)</span>. Then</strong> <span class="math display">\[
|\lambda(G,\sigma)|\leq\Delta(G)\ where\ \lambda(G,\sigma)\ is\ an\
eigenvalue\ of\ G
\]</span></p>
<hr />
<p><strong>Proof</strong></p>
<p>Let <span class="math inline">\(\vec{x}\)</span> be an eigenvector of
<span class="math inline">\(\lambda\)</span> s.t. <span
class="math inline">\(\lambda\vec{x}=A\vec{x}\)</span>.</p>
<p>Let <span
class="math inline">\(\vec{x}=[\begin{matrix}x_1&amp;x_2&amp;\cdots&amp;x_n\end{matrix}]^T\)</span>
and <span class="math inline">\(x_i\)</span> has the largest absolute
value among all <span class="math inline">\(x_j\)</span>'s.</p>
<p>Then <span class="math display">\[
|\lambda x_i|=|(A\vec{x})_i|=|\sum_{v_iv_j\in
E(G)}\sigma(v_iv_j)x_j|\leq\sum_{v_iv_j\in E(G)}|x_j|\leq\Delta(G)|x_i|
\]</span> So <span
class="math inline">\(|\lambda|\leq\Delta(G)\)</span></p>
<hr />
<p><strong>Thm A graph G is bipartite <span
class="math inline">\(\Leftrightarrow\)</span> its spectrum is symmetric
about the origin.</strong></p>
<p><font color='red'>(i.e. <span
class="math inline">\(\lambda_i\)</span> is an eigenvalue of G <span
class="math inline">\(\Leftrightarrow\)</span> <span
class="math inline">\(-\lambda_i\)</span> is an eigenvalue of
G)</font></p>
<hr />
<p><strong>Proof</strong></p>
<p><span class="math inline">\(\Rightarrow\)</span> Assume that G is
bipartite.Let A be The adjacency matrix of G. <span
class="math display">\[
A=\left[\begin{matrix}O&amp;B\\B^T&amp;O\end{matrix}\right]\ where\ B\
is\ the\ bipartite\ adjacency\ matrix.
\]</span> Let <span class="math inline">\(\lambda\)</span> be an
eigenvalue of G with <span
class="math inline">\(\vec{x}=(\begin{matrix}\vec{x}_1^T&amp;\vec{x}_2^T\end{matrix})^T\)</span></p>
<p>Then <span class="math display">\[
A\vec{x}=\left(\begin{matrix}O&amp;B\\B^T&amp;O\end{matrix}\right)\left(\begin{matrix}\vec{x}_1\\\vec{x}_2\end{matrix}\right)=\left(\begin{matrix}B\vec{x}_2\\B^T\vec{x}_1\end{matrix}\right)=\lambda\left(\begin{matrix}\vec{x}_1\\\vec{x}_2\end{matrix}\right)
\]</span> Then <span class="math display">\[
A\left(\begin{matrix}\vec{x}_1\\-\vec{x}_2\end{matrix}\right)=\left(\begin{matrix}O&amp;B\\B^T&amp;O\end{matrix}\right)\left(\begin{matrix}\vec{x}_1\\-\vec{x}_2\end{matrix}\right)=\left(\begin{matrix}-B\vec{x}_2\\B^T\vec{x}_1\end{matrix}\right)=-\lambda\left(\begin{matrix}\vec{x}_1\\-\vec{x}_2\end{matrix}\right)
\]</span> So <span class="math inline">\(-\lambda\)</span> is also an
eigenvalue of A.</p>
<p><span class="math inline">\(\Leftarrow\)</span> Let <span
class="math inline">\(\lambda_1\geq\cdots\geq\lambda_n\)</span> be all
eigenvalues of G.<font color='red'>(i.e. <span
class="math inline">\(\lambda_i=-\lambda_{n-i+1}\)</span>)</font></p>
<p>For any positive integer k, the matrix <span
class="math inline">\(A^k\)</span> has eigenvalues <span
class="math inline">\(\lambda_1^k,\lambda_2^k,\cdots,\lambda_n^k\)</span></p>
<p>Because G has symmetric specturm, it follows that <span
class="math inline">\(\lambda_1^k+\cdots+\lambda_n^k=0\)</span> if k is
an odd integer. <span class="math display">\[
0=\lambda_1^k+\cdots+\lambda_n^k=w_k(G)
\]</span> <font color='red'><span
class="math inline">\(w_k(G)\)</span> is the total number of close
k-walks.</font></p>
<p>So every closed walk have an even length. Therefore G is
bipartite.</p>
<hr />
<p><font color='red'>Problem: characterize graphs which satisfy #
positive eigenvalues=# negative eigenvalues</font></p>
]]></content>
      <categories>
        <category>Graph Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Operation Research</tag>
      </tags>
  </entry>
  <entry>
    <title>Lecture series on graph theory 7</title>
    <url>/2022/05/26/Graph7/</url>
    <content><![CDATA[<p><strong>Thm(Wilf,1967) Let G be a graph. Then <span
class="math inline">\(\chi(G)\leq\lambda_1(G)+1\)</span>, where <span
class="math inline">\(\chi(G)\)</span> is the chromatic
number</strong></p>
<p>A proper vertex coloring is a map <span
class="math inline">\(c:V(G)\to\mathbb{N}\)</span> s.t. <span
class="math inline">\(c(v_i)\not=c(v_j)\)</span> if <span
class="math inline">\(v_iv_j\in E(G)\)</span>.</p>
<p>A graph is a k-colorable if there exists a proper vertex coloring
<span class="math display">\[
c:V(G)\to\{1,2,\cdots,k\}=[k].
\]</span> <span id="more"></span></p>
<p>The chromatic number of a graph is the smallest integer k s.t. G is
k-colorable, denoted by <span
class="math inline">\(\chi(G)\)</span>.</p>
<p><strong>(Four color Theorem) Every planar graph is
4-colorable</strong></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-26_011010.4crr6n46e6o.jpg" /></p>
<p><strong>Lemma Let G be a graph. Then</strong> <span
class="math display">\[
\chi(G)\leq1+p
\]</span> <strong>where <span
class="math inline">\(p=\max\{\delta(H)|\text{H is an induced subgraph
of G}\}\)</span>, <font color='red'><span
class="math inline">\(\delta(G)\)</span> is minimum degree of
H.</font></strong></p>
<hr />
<p><strong>Proof of Lemma</strong></p>
<p>Use greedy algorithm:</p>
<p>Consider that <span
class="math inline">\(c:V(G)\to\mathbb{N}\)</span> such that color the
first vertex by the smallest value and color the k-th vertex by using
the smallest value that has not been used to color any of the first k-1
vertices that are adjacent to the k-th vertex. So it suffices to show
that there is an ordering of the vertices of G for which the greedy
algorithm gives an (1+p)-coloring of G.</p>
<p>Let <span class="math inline">\(x_n\)</span> be a vertex of <span
class="math inline">\(G=H_n\)</span> having degree at most
p.<font color='red'>（<span class="math inline">\(x_n\)</span> could be
the vertex of G with the degree <span
class="math inline">\(\delta(G)\)</span></font></p>
<p>By the definition of the value of p, the subgraph <span
class="math inline">\(H_{n-1}=G-x_n\)</span> has a vertex <span
class="math inline">\(x_{n-1}\)</span> of degree at most p. Continue
this process, let <span
class="math inline">\(H_{n-1}=G-\{x_n,x_{n-1},\cdots,x_{n-i+1}\}\)</span>
which again has a vertex of degree at most p, denoted by <span
class="math inline">\(x_{n-i}\)</span> for <span
class="math inline">\(i\in[n]\)</span></p>
<p>Then <span
class="math inline">\(H_{1}=G-\{x_n,x_{n-1},\cdots,x_{2}\}\)</span> is
an isolated vertex of degree 0.</p>
<p>Then the ordering <span
class="math inline">\(x_1,x_2,\cdots,x_n\)</span> is a desired one.</p>
<hr />
<p><strong>Proof of Wilf's Theorem</strong></p>
<p>Let G be a graph. For any induced subgraph H of G, <span
class="math inline">\(\delta(G)\leq
d(G)\leq\lambda_1(H)\leq\lambda_1(G)\)</span></p>
<p>Therefore, <span class="math inline">\(p=\max\{\delta(H)|\text{H is
an induced subgraph of G}\}\leq\lambda_1(G)\)</span></p>
<p>It follows from the above Lemma <span class="math display">\[
\chi(G)\leq 1+p\leq1+\lambda_1(G)
\]</span></p>
<hr />
<p>If G is k-colorable, let <span
class="math inline">\(c:V(G)\to[k]\)</span> be a k-colorable. <span
class="math inline">\(c^{-1}(i)\)</span>, the preimage of color i, is an
independent set. Independent number of G: <span
class="math inline">\(\alpha(G)\geq\max\left\{|c^{-1}(i)|\big|i\in[k]\right\}\geq
n/k\)</span>.</p>
<p>So <span class="math inline">\(\alpha(G)\geq n/\chi(G)\geq
n/(1+\lambda_1(G))\)</span>.</p>
<p><strong>Coloring, Integer flow</strong></p>
<p>Let G be a graph. An integer flow of G is an ordered pair <span
class="math inline">\((D,f)\)</span> where <span
class="math inline">\(D\)</span> is an orientation of <span
class="math inline">\(G\)</span> and <span
class="math inline">\(f\)</span> is an integer function <span
class="math inline">\(f:E(G)\to\mathbb{Z}\)</span> s.t. for every vertex
v of G <span class="math inline">\(\sum_{e\in E^{+}(v)}f(e)=\sum_{e\in
E^{-}(v)}f(e)\)</span> where <span
class="math inline">\(E^{+}(v)\)</span> is the set of all arcs with
<span class="math inline">\(v\)</span> as tail and <span
class="math inline">\(E^{-}(v)\)</span> is the set of all arcs with v as
head.</p>
<p><font color='red'>Recall: this incidence matrix of D is matrix is
<span class="math inline">\(\vec{B}\)</span> and <span
class="math inline">\(\vec{f}=&lt;f(e_1),\cdots,f(e_m)&gt;\in\mathbb{Z}^m\)</span> where
<span class="math inline">\(m=|E(G)|\)</span> , then <span
class="math inline">\(\vec{B}\cdot\vec{f}=0\)</span>. This vector <span
class="math inline">\(\vec{f}\)</span> is called an integer flow of
G.</font></p>
<p>An integer flow <span class="math inline">\((D,f)\)</span> is
nowhere-zero if <span class="math inline">\(f(e)\not=0\)</span> for all
edges of <span class="math inline">\(G\)</span> <font color='red'>i.e.
<span class="math inline">\(\vec{f}\)</span> has no zero
component.</font></p>
<p>An integer flow <span class="math inline">\((D,f)\)</span> is a
k-flow if <span class="math inline">\(|f(e)|&lt;k\)</span>
<font color='red'>i.e. every component of <span
class="math inline">\(\vec{f}\)</span> has absolute value less than
k.</font></p>
<p>If G has a k-flow, the orientation in the k-flow does not matter.</p>
<p><strong>Proposition Every Eulerian graph has nowhere 2-flow. A
connected graph with a nowhere-zero 2-flow is Eulerian.</strong></p>
<p><strong>Lemma Let <span class="math inline">\((D,f)\)</span> be a
flow of G, then for each <span class="math inline">\(X\subset
V(G)\)</span>, it holds that</strong><br />
<span class="math display">\[
\sum_{e\in E^{+}X}f(e)=\sum_{e\in E^{-}(X)}f(e)
\]</span></p>
<hr />
<p><strong>Proof</strong></p>
<p>Since <span class="math inline">\((D,f)\)</span> is a flow, it
follows that, for every vertex v, <span class="math display">\[
\sum_{e\in E^{+}(v)}f(e)=\sum_{e\in E^{-}(v)}f(e)
\]</span> So <span class="math inline">\(\sum_{e\in
E^{+}(v)}f(e)-\sum_{e\in E^{-}(v)}f(e)=0\)</span></p>
<p>Note that <span class="math display">\[
0=\sum_{v\in X}(\sum_{e\in E^{+}(v)}f(e)-\sum_{e\in
E^{-}(v)}f(e))=\sum_{e\in E^{+}(X)}f(e)-\sum_{e\in E^{-}(X)}f(e)
\]</span></p>
<hr />
<p><strong>Corollary A graph has a nowhere-zero integer flow <span
class="math inline">\(\Leftrightarrow\)</span> it has no
cut-edge.</strong></p>
<hr />
<p><strong>Proof</strong></p>
<p><span class="math inline">\(\Rightarrow\)</span> It follows directly
from the above lemma.</p>
<p><span class="math inline">\(\Leftarrow\)</span> Assume that G has no
cut-edge. Then every edge of G belongs to a cycle.</p>
<p>Let <span class="math inline">\(C=\{C_1,C_2,\cdots,C_k\}\)</span> be
a set of cycles covering all edges of G.</p>
<p>Every <span class="math inline">\(C_1\)</span> has nowhere-zero
2-flow <span class="math inline">\((D,f_1)\)</span> <span
class="math display">\[
f:E(G)\to\mathbb{N}\quad s.t.\ f(e)=\sum_{i=1}^k 2^if_i(e)
\]</span> Then <span class="math inline">\((D,f)\)</span> is a
nowhere-zero integer flow.</p>
<hr />
<p><strong>The Coloring-Flow Duality Theorem (Tutte)<br>Let G be a
2-connected planar graph. Then G is proper face k-colorable if and only
if G has a nowhere-zero k-flow. In other words, let <span
class="math inline">\(G^{\star}\)</span> be its planar dual. Then <span
class="math inline">\(G^{\star}\)</span> is proper k-colorable <span
class="math inline">\(\Leftrightarrow\)</span> G has a nowhere-zero
k-flow.</strong></p>
]]></content>
      <categories>
        <category>Graph Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Operation Research</tag>
      </tags>
  </entry>
  <entry>
    <title>Lecture series on graph theory 6</title>
    <url>/2022/05/24/Graph6/</url>
    <content><![CDATA[<p><strong>Interlacing Theorem</strong></p>
<p><strong>Thm(Eigenvalue Interlacing Theorem)<br>Let A be a symmetric
real <span class="math inline">\(n\times n\)</span> and let B be an m-th
principal submatrix<font color='red'>(obtained by deleting both i-th row
and i-th column for some n-m values of i)</font>. Suppose A has
eigenvalues <span
class="math inline">\(\lambda_1\geq\cdots\geq\lambda_n\)</span>, and B
has eigenvalues <span
class="math inline">\(\beta_1\geq\cdots\geq\beta_m\)</span>.Then</strong>
<span class="math display">\[
\lambda_k\geq\beta_k\geq\lambda_{k+(n-m)},\qquad k=1,2,\dots,m
\]</span> <span id="more"></span></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-24_122238.626hmwj3vik0.jpg" /></p>
<p>Further,if <span class="math inline">\(m=n-1\)</span>, then <span
class="math display">\[
\lambda_1\geq\beta_1\geq\lambda_2\geq\beta_2\geq\cdots\geq\lambda_i\geq\beta_i\geq\lambda_{i+1}\geq\cdots\geq\beta_{n-1}\geq\lambda_{n}
\]</span></p>
<hr />
<p><strong>Proof</strong></p>
<p>Without loss of generality, assume that <span
class="math inline">\(A=\left[\begin{matrix}B&amp;X\\X^T&amp;C\end{matrix}\right]\)</span>
and <span class="math inline">\(\lambda_i\vec{x}_i=A\vec{x}_i\)</span>
for <span class="math inline">\(i\in[n]\)</span> such that all <span
class="math inline">\(\vec{x}_i\)</span> are linearly independent and
normalized, and <span
class="math inline">\(\beta_j\vec{y}_j=B\vec{y}_j\)</span> for <span
class="math inline">\(j\in[m]\)</span> s.t. all <span
class="math inline">\(\vec{y}_j\)</span> are linearly independent and
normalized.</p>
<p>Let <span
class="math inline">\(V=span\{\vec{x}_k,\cdots,\vec{x}_n\}\)</span> and
<span
class="math inline">\(W=span\{\vec{y}_1,\cdots,\vec{y}_k\}\)</span>.
Extend <span class="math inline">\(W\)</span> to a subspace of <span
class="math inline">\(\mathbb{R}^n\)</span> s.t. <span
class="math display">\[
\widetilde{W}=\left\{\left.\left[\begin{matrix}\vec{w}\\0\end{matrix}\right]\right|\vec{w}\in
W\right\}\subset\mathbb{R}^n
\]</span> Then <span class="math inline">\(\dim(V)=n-k+1\)</span> and
<span class="math inline">\(\dim(\widetilde{W})=\dim(W)=k\)</span></p>
<p>Note that both <span class="math inline">\(V\subset
\mathbb{R}^n\)</span> and <span
class="math inline">\(\widetilde{W}\subset \mathbb{R}^n\)</span>, and
<span class="math display">\[
\dim(V)+\dim(\widetilde{W})=n-k+1+k=n+1&gt;\dim(\mathbb{R}^n)
\]</span> It follows that there exists a vector <span
class="math inline">\(\tilde{w}\)</span> which satisfies <span
class="math inline">\(\tilde{w}\in V\cap\widetilde{W}\)</span>.</p>
<p><font color='red'><span class="math inline">\(\lambda_k\)</span> is
the largest eigenvalue with an eigenvector in V; <span
class="math inline">\(\beta_k\)</span> is the smallest eigenvalue with
an eigenvector in W.</font></p>
<p>Therefore, <span class="math display">\[
\begin{aligned}
\lambda_k&amp;=\max\{\vec{x}^TA\vec{x}|\vec{x}\in V,||\vec{x}||=1\}\\
&amp;\geq\frac{\tilde{w}^TA\tilde{w}}{\tilde{w}^T\tilde{w}}=\left.\left[\begin{matrix}\vec{w}^T&amp;0\end{matrix}\right]\left[\begin{matrix}B&amp;X\\X^T&amp;C\end{matrix}\right]\left[\begin{matrix}\vec{w}\\0\end{matrix}\right]\right/\left[\begin{matrix}\vec{w}^T&amp;0\end{matrix}\right]\left[\begin{matrix}\vec{w}\\0\end{matrix}\right]\\
&amp;=\frac{\tilde{w}^TB\tilde{w}}{\tilde{w}^T\tilde{w}}\geq\min\{\vec{y}^TA\vec{y}|\vec{y}\in
W,||\vec{y}||=1\}=\beta_k
\end{aligned}
\]</span> So we have <span
class="math inline">\(\lambda_k\geq\beta_k\)</span>.</p>
<p>On the other hand, let <span
class="math inline">\(V=span\{\vec{x}_1,\cdots,\vec{x}_{k+n-m}\}\)</span>
<font color='red'>(i.e. <span
class="math inline">\(\lambda_{k+n-m}\)</span> is the smallest
eigenvalue with an eigenvector in V)</font> and <span
class="math inline">\(W=span\{\vec{y}_1,\cdots,\vec{y}_{k+n-m}\}\)</span>
<font color='red'>(i.e. <span class="math inline">\(\beta_k\)</span> is
the largest eigenvalue with an eigenvector in W)</font>.</p>
<p>Let <span
class="math inline">\(\widetilde{W}=\left\{\left.\left[\begin{matrix}\vec{w}\\0\end{matrix}\right]\right|\vec{w}\in
W\right\}\subset\mathbb{R}^n\)</span>. Then <span
class="math inline">\(\dim(V)=k+n-m\)</span> and <span
class="math inline">\(\dim(\widetilde{W})=m-k+1\)</span>.</p>
<p>Therefore, <span
class="math inline">\(\dim(V)+\dim(\widetilde{W})=n-k+1+k=n+1&gt;\dim(\mathbb{R}^n)\)</span></p>
<p>It follows that there exists a vector <span
class="math inline">\(\tilde{w}\)</span> which satisfies <span
class="math inline">\(\tilde{w}\in V\cap\widetilde{W}\)</span>. <span
class="math display">\[
\begin{aligned}
\lambda_{k+n-m}&amp;=\min\{\vec{x}^TA\vec{x}|\vec{x}\in
V,||\vec{x}||=1\}\\
&amp;\leq\frac{\tilde{w}^TA\tilde{w}}{\tilde{w}^T\tilde{w}}=\left.\left[\begin{matrix}\vec{w}^T&amp;0\end{matrix}\right]\left[\begin{matrix}B&amp;X\\X^T&amp;C\end{matrix}\right]\left[\begin{matrix}\vec{w}\\0\end{matrix}\right]\right/\left[\begin{matrix}\vec{w}^T&amp;0\end{matrix}\right]\left[\begin{matrix}\vec{w}\\0\end{matrix}\right]\\
&amp;=\frac{\tilde{w}^TB\tilde{w}}{\tilde{w}^T\tilde{w}}\leq\max\{\vec{y}^TA\vec{y}|\vec{y}\in
W,||\vec{y}||=1\}=\beta_k
\end{aligned}
\]</span> So <span
class="math inline">\(\lambda_{k+n-m}\leq\beta_k\)</span>.</p>
<p><strong>Bounding degree of induced subgraph</strong></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-24_162631.52l2utvvt9c0.jpg" /></p>
<p><strong>Thm(Huang,2019) Let H be an induced subgraph of the
n-dimensional hypercube <span class="math inline">\(Q_n\)</span>. If
<span
class="math inline">\(|V(H)|&gt;2^{n-1}=\frac{|V(Q)|}{2}\)</span>,then
<span class="math inline">\(\Delta (H)\geq\sqrt{n}\)</span></strong></p>
<p><font color='red'><strong>n-dimensional hypercube <span
class="math inline">\(Q_n\)</span></strong></font></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-24_163151.69hvpww08lw0.jpg" /></p>
<p><font color='red'>An n-dimensional hypercube <span
class="math inline">\(Q_n\)</span> is a graph with vertex set consisting
of all <span class="math inline">\(\{0,1\}\)</span>-sequences of length
n and two vertices are adjacent to each other if and only if the
two <span class="math inline">\(\{0,1\}\)</span>-sequences has exactly
one position different</font></p>
<p>The adjacency matrix of n-dimensional hypercube <span
class="math inline">\(Q_n\)</span> satisfies the following</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-24_163613.2kylzo171uo0.jpg" /></p>
<p>If we give every edge in n-dimensional hypercube <span
class="math inline">\(Q_n\)</span> a sign, we can get a signed adjacency
matrix.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-24_163633.44mufz49sju0.jpg" /></p>
<p><strong>Lemma The signed n-dimensional hypercube with adjacency
matrix <span class="math inline">\(S_n\)</span> has an eigenvalue <span
class="math inline">\(\sqrt{n}\)</span> of multiplicity <span
class="math inline">\(2^{n-1}\)</span> and eigenvalue <span
class="math inline">\(-\sqrt{n}\)</span> with multiplicity <span
class="math inline">\(2^{n-1}\)</span></strong></p>
<hr />
<p><strong>Proof</strong></p>
<p><strong>claim: <span
class="math inline">\(S_n^2=nI\)</span></strong></p>
<p>If <span class="math inline">\(n=1\)</span>, <span
class="math inline">\(S_1=\left[\begin{matrix}0&amp;1\\1&amp;0\end{matrix}\right],S_1^2=I\)</span>.
Assume that <span class="math inline">\(S_{n-1}^2=(n-1)I\)</span></p>
<p>Then <span class="math display">\[
S_n^2=\left[\begin{matrix}S_{n-1}&amp;I\\I&amp;-S_{n-1}\end{matrix}\right]\left[\begin{matrix}S_{n-1}&amp;I\\I&amp;-S_{n-1}\end{matrix}\right]=\left[\begin{matrix}S_{n-1}^2+I&amp;O\\O&amp;S_{n-1}^2+I\end{matrix}\right]=nI
\]</span> So claim holds.</p>
<p>Note that <span class="math inline">\(nI\)</span> has eigenvalues n.
Therefore, <span class="math inline">\(S_n\)</span> has eigenvalues
either <span class="math inline">\(\sqrt{n}\)</span> or <span
class="math inline">\(-\sqrt{n}\)</span>.</p>
<p>Since the trace of <span class="math inline">\(S_n\)</span> is 0,
<span class="math inline">\(S_n\)</span> has eigenvalues <span
class="math inline">\(\sqrt{n}\)</span> of multiplicity <span
class="math inline">\(2^{n-1}\)</span> and <span
class="math inline">\(-\sqrt{n}\)</span> with multiplicity <span
class="math inline">\(2^{n-1}\)</span>.</p>
<hr />
<p><strong>Proof of Huang's Theorem</strong></p>
<p>Let H be a induced subgraph of <span
class="math inline">\(Q_n\)</span> with more than <span
class="math inline">\(2^{n-1}\)</span> vertices. It suffices to show
that every subgraph H with exactly <span
class="math inline">\(2^{n-1}+1\)</span> vertices has maximum degree at
least <span class="math inline">\(\sqrt{n}\)</span>.</p>
<p>Let <span class="math inline">\((Q_n,\sigma)\)</span> be the signed
graph with adjacency matrix <span class="math inline">\(S_n\)</span> as
defined above. Then <span class="math inline">\((H,\sigma)\)</span> is a
signed induced subgraph of <span
class="math inline">\((Q_n,\sigma)\)</span>, whose adjacency matrix
<span class="math inline">\(A\)</span> is a <span
class="math inline">\((2^{n-1}+1)\)</span>-principle submatrix of <span
class="math inline">\(S_n\)</span>.</p>
<p>By the proposition <font color='red'>(i.e. <span
class="math inline">\(|\lambda(H,\sigma)|\leq\Delta (H)\)</span></font>
and the interlacing theorem <span class="math display">\[
\Delta
(H)\geq\lambda_1(A)\geq\lambda_{1+2^n-(2^{n-1}+1)}(S_n)=\lambda_{2^{n-1}}(S_n)=\sqrt{n}
\]</span> <span class="math inline">\(\lambda_1(A)\)</span> means the
largest eigenvalue of A.</p>
<hr />
<p><strong>Unfriendly partitions of subcubic graphs</strong></p>
<p>A graph G is subcubic if the maximum degree of G is at most 3.
<font color='red'>(<span
class="math inline">\(\Delta(G)\leq3\)</span>)</font></p>
<p><font color='red'>Conjecture (Pisanski and Fowler,2012)  <br>All
subcubic graphs except finitely many have median eigenvalues in the
interval<span class="math inline">\([-1,1]\)</span>. There exists a
constant c such that if <span
class="math inline">\(|V(G)|\geq|c|\)</span>, then <span
class="math inline">\(\lambda_{\lfloor\frac{\lambda+1}{2}\rfloor},\lambda_{\lceil\frac{\lambda+1}{2}\rceil}\in[-1,1]\)</span></font></p>
<p>Let G be a graph. A partition <span
class="math inline">\((X,Y)\)</span> of <span
class="math inline">\(V(G)\)</span> is unfriendly if every vertex has at
least the same number of neighbors in the other subset as in it
owns.</p>
<p><font color='red'>In other words, for every vertex v of G, <span
class="math inline">\(d_{G[x]}(v)\leq\frac{1}{2}d_G(v)\)</span>
and <span
class="math inline">\(d_{G[Y]}(v)\leq\frac{1}{2}d_G(v)\)</span></font></p>
<p>If <span class="math inline">\((X,Y)\)</span> is an unfriendly
partition of a subcubic graph <span class="math inline">\(G\)</span>,
then the maximum degree of the induced subgraph <span
class="math inline">\(G[X]\)</span> and <span
class="math inline">\(G[Y]\)</span> is at most 1.</p>
<p>An unfriendly partition <span class="math inline">\((X,Y)\)</span> is
unbalanced if <span class="math inline">\(|X|\not=|Y|\)</span></p>
<p>A partition is a bisection if <span
class="math inline">\(\Big||X|-|Y|\Big|\leq1\)</span></p>
<p><strong>Proposition Let G be a subcubic graph with n vertices. If G
has an unbalanced unfriendly partition, then</strong> <span
class="math display">\[
\lambda_{\lfloor\frac{n+1}{2}\rfloor}\in[-1,1]\qquad\lambda_{\lceil\frac{n+1}{2}\rceil}\in[-1,1]
\]</span></p>
<hr />
<p><strong>Proof</strong></p>
<p>Let G be a subcubic graph with n vertices and <span
class="math inline">\((X,Y)\)</span> be an unbalanced unfriendly
partition.</p>
<p>Without loss of generality, assume that <span
class="math inline">\(|X|&gt;|Y|\)</span></p>
<p>Then <span
class="math inline">\(\lambda_1(X)\leq\Delta(X)\leq1\)</span> and <span
class="math inline">\(\lambda_1(Y)\leq\Delta(Y)\leq1\)</span>. So both X
and Y are bipartite.</p>
<p>All eigenvalues of <span class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span> belong to <span
class="math inline">\([-1,1]\)</span>. Since <span
class="math inline">\(|X|&gt;|Y|\)</span>, it holds that <span
class="math display">\[
|X|\geq\left\lceil
\frac{n}{2}\right\rceil+1&gt;\frac{n}{2}&gt;\left\lceil
\frac{n}{2}\right\rceil-1\geq|Y|
\]</span> It follows that the interlacing theorem that <span
class="math display">\[
1\geq\lambda_1(X)\geq\lambda_{1+n-|X|}(G)\geq\lambda_{1+n-(\lceil\frac{n}{2}\rceil+1)}(G)\geq\lambda_{\left\lfloor
\frac{n+1}{2}\right\rfloor}(G)\geq\lambda_{\left\lceil
\frac{n+1}{2}\right\rceil}(G)\geq\lambda_{\left\lceil
\frac{n+1}{2}\right\rceil}(X)\geq-1
\]</span> The completes the proof.</p>
<hr />
<p><font color='red'>Q: Does every subcubic graph have an unbalanced
unfriendly partition?</font></p>
<p><font color='red'>A: No</font></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-26_005635.7fv36qke8og0.jpg" /></p>
<p><font color='red'>Conjecture(Ban&amp;Linal,2016) Every cubic graph
has an unfriendly almost balanced partition <span
class="math inline">\((X,Y)\)</span> s.t. <span
class="math inline">\(\Big||X|-|Y|\Big|\leq2\)</span></font></p>
]]></content>
      <categories>
        <category>Graph Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Operation Research</tag>
      </tags>
  </entry>
  <entry>
    <title>SVD-Dominnant Correlation</title>
    <url>/2024/06/05/SVD-Dominnant%20Correlation/</url>
    <content><![CDATA[<h2 id="dominant-correlation">Dominant Correlation</h2>
<p>Here we introduce one of the most useful interpretation of the SVD.
It is in terms of correlations among the columns of <span
class="math inline">\(X\)</span> and correlations among the rows of
<span class="math inline">\(X\)</span>. We claim that the matrix <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> given by the SVD can be seen as the
eigenvectors of a correlation matrix given by <span
class="math inline">\(XX^T\)</span> or <span
class="math inline">\(X^TX\)</span>. Now we try to explain this
claim.</p>
<span id="more"></span>
<p>We consider the structure of the correlation matrix <span
class="math inline">\(X^TX\)</span> firstly. Since the size of the data
matrix <span class="math inline">\(X\)</span> is <span
class="math inline">\(n\times m\)</span>, the size of the correlation
matrix <span class="math inline">\(X^TX\)</span> is <span
class="math inline">\(m\times m\)</span>. <span class="math display">\[
X^TX=\begin{bmatrix}\cdots&amp;x_1^T&amp;\cdots\\\cdots&amp;x_2^T&amp;\cdots\\\
&amp;\vdots&amp;\
\\\cdots&amp;x_m^T&amp;\cdots\end{bmatrix}\begin{bmatrix}\vdots&amp;\vdots&amp;\
&amp;\vdots\\x_1&amp;x_2&amp;\cdots&amp;x_m\\\vdots&amp;\vdots&amp;\
&amp;\vdots\end{bmatrix}=\begin{bmatrix}x_1^Tx_1&amp;x_1^Tx_2&amp;\cdots&amp;x_1^Tx_m\\x_2^Tx_1&amp;x_2^Tx_2&amp;\cdots&amp;x_2^Tx_m\\\vdots&amp;\vdots&amp;\
&amp;\vdots\\x_m^Tx_1&amp;x_m^Tx_2&amp;\cdots&amp;x_m^Tx_m\end{bmatrix}
\]</span> Based on the definition of <span
class="math inline">\(X^TX\)</span>, we realize that <span
class="math inline">\(X^TX\)</span> is a correlation matrix among the
columns of <span class="math inline">\(X\)</span>. All entries of <span
class="math inline">\(X^TX\)</span> are essentially an inner product
between two columns of the data matrix <span
class="math inline">\(X\)</span>. Interestingly, if the entry <span
class="math inline">\(x_i^Tx_j\)</span> is a large number, it means that
the columns <span class="math inline">\(x_i\)</span> and <span
class="math inline">\(x_j\)</span> are highly correlated. However, if
this number is very small, it means that the columns <span
class="math inline">\(x_i\)</span> and <span
class="math inline">\(x_j\)</span> are nearly orthogonal, which means
that the columns <span class="math inline">\(x_i\)</span> and <span
class="math inline">\(x_j\)</span> are almost different. Therefore, the
correlation matrix <span class="math inline">\(X^TX\)</span> can be seen
as a matrix that measures the correlation among the columns of <span
class="math inline">\(X\)</span>.</p>
<p>Based on the structure of the correlation matrix <span
class="math inline">\(X^TX\)</span>, <span
class="math inline">\(X^TX\)</span> is a symmetric and positive
semi-definite matrix. It guarantees that we have non negative real
eigenvalues, which have a direct correspondence on the singular values
of the data matrix <span class="math inline">\(X\)</span>. Now we assume
that the matrix <span class="math inline">\(X\)</span> have the singular
value decomposition <span class="math inline">\(X=U\Sigma V^T\)</span>.
We can derive that <span class="math display">\[
X^TX=V\Sigma^TU^TU\Sigma V^T=V\Sigma^2 V^T\Rightarrow X^TXV=V\Sigma^2
\]</span> which means that the columns of <span
class="math inline">\(V\)</span> are the eigenvectors of the correlation
matrix <span class="math inline">\(X^TX\)</span>. The eigenvalues of the
correlation matrix <span class="math inline">\(X^TX\)</span> are the
squares of the singular values of the data matrix <span
class="math inline">\(X\)</span>. The same is true for the correlation
matrix <span class="math inline">\(XX^T\)</span>. We can derive that
<span class="math display">\[
XX^T=U\Sigma V^TV\Sigma U^{T}=U\Sigma^2 U^T\Rightarrow XX^TU=U\Sigma^2
\]</span> which means that the columns of <span
class="math inline">\(U\)</span> are the eigenvectors of the correlation
matrix <span class="math inline">\(XX^T\)</span>. The eigenvalues of the
correlation matrix <span class="math inline">\(XX^T\)</span> are the
squares of the singular values of the data matrix <span
class="math inline">\(X\)</span>. Therefore, we can conclude that the
singular matrix <span class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> given by the SVD of the data matrix
<span class="math inline">\(X\)</span> can be seen as the eigenvectors
of the correlation matrix <span class="math inline">\(X^TX\)</span> or
<span class="math inline">\(XX^T\)</span>. The importance of this
columns of <span class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> is quantified by eigenvalues of <span
class="math inline">\(X^TX\)</span> or <span
class="math inline">\(XX^T\)</span>, which are the squares of the
singular values of the data matrix <span
class="math inline">\(X\)</span>.</p>
<p>Here we introduce the method of snapshots to compute the singular
value decomposition of the data matrix. If the data matrix <span
class="math inline">\(X\)</span> is so large that you cannot actually
compute it or store it all in memory, you can use the method of
snapshots. However, in the vast majority of cases, we don't recommend
that you compute the SVD using correlation matrices.</p>
<p>Although you cannot load all columns of <span
class="math inline">\(X\)</span> into memory, what you can do is calling
two columns at a time. For example, you can call the first column <span
class="math inline">\(x_1\)</span> and itself, then you will get the
inner product <span class="math inline">\(x_1^Tx_1\)</span>. After, you
can call the first column <span class="math inline">\(x_1\)</span> and
the second column <span class="math inline">\(x_2\)</span>, then you
will get the inner product <span class="math inline">\(x_1^Tx_2\)</span>
and so on. You can compute the inner product by calling two columns of
data matrix each time to get the correlation matrix <span
class="math inline">\(X^TX\)</span>. Here the resulting matrix is a
<span class="math inline">\(m\times m\)</span> matrix. It is small
enough that you can put it into memory and compute its
eigendecomposition. Based the above discussion, we know that the righ
singular vector <span class="math inline">\(V\)</span> and the singular
value <span class="math inline">\(\Sigma\)</span> can be goten by this
eigendecomposition. Then you can use this matrix to solve the left
singular vector <span class="math inline">\(U\)</span>.</p>
]]></content>
      <categories>
        <category>Reduced Order Model</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Lecture series on graph theory 4</title>
    <url>/2022/05/22/Graph4/</url>
    <content><![CDATA[<p><strong>Proposition. The adjacency matrix of a graph is a real
symmetric matrix.</strong></p>
<span id="more"></span>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-22_131847.16yosr9l8xs0.jpg" /></p>
<p><font color='red'>Here we broefly discuss the topic of random
matrices.<br>Probability of adjacent connection between <span
class="math inline">\(R_j\)</span> and <span
class="math inline">\(C_j\)</span>, <span
class="math inline">\(P(R_j\sim C_j)=\frac{1}{2}\)</span>, <span
class="math inline">\(\ P(R_j\not\sim C_j)=\frac{1}{2}\)</span></font>
<span class="math display">\[
P(A(G)\ is \ invertible)=P(M\ is\ invertible)
\]</span> <strong>Thm Let G be a graph. The number of walks from <span
class="math inline">\(v_i\)</span> to <span
class="math inline">\(v_j\)</span> of length k is equal to the <span
class="math inline">\((i,j)\)</span>-entry of <span
class="math inline">\((A(G))^k\)</span></strong></p>
<hr />
<p><strong>Proof</strong></p>
<p>Let <span class="math inline">\(A(G)=(a_{ij})_{n\times n}\)</span>.
Then <span class="math inline">\((i,j)\)</span>-entry of <span
class="math inline">\((A(G))^k\)</span> is given by <span
class="math display">\[
[(A(G))^k]_{ij}=\sum a_{ii_1}a_{i_1i_2}\cdots a_{i_{k-1}j}
\]</span> where the sum ranges over all sequences <span
class="math inline">\((i_1,i_2,\dots,i_{k-1})\)</span> with <span
class="math inline">\(i_t\in[n]=\{1,2,\dots,n\}\)</span> <span
class="math display">\[
\begin{aligned}
(A(G))^2_{ij}&amp;=(A(G)\cdot
A(G))_{ij}=\sum_{i_1=1}^na_{ii_1}a_{i_1j}\\
(A(G))^3_{ij}&amp;=((A(G))^2\cdot
A(G))_{ij}=\sum_{i_2=1}^n(\sum_{i_1=1}^na_{ii_1}a_{i_1i_2})a_{i_2j}=\sum_{i_1,i_2=1}^{n}a_{ii_1}a_{i_1i_2}a_{i_2j}\\
\end{aligned}
\]</span> By the definition of adjacency matrix, <span
class="math inline">\(a_j=1\Leftrightarrow v_iv_j\in E(G)\)</span></p>
<p>It follows that the summand <span
class="math inline">\(a_{ii_1}a_{i_1i_2}\cdots a_{i_{k-1}j}\)</span> is
1 <span class="math inline">\(\Leftrightarrow\ v_iv_{i_1}v_{i_2}\cdots
v_{i_{k-1}}v_j\)</span> is a walk of length k. Hence the summing over
all <span class="math inline">\((i_1,\cdots,i_{k-1})\)</span> just gives
the total number of walks of length k from <span
class="math inline">\(v_i\)</span> to <span
class="math inline">\(v_j\)</span></p>
<hr />
<p>A(G) is <span class="math inline">\(n\times n\)</span> symmetric
matrix. All n eigenvalues of A(G) are real. Assume the n eigenvalues
satisfy <span class="math display">\[
\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_n
\]</span> <font color='red'>Note that <span
class="math inline">\(\lambda_1+\lambda_2+\cdots+\lambda_n=0\)</span>,some
eigenvalues could be negative.<br>If G is a bipartite graph, the
eigenvalues are symmetric about origin.</font></p>
<p><strong>Thm Let G be a graph with eigenvalues <span
class="math inline">\(\lambda_1,\cdots,\lambda_n\)</span>. Then the
number of closed walks in G of length k is given by</strong> <span
class="math display">\[
w_G(k)=\lambda_1^k+\cdots+\lambda_n^k
\]</span></p>
<hr />
<p><strong>Proof</strong></p>
<p>Let G be a graph with adjacency matrix A(G). Then it follows from the
above theorem that <span class="math display">\[
w_G(k)=\sum_{i=1}^n[(A(G))^k]_{ii}=tr(A(G)^k)
\]</span> Since A(G) has <span class="math inline">\(\lambda_i\)</span>
as an eigenvalue with <span
class="math inline">\(i\in\{1,2,\cdots,n\}\)</span>, the matrix <span
class="math inline">\(A(G)^k\)</span> has eigenvalue <span
class="math inline">\(\lambda_i^k\)</span>for <span
class="math inline">\(i\in\{1,2,\cdots,n\}\)</span></p>
<p><font color='red'><span
class="math inline">\(A(G)\vec{v_i}=\lambda_i\vec{v_i},\quad
A(G)^k\vec{v_i}=A(G)^{k-1}A(G)\vec{v_i}=A(G)^{k-1}\lambda\vec{v_i}=\lambda_i^k\vec{v_i}\)</span></font></p>
<p>Then, <span
class="math inline">\(tr(A(G)^k)=\sum_{i=1}^n\lambda_i^k\)</span>, so
<span
class="math inline">\(w_G(k)=tr(A(G)^k)=\sum_{i=1}^n\lambda_i^k\)</span></p>
<hr />
<p><strong>Incidence matrix and Laplacian matrix</strong></p>
<p>Let G be a graph s.t. <span
class="math inline">\(V(G)=\{v_1,v_2,\cdots,v_n\}\)</span> and <span
class="math inline">\(E(G)=\{e_1,e_2,\cdots,e_m\}\)</span> <span
class="math display">\[
B(G)=(b_{ij})_{n\times m}\ where\
b_{ij}=\left\{\begin{aligned}&amp;1\quad v_i\ is\ incident\ with\ e_j\\
&amp;0\quad v_i\ isn&#39;t\ incident\ with\ e_j\end{aligned}\right.
\]</span> <img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-22_162156.x0o95v76evk.jpg" /></p>
<p><font color='red'>We can find that evey column has two non-zero
entries.</font></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-22_162610.6c5zcmwtfbc0.jpg" /></p>
<p><font color='red'>We can find that each column of <span
class="math inline">\(\vec{B}\)</span> has one entry of 1, one entry of
-1 and other entry of 0.Therefore, add all entries in column and get
0</font></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-22_163017.jpg" /></p>
<p><span class="math inline">\(D(G)\)</span> is a diagonal matrix whose
diagonal entries are the degrees of vertices.</p>
<p>Laplacian matrix of G: <span class="math inline">\(L(G)=
D(G)-A(G)=\vec{B}\cdot\vec{B}^T\)</span></p>
<p><strong>Proposition. Let G be a graph. Then L(G) is positive
semi-definite and consequently, all eigenvalues of L(G) are real and
non-negative.</strong> <span class="math display">\[
\beta_1\geq\beta_2\geq\cdots\geq\beta_n\geq0
\]</span> <span class="math inline">\(\tau(G)\)</span>= the total number
of spanning trees in G.</p>
<p><strong>Thm(Matrix Tree Theorem) Let G be a graph with vertices <span
class="math inline">\(v_1,\dots,v_n\)</span>. Then <span
class="math inline">\(\tau(G)=det(L_0(G))\)</span> where <span
class="math inline">\(L_0(G)\)</span> is obtained from <span
class="math inline">\(L(G)\)</span> by deleting the i-th row and i-th
column for any <span
class="math inline">\(i\in[n]=\{1,2,\dots,n\}\)</span></strong></p>
<hr />
<p><strong>Proof.</strong></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-22_174049.4vsowf7szwq0.jpg" /></p>
<p>If T is a spanning tree, either <span class="math inline">\(e\in
T\)</span> or <span class="math inline">\(e\not\in T\)</span>.</p>
<p>An edge e of G is said to be contracted if it is deleted and its ends
are identified, the resulting graph is denoted by <span
class="math inline">\(G\cdot e\)</span>.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-22_174508.6er13gzch2o0.jpg" /></p>
<p>If <span class="math inline">\(e\not\in T\)</span>, T is a spanning
tree of G-e; if <span class="math inline">\(e\in T\)</span>, <span
class="math inline">\(T\cdot e\)</span> is a spanning tree of <span
class="math inline">\(G\cdot e\)</span>.</p>
<p>Therefore, <span class="math inline">\(\tau(G)=\tau(G-e)+\tau(G\cdot
e)\)</span></p>
<p>Use induction on the number of edges of G.</p>
<p>If G has no edges and n=1, so <span
class="math inline">\(\tau(G)=1=det(L_0(G))\)</span> where <span
class="math inline">\(L_0(G)\)</span> is a <span
class="math inline">\(0\times 0\)</span> matrix with determinant 1 by
convenience.</p>
<p>If <span class="math inline">\(n&gt;1\)</span>, then <span
class="math inline">\(\tau(G)=0\)</span>, and <span
class="math inline">\(L_0(G)\)</span> is a 0-matrix in which every entry
is 0. So <span class="math inline">\(\tau(G)=det(L_0(G))\)</span> .</p>
<p>Therefore, the statement holds for <span
class="math inline">\(|E(G)|=0\)</span>.</p>
<p>Assume the statement holds all graph with number of edges less than
<span class="math inline">\(|E(G)|\)</span>.</p>
<p>By recordering the vertices of G, we may assume that <span
class="math inline">\(i=1\)</span> and <span
class="math inline">\(v_1v_2\in E(G)\)</span>, Let <span
class="math inline">\(e=v_1v_2\)</span>. Then <span
class="math inline">\(\tau(G)=\tau(G-e)+\tau(G\cdot e)\)</span>.</p>
<p>By inductive hypothesis, <span
class="math inline">\(\tau(G-e)=det(L_0(G-e))\)</span> and <span
class="math inline">\(\tau(G\cdot e)=det(L_0(G\cdot e))\)</span>.</p>
<p>Assume <span
class="math inline">\(L_0(G)=\begin{pmatrix}d_2&amp;P\\P^T&amp;R\end{pmatrix}\)</span>
obtained from <span
class="math inline">\(L(G)=\left(\begin{array}{c:c:c}d_1&amp;1&amp;\cdots\\\hdashline1&amp;d_2&amp;P\\\hdashline\vdots&amp;P^T&amp;R\end{array}\right)\)</span>
by deleting the first row and the first column.</p>
<p>Then <span
class="math inline">\(L_0(G-e)=\begin{pmatrix}d_2-1&amp;P\\P^T&amp;R\end{pmatrix}\)</span>
and <span class="math inline">\(L_0(G\cdot e)=R\)</span></p>
<p>Note that <span
class="math inline">\(L_0(G)=L_0(G-e)+\begin{pmatrix}1&amp;\vec{0}\\\vec{0}^T&amp;R\end{pmatrix}\)</span></p>
<p>Using the Laplace expansion along the first row, <span
class="math display">\[
det(L_0(G))=det(L_0(G-e))+det(L_0(G\cdot e))=\tau(G-e)+\tau(G\cdot
e)=\tau(G)
\]</span></p>
<hr />
<p><strong>Thm Let G be a connected graph with n vertices. Suppose that
the eigenvalues of L(G) are <span
class="math inline">\(\mu_1\geq\mu_2\geq\cdots\geq\mu_{n-1}\geq\mu_n=0\)</span>.
Then</strong> <span class="math display">\[
\tau(G)=\frac{1}{n}\mu_1\mu_2\cdots\mu_{n-1}
\]</span></p>
<hr />
<p><strong>Proof</strong></p>
<p>Let <span class="math inline">\(L(G)\)</span> be the Laplacian matrix
and let <span class="math display">\[
det(L(G)-\lambda
I)=(\mu_1-\lambda)(\mu_2-\lambda)\cdots(\mu_n-\lambda)=-\lambda(\mu_1-\lambda)(\mu_2-\lambda)\cdots(\mu_{n-1}-\lambda)
\]</span> So the cofficient of <span
class="math inline">\(\lambda\)</span> term is <span
class="math inline">\(-\mu_1\mu_2\cdots\mu_{n-1}\)</span>. Note that,
consider <span class="math inline">\(L(G)-\lambda I\)</span>.</p>
<p>Add all rows of <span class="math inline">\(L(G)-\lambda I\)</span>
except the first row to the first row, and let <span
class="math inline">\(M(\lambda)\)</span> be the new resulting
matrix.</p>
<p>Then <span class="math inline">\(det(M(\lambda))=det(L(G)-\lambda
I)\)</span> and the first row of <span
class="math inline">\(M(\lambda)=[-\lambda,-\lambda,\cdots,-\lambda]\)</span>.</p>
<p>Let <span class="math inline">\(N(\lambda)\)</span> be the resulting
matrix obtained from <span class="math inline">\(M(\lambda)\)</span> by
factoring out <span class="math inline">\(-\lambda\)</span>.</p>
<p>Then <span class="math inline">\(M(\lambda)=-\lambda
N(\lambda)\)</span> and <span
class="math inline">\(det(M(\lambda))=-\lambda det(N(\lambda))\)</span>,
So <span class="math display">\[
det(L(G)-\lambda I)=-\lambda det(N(\lambda))
\]</span> Therefore, the cofficient of <span
class="math inline">\(\lambda\)</span> term in <span
class="math inline">\(det(L(G)-\lambda I)\)</span>is equal to <span
class="math inline">\(-det(N(0))\)</span>.</p>
<p>Add all columns of <span class="math inline">\(N(0)\)</span> except
the first column to the first column.</p>
<p><font color='red'>(Note that <span
class="math inline">\(i^{th}\)</span>-row <span
class="math inline">\((i&gt;1)\)</span> of <span
class="math inline">\(N(0)\)</span> is the same as the  <span
class="math inline">\(i^{th}\)</span>-row <span
class="math inline">\((i&gt;1)\)</span> of <span
class="math inline">\(L(G)\)</span></font></p>
<p>Then the first column of the new matrix is <span
class="math inline">\(\left[\begin{matrix}n&amp;0&amp;0&amp;\cdots&amp;0\end{matrix}\right]^T\)</span>.</p>
<p>Use Laplace expansion along the <span
class="math inline">\(1^{st}\)</span> column, <span
class="math inline">\(det(N(0))=n\cdot det(L_0(G))\)</span> <span
class="math display">\[
n\cdot det(L_0(G))=(-1)\cdot(-\mu_1\mu_2\cdots\mu_{n-1})\Rightarrow
det(L_0(G))=\frac{1}{n}\mu_1\mu_2\cdots\mu_{n-1}
\]</span> By previous theorem, <span class="math display">\[
\tau(G)=\frac{1}{n}\mu_1\mu_2\cdots\mu_{n-1}
\]</span></p>
<hr />
]]></content>
      <categories>
        <category>Graph Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Operation Research</tag>
      </tags>
  </entry>
  <entry>
    <title>Lecture series on graph theory 8</title>
    <url>/2022/05/27/Graph8/</url>
    <content><![CDATA[<p><strong>Proof of the coloring-flow duality theorem</strong></p>
<span id="more"></span>
<p><span class="math inline">\(\Rightarrow\)</span> Assume that G is
face k-colorable (proper coloring)</p>
<p>We need to show that G has a nowhere-zero k-flow <span
class="math inline">\((D,f)\)</span> <span class="math display">\[
\begin{aligned}
(i.e.
&amp;f:E(G)\to\{-(k-1),-(k-2),\cdots,-1,1,\cdots,k-2,k-1\}\\&amp; s,t,\forall
v\in V(G),\sum_{e\in E^{+}(v)}f(e)=\sum_{e\in E^{-}(v)}f(e))
\end{aligned}
\]</span> Let <span class="math inline">\(F(G)\)</span> be the set of
all faces of G and let <span class="math inline">\(c:F(G)\to[k]\)</span>
be a face coloring.</p>
<p>Assign an orientation <span class="math inline">\(D\)</span> to <span
class="math inline">\(E(G)\)</span>: each edge <span
class="math inline">\(e\in E(G)\)</span> is oriented such that the face
with a smaller color index is on the right side of the arc.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-27_121553.4l885wgltpq0.jpg" /></p>
<p>Define a function <span class="math inline">\(f:E(G)\to[k-1]\)</span>
such that <span class="math inline">\(f(e)\)</span> is the absolute
value of the difference of the colors of two faces incident with e.</p>
<p><font color='blue'><strong>claim: <span
class="math inline">\((D,f)\)</span> is a nowhere-zero
k-flow</strong></font></p>
<hr />
<p><strong><font color='blue'>Proof of claim:</font></strong></p>
<p><font color='blue'>Clearly, <span
class="math inline">\(0&lt;f(e)&lt;k\)</span>, because the smallest
color index is 1 and the largest color index is k;and two faces share
edges have different color indices.So f is nowhere-zero.</font></p>
<p><font color='blue'>Let v be vertex of G and <span
class="math inline">\(v_1,v_2,\cdots,v_d\)</span> be all neighbors of v
appearing around v in a clockwise order.Let <span
class="math inline">\(f_i\)</span> be the face containing edges <span
class="math inline">\(vv_1\)</span> and <span
class="math inline">\(vv_{i+1}\)</span> . Note that, for any <span
class="math inline">\(i\not=j,f_i\not=f_j\)</span> because G is
2-connected.</font></p>
<p><font color='blue'>Define</font> <span class="math display">\[
\textcolor{blue}{\epsilon_i=\left\{\begin{aligned}&amp;-1,\quad if\ v\to
v_i\ in\ D\\&amp;1,\quad if\ v_i\to v\ in\ D
\end{aligned}\right.}
\]</span> <font color='blue'>Then, for each <span
class="math inline">\(i\in[d]\)</span></font> <span
class="math display">\[
\textcolor{blue}{c(f_i)=c(f_d)+\sum_{j=1}^i\epsilon_if(vv_j) \tag{*}}
\]</span> <img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-27_124414.4gsbkbgif9u0.jpg" />
<span class="math display">\[
\begin{aligned}&amp;\textcolor{blue}{c(f_1)=c(f_d)}+\textcolor{red}{f(vv_1)}\\&amp;\textcolor{blue}{c(f_2)=c(f_1)-f(vv_2)=c(f_d)+f(vv_1)-f(vv_2)=c(f_d)+\sum_{j=1}^2\epsilon_jf(vv_j)}\end{aligned}
\]</span> <font color='blue'>From (*), <span
class="math inline">\(c(f_i)=c(f_d)+\sum_{j=1}^i\epsilon_if(vv_j)\)</span>.
It follows that</font> <span class="math display">\[
\textcolor{blue}{\sum_{j=1}^d\epsilon_jf(vv_j)=0\Rightarrow\sum_{e\in
E^{+}(v)}f(e)=\sum_{e\in E^{-}(v)}f(e)}
\]</span></p>
<hr />
<p><span class="math inline">\(\Leftarrow\)</span> Assume that G has a
nowhere-zero k-flow <span class="math inline">\((D,f)\)</span>.</p>
<p>Define a map <span
class="math inline">\(c:F(G)\to\{1,2,\cdots,k-1\}\)</span></p>
<p>Choose an arbitrary face <span class="math inline">\(f_0\)</span> and
let <span class="math inline">\(c(f_0)=1\)</span>. For each arc <span
class="math inline">\(e_i\)</span> in D, let <span
class="math inline">\(f_i&#39;\)</span> and <span
class="math inline">\(f_i&#39;&#39;\)</span> be two faces incident with
<span class="math inline">\(e_i\)</span>. If one of <span
class="math inline">\(f_i&#39;\)</span> and <span
class="math inline">\(f_i&#39;&#39;\)</span> is colored, then the color
of another face is given by the following equality <span
class="math display">\[
c(f_i&#39;&#39;)=c(f_i&#39;)+f(e_i)(mod\ k)
\]</span> It suffices to show that the vertex coloring is
well-defined.<font color ='red'>In other words, the process does not
color one face by two or more different colors.</font></p>
<p>Let <span class="math inline">\(f_1\)</span> be uncolored face
incident with two colored faces <span class="math inline">\(f_2\)</span>
and <span class="math inline">\(f_3\)</span>.Let <span
class="math inline">\(e_i\)</span> be the edge on the boundary of <span
class="math inline">\(f_1\)</span> and <span
class="math inline">\(f_i\)</span> for <span
class="math inline">\(i=2,3\)</span>. Without loss of generality, let
<span class="math inline">\(f_i\)</span> be on the left side of the arcs
<span class="math inline">\(e_2\)</span> and <span
class="math inline">\(e_3\)</span>.</p>
<p><font color='blue'><strong>claim: <span
class="math inline">\(c(f_2)+f(e_2)\equiv c(f_3)+f(e_3)(mod\
k)\)</span></strong></font></p>
<hr />
<p><font color='blue'><strong>Proof of claim:</strong></font></p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picture@main/20220226/2022-05-27_164800.1de6ecdbpaps.jpg" /></p>
<p><font color='blue'>Consider the dual graph <span
class="math inline">\(G^\star\)</span> of G.</font></p>
<p><font color='blue'>The vertex subset <span
class="math inline">\(X^\star=\{f\in V(G^\star)\big|f\in F(G)\ is\
colored\ already\}\)</span> induces a connected subgraph of <span
class="math inline">\(G^\star\)</span>. Thus, there is a cycle <span
class="math inline">\(C^\star\)</span> of <span
class="math inline">\(G^\star\)</span> containing <span
class="math inline">\(f_1,f_2\)</span> and <span
class="math inline">\(f_3\)</span>.The cycles <span
class="math inline">\(C^\star\)</span> also contains the edge <span
class="math inline">\(f_1f_2\)</span> and<span
class="math inline">\(f_1f_3\)</span>, <span
class="math inline">\(V(C^\star)\backslash \{f_1\}\subset
X^\star\)</span>.</font></p>
<p><font color='blue'>Since all edges of G corresponding to the edge of
<span class="math inline">\(C^\star\)</span>​ separate G into two
parts</font> <font color='red'>(i.e. these edges form an
edge-cut)</font><font color='blue'>, Let X be the set of vertices in one
part. It follows from a lemma that </font> <span class="math display">\[
\textcolor{blue}{\sum_{e\in E^{+}(v)}f(e)-\sum_{e\in E^{-}(v)}f(e)=0}
\]</span> <font color='blue'>So the clain follows.</font></p>
<hr />
<p>This completes the whole proof.</p>
<p><font color='red'>Recall: Every Eulerian graph G has a cycle
decomposition: edges of G can be decomposed into edge-disjoint
cycles.</font></p>
<p>A cycle cover of a graph is a family of cycles which cover all edges
of G.</p>
<p>A graph has a cycle cover <span
class="math inline">\(\Leftrightarrow\)</span> it has even-subgraph
cover.</p>
<p><strong>Thm If a graph G has k-even-subgraph cover, then G has a
nowhere-zero <span class="math inline">\(2^k\)</span>-flow</strong></p>
<hr />
<p><strong>Proof</strong></p>
<p>For each even-subgraph <span class="math inline">\(H_i\)</span>, it
has a 2-flow <span class="math inline">\((D,f_i)\)</span>.</p>
<p>So <span class="math inline">\((D,f)\)</span> is a nowhere-zero <span
class="math inline">\(2^k\)</span>-flow where <span
class="math inline">\(f=\sum_{e\in E(G)}2^{i-1}f_i(e)\)</span></p>
<hr />
<p><strong>(8-flow theorem,Jaeger) Every bridgeless graph has
nowhere-zero 8-flow</strong></p>
<p><strong>(Splitting Lemma) Let G be a connected graph without
cut-edge.If G has a vertex v of degree at least four. Then there are two
edges <span class="math inline">\(e_1\)</span> and <span
class="math inline">\(e_2\)</span> incident with v s.t. the resulting
graph after splitting <span class="math inline">\(e_1\)</span> and <span
class="math inline">\(e_2\)</span> from v remains to be connected and
without cut-edge.</strong></p>
<p><font color='red'>(Cycle double cover conjecture,1970s) Every
connected cubic graph without cut-edge has a family of cycles covering
every edge exactly twice.</font></p>
<p>A proper k-edge-coloring of a graph G is a map <span
class="math inline">\(c:E(G)\to[k]\)</span> s.t. <span
class="math inline">\(c(e)\not=c(e&#39;)\)</span> if <span
class="math inline">\(e\)</span> and <span
class="math inline">\(e&#39;\)</span> have a common end vertex.</p>
<p><strong>Thm Every 3-edge-colorable cubic graph has a family of cycles
which cover every edge exactly twice</strong></p>
<hr />
<p><strong>Proof</strong></p>
<p>Let <span class="math inline">\(c:E(G)\to[3]\)</span>. Then <span
class="math inline">\(c^{-1}(1)\cup c^{-1}(2)\)</span> is a family of
cycles, <span class="math inline">\(c^{-1}(1)\cup c^{-1}(3)\)</span> is
a family of cycles, <span class="math inline">\(c^{-1}(2)\cup
c^{-1}(3)\)</span> is a family of cycles. <span class="math display">\[
l_{e_1}=c^{-1}(1)\cup c^{-1}(2),l_{e_2}=c^{-1}(1)\cup
c^{-1}(3),l_{e_3}=c^{-1}(2)\cup c^{-1}(3)
\]</span> So <span class="math inline">\(l_{e_1}\cup l_{e_2}\cup
l_{e_3}\)</span> is a family of cycles which cover every edge exactly
twice.</p>
<hr />
<p><font color='blue'>(3-flow conjecture) Every 5-edge connected graph
has a nowhere-zero 3-flow.</font></p>
<p><font color='red'>A graph is a k-edge-connected if for any two
vertices x ang y, the graph has k edge-disjoint paths joining x and
y.</font></p>
<p><font color='blue'>(Tutte-Nashwillam Theorem) Every 2k-edge-connected
graph has a k edge-disjoint spanning trees.</font></p>
<p><font color='blue'>(Lovasz,Wu,Thomassen,Zhang,2012) Every
6-edge-connected graph has a nowhere-zero 3-flow.</font></p>
<p><font color='blue'>(5-flow conjecture.1950s) Every bridgeless graph
has a nowhere-zero 5-flow</font></p>
<p><font color='blue'>(Seymour,1980) Every bridgeless graph has a
nowhere-zero 6-flow.</font></p>
<p><font color='blue'>For cubic graph,cycle double cover conjecture<span
class="math inline">\(\approx\)</span> the strong embedding
conjecture</font></p>
<p><font color='blue'>(Strong Embedding Conjecture,1980s) Every
2-connected graph can be embedded on a closed surface such that every
face is bounded by a cycle.</font></p>
]]></content>
      <categories>
        <category>Graph Theory</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Operation Research</tag>
      </tags>
  </entry>
  <entry>
    <title>SVD-Eckart-Young-Mirsky theorem</title>
    <url>/2024/05/28/SVD-Eckart_Young_Mirsky/</url>
    <content><![CDATA[<p>Firstly, we introduce three different kinds of matrix norms and based
on singular value decomposition, verify that these norms are only
related to singular values.</p>
<ol type="1">
<li>2-norm (Spectral norm)</li>
</ol>
<p><span class="math display">\[
\left\Vert
A\right\Vert_2=\sqrt{\lambda_{max}(A^TA)}=\sqrt{\lambda_{max}(U\Sigma
V^TV\Sigma U^T)}=\sqrt{\lambda_{max}(\Sigma^2)}=\sigma_1
\]</span></p>
<ol type="1">
<li>Frobenius norm</li>
</ol>
<p><span class="math display">\[
\left\Vert A\right\Vert_F=\sqrt{tr(A^TA)}=\sqrt{tr(U\Sigma V^TV\Sigma
U^T)}=\sqrt{tr(\Sigma^2)}=\sqrt{\sum_{i=1}^m\sigma_i^2}
\]</span></p>
<ol type="1">
<li>Nuclear norm</li>
</ol>
<p><span class="math display">\[
\left\Vert
A\right\Vert_N=\sigma_1+\sigma_2+\dots+\sigma_m=\sum_{i=1}^m\sigma_i
\]</span></p>
<span id="more"></span>
<p>Based on the results of matrix norms, we realize that the above three
matrix norms only depend on the singular values of the matrix <span
class="math inline">\(A\)</span>. Now we claim that the product with
orthogonal matrices will not change the singular values of the original
matrix. For simplicity, we call the left orthogonal matrix and right
orthogonal matrix as <span class="math inline">\(P\)</span> and <span
class="math inline">\(Q\)</span>, respectively. Based on the singular
value decomposition of <span class="math inline">\(A\)</span>, we can
derive the following results about the product of matrix <span
class="math inline">\(A\)</span> and orthogonal matrices <span
class="math inline">\(P,\ Q\)</span>, <span class="math display">\[
PAQ=PU\Sigma V^TQ=U_1\Sigma V_1^T,\ U_1=PU,\ V_1=Q^TV
\]</span> where <span
class="math inline">\(U_1U_1^T=PUU^TP^T=I=U_1^TU_1\)</span> means <span
class="math inline">\(U_1\)</span> is an orthogonal matrix. The same is
true for <span class="math inline">\(V_1\)</span>. We claim that the
singular values of <span class="math inline">\(PAQ\)</span> is the same
as the singular values of <span class="math inline">\(A\)</span>. So we
realize that the product with orthogonal matrices does not affect matrix
norm.</p>
<p><strong>Thm(Eckart-Young-Mirsky Theorem)</strong> The optimal rank-r
approximation to <span class="math inline">\(A\)</span>, under the
spectral norm and the Frobenius norm, respectively, is given by the
rank-r SVD truncation <span class="math inline">\(\tilde{A}\)</span>,
<span class="math display">\[
argmin_{\hat{A},\ s.t.\ rank(\hat{A})=r}\left\Vert
A-\hat{A}\right\Vert_F=\tilde{A}\quad argmin_{\hat{A},\ s.t.\
rank(\hat{A})=r}\left\Vert A-\hat{A}\right\Vert_2=\tilde{A}
\]</span> where <span
class="math inline">\(\tilde{A}=\tilde{U}\tilde{\Sigma}\tilde{V}^T\)</span>
is the rank-r SVD truncation of <span class="math inline">\(A\)</span>.
Again, <span class="math inline">\(\tilde{U}\)</span> and <span
class="math inline">\(\tilde{V}\)</span> denote the first <span
class="math inline">\(r\)</span> leading columns of <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span>, respectively, and <span
class="math inline">\(\tilde{\Sigma}\)</span> is the <span
class="math inline">\(r\times r\)</span> leading submatrix of <span
class="math inline">\(\Sigma\)</span>.</p>
<p>Proof.</p>
<ol type="1">
<li><p>One needs to show that if we have a matrix approximation <span
class="math inline">\(B\)</span> whose rank is <span
class="math inline">\(r\)</span> and size is <span
class="math inline">\(n\times m\)</span>, then <span
class="math inline">\(\left\Vert A-B\right\Vert_2\geq \left\Vert
A-\tilde{A}\right\Vert_2\)</span> holds. This can be done as
follows.</p>
<p>Here we introduce the concept of matrix kernel before the discussion.
The kernel of matrix <span class="math inline">\(A\)</span> is defined
as <span class="math display">\[
   \mathcal{N}(A)=\{x\in \mathbb{R}^n:Ax=0\}
\]</span> Therefore, we realize that the kernel of matrix <span
class="math inline">\(\mathcal{N}(A)\)</span> actually means the
solution space of the linear equation <span
class="math inline">\(Ax=0\)</span>. Based on the linear equation
theory, we know that when the rank of matrix <span
class="math inline">\(B\)</span> is <span
class="math inline">\(r\)</span>, the dimension of <span
class="math inline">\(\mathcal{N}(B)\)</span> is <span
class="math inline">\(n-r\)</span>. Here we consider the linear space
<span class="math inline">\(V_{r+1}\)</span> spanned by the first <span
class="math inline">\(r+1\)</span> leading right singular vectors <span
class="math inline">\(v_1,\ v_2,\dots,v_{r+1}\)</span>, whose dimension
is <span class="math inline">\(r+1\)</span>. Based on the dimensional
formula, we derive that <span class="math display">\[
   dim(\mathcal{N}(B)\cap
V_{r+1})=dim(\mathcal{N}(B))+dim(V_{r+1})-dim(\mathcal{N}(B)+V_{r+1})=n+1-dim(\mathcal{N}(B)+V_{r+1})\geq1
\]</span> which means that <span
class="math inline">\(\mathcal{N}(B)\cap V_{r+1}\not=\empty\)</span>.
Therefore, we have that there exists an vector <span
class="math inline">\(x=\gamma_1v_1+\cdots+\gamma_{r+1}v_{r+1}\)</span>
such that <span class="math display">\[
   x\in \mathcal{N}(B)\cap V_{r+1},\ \left\Vert x\right\Vert_2=1
\]</span> Based on the definition of 2-norm, we can derive that <span
class="math display">\[
   \left\Vert A-B\right\Vert_2\geq \left\Vert
(A-B)x\right\Vert_2=\left\Vert
Ax\right\Vert_2=\gamma_1^2\sigma_1^2+\cdots+\gamma_{r+1}^2\sigma_{r+1}^2\geq
\sigma_{r+1}^2=\left\Vert A-\tilde{A}\right\Vert_2
\]</span> which means that the optimal rank-r approximation to <span
class="math inline">\(A\)</span> under the 2-norm is given by the rank-r
SVD truncation <span class="math inline">\(\tilde{A}\)</span>.</p></li>
<li><p>Now we claim that the best rank <span
class="math inline">\(r\)</span> approximation to <span
class="math inline">\(A\)</span> in the Frobenius norm, denoted by <span
class="math inline">\(\Vert\cdot\Vert_F\)</span>, is given by <span
class="math display">\[
A_r=U_r\Sigma_rV_r^T=\sum_{i=1}^r\sigma_iu_iv_i^T
\]</span> First, note that we have <span class="math display">\[
\left\Vert A-A_k\right\Vert_F^2=\left\Vert
\sum_{i=r+1}^m\sigma_iu_iv_i^T\right\Vert_F^2=\sum_{i=r+1}^m\sigma_i^2
\]</span> Hence, we need to show that if <span
class="math inline">\(B_r\)</span> is any rank-<span
class="math inline">\(r\)</span> matrix, then, <span
class="math display">\[
\left\Vert A-B_r\right\Vert_F^2\geq \sum_{i=r+1}^m\sigma_i^2
\]</span> Based on the triangle inequality with the spectral norm, if
<span class="math inline">\(A=A&#39;+A&#39;&#39;\)</span>, then <span
class="math inline">\(\sigma_1(A)\leq\sigma_1(A&#39;)+\sigma_1(A&#39;&#39;)\)</span>.
Suppose <span class="math inline">\(A_r&#39;\)</span> and <span
class="math inline">\(A_r&#39;&#39;\)</span> denote the rank <span
class="math inline">\(r\)</span> approximation to <span
class="math inline">\(A&#39;\)</span> and <span
class="math inline">\(A&#39;&#39;\)</span> by SVD method described
above, respectively. Then, for any <span
class="math inline">\(i,j\geq1\)</span> <span class="math display">\[
\begin{aligned}
\sigma_i(A&#39;)+\sigma_j(A&#39;&#39;)&amp;=\sigma_1(A&#39;-A_{i-1}&#39;)+\sigma_1(A&#39;&#39;-A_{j-1}&#39;&#39;)\\
&amp;\geq\sigma_1(A&#39;-A_{i-1}&#39;+A&#39;&#39;-A_{j-1}&#39;&#39;)\\
&amp;\geq\sigma_1(A-A_{i-1}&#39;-A_{j-1}&#39;&#39;)\\
\end{aligned}
\]</span> Here we know that <span
class="math inline">\(rank(A_{i-1}&#39;+A_{j-1}&#39;&#39;)\leq
i+j-2\)</span>, we can derive that</p></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\sigma_i(A&#39;)+\sigma_j(A&#39;&#39;)&amp;\geq\sigma_1(A-A_{i+j-2})\\
&amp;=\sigma_{i+j-1}(A)
\end{aligned}
\]</span> Since <span
class="math inline">\(\sigma_{r+1}(B_r)=0\)</span>, when <span
class="math inline">\(A&#39;=A-B_r\)</span> and <span
class="math inline">\(A&#39;&#39;=B_r\)</span>, we conclude that for
<span class="math inline">\(i\geq1,\ j=r+1\)</span> <span
class="math display">\[
\sigma_i(A-B_r)= \sigma_i(A-B_r)+\sigma_{r+1}(B_r)\geq\sigma_{i+r}(A)
\]</span> Therefore, we have <span class="math display">\[
\left\Vert
A-B_r\right\Vert_F^2=\sum_{i=1}^m\sigma_i(A-B_r)^2\geq\sum_{i=r+1}^m\sigma_i(A)^2=\left\Vert
A-A_r\right\Vert_F^2
\]</span></p>
]]></content>
      <categories>
        <category>Reduced Order Model</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>SVD-Mathematical Overview</title>
    <url>/2024/04/18/SVD-Mathematical%20Overview/</url>
    <content><![CDATA[<p>​ Firstly, we consider a large data matrix <span
class="math inline">\(X\in \mathbb{C}^{n\times m}\)</span>:</p>
<p><span class="math display">\[
X=\begin{bmatrix}\vdots&amp;\vdots&amp;\
&amp;\vdots\\x_1&amp;x_2&amp;\cdots&amp;x_m\\\vdots&amp;\vdots&amp;\
&amp;\vdots\end{bmatrix}
\]</span> The column <span class="math inline">\(x_k\in
\mathbb{C}^n\)</span> is obtained from simulations or experiments.
Here,we always consider the column vectors may also represent the state
of a physical system that is evolving in time.The column are often
called snapshots and <span class="math inline">\(m\)</span> is the
number of snapshots in <span class="math inline">\(X\)</span>.</p>
<p>The Singular Value Decomposition allows us to decompose any
complex-valued matrix as the product of three other matrices,</p>
<span id="more"></span>
<p><span class="math display">\[
X=U\Sigma V^T=\begin{bmatrix}\vdots&amp;\vdots&amp;\
&amp;\vdots\\u_1&amp;u_2&amp;\cdots&amp;u_n\\\vdots&amp;\vdots&amp;\
&amp;\vdots\end{bmatrix}\begin{bmatrix}\sigma_1&amp;\ &amp;\ &amp;\ \\\
&amp;\sigma_2&amp;\ &amp;\ \\\ &amp;\ &amp;\ddots&amp;\ \\
\ &amp;\ &amp;\ &amp;\sigma_m\\\ &amp;\ &amp;\ &amp;\ \\\ &amp;\ &amp;\
&amp;\ \\\ &amp;\ &amp;\ &amp;\
\end{bmatrix}\begin{bmatrix}\vdots&amp;\vdots&amp;\
&amp;\vdots\\v_1&amp;v_2&amp;\cdots&amp;v_m\\\vdots&amp;\vdots&amp;\
&amp;\vdots\end{bmatrix}^T
\]</span> where <span class="math inline">\(U\in \mathbb{C}^{n\times
n}\)</span> and <span class="math inline">\(V\in\mathbb{C}^{m\times
m}\)</span> are unitary matrices. If a square matrix <span
class="math inline">\(U\)</span> satisfies <span
class="math inline">\(U^{\star}U=UU^{\star}=I\)</span>, we call it
unitary matrix. Here * denotes the complex conjugate transpose. For
real-valued matrices, it is the same as the regular transpose, <span
class="math inline">\(X^{\star}=X^T\)</span>. So if <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> are real-valued matrices, we call them
orthogonal matrices. Since the conclusions of real-valued and
complex-valued <span class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> are parallel, we will only discuss the
real-valued case in the following. <span class="math inline">\(\Sigma\in
\mathbb{R}^{n\times m}\)</span> is a diagonal matrix, which means
real,non-zero entries on thediagonal and zeros off the diagonal.</p>
<p>​ These column of <span class="math inline">\(U\)</span> have the same
shape as a column of <span class="math inline">\(X\)</span>. The columns
of <span class="math inline">\(U\)</span> describe the eigen of the data
in <span class="math inline">\(X\)</span>. So in the case of faces,
these mean eigenfaces and in the case of flow fields, these mean
eigenflow fields. Furthermore, the columns of <span
class="math inline">\(U\)</span> are hierarchically arranged in terms of
their ability to describe the variance in the column of <span
class="math inline">\(X\)</span>. In other words, <span
class="math inline">\(u_1\)</span> is somehow more important than <span
class="math inline">\(u_2\)</span> and so on and so forth.</p>
<p>​ <span class="math inline">\(U\)</span> gives me a basis, based on
which we can represent each column of original data in <span
class="math inline">\(X\)</span>.Actually, these basis have great
properties. Since <span class="math inline">\(U\)</span> is an unitary
matrix, it means the column of <span class="math inline">\(U\)</span>
are orthonormal. So these column are all orthogonal , have unit length
and provide a complete basis for n-dimensional subspace where the column
of data matrix lives.</p>
<p>​ <span class="math inline">\(\Sigma\)</span> is not only a diagnol
matrix, but also non-negative and hierarchically ordered matrix. So we
have <span
class="math inline">\(\sigma_1\geq\sigma_2\geq\sigma_3\geq\cdots\geq\sigma_m\geq0\)</span>.
They are all non-negative, although some of them could be zero.
According to the matrix multiplication, we can find that <span
class="math inline">\(\sigma_1\)</span> correspond to the first columns
of <span class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span>.Since <span
class="math inline">\(\sigma_1\geq\sigma_2\)</span>, it means that the
first columns are somehow important than the second ones when we
describe the information of <span class="math inline">\(X\)</span>. In
other words, the sigular value provides the relative imporatance of
these corresponding columns of <span class="math inline">\(U\)</span>
and <span class="math inline">\(V\)</span>. Finally, we say that the
sigular values are ordered by importance.</p>
<p>​ Here we will start in the case of flow fields. The columns of <span
class="math inline">\(U\)</span> will be eigen flows hierarchically
organized. We call the first column of <span
class="math inline">\(V\)</span> as <span
class="math inline">\(v_1\)</span>. So <span
class="math inline">\(v_1\)</span> would be the time series for how this
first eigen mode <span class="math inline">\(u_1\)</span> evolves in
this flow.</p>
<p>The matrix <span class="math inline">\(U\)</span> is called left
sigular column and the columns are called left singular vectors. <span
class="math inline">\(V\)</span> is similar to <span
class="math inline">\(U\)</span>. The diagonal elements of <span
class="math inline">\(\Sigma\)</span> are called sigular values.The rank
of X is equal to the number of non-zero sigular value.</p>
]]></content>
      <categories>
        <category>Reduced Order Model</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>SVD-Linear Regression</title>
    <url>/2024/06/21/SVD-Linear%20Regression/</url>
    <content><![CDATA[<p>Now we are ready to talk about building linear regression models from
data. When we have gotten measurement data in experiments, it is
important to construct a linear regression model as a predictive model.
The basic structure of the linear regression is defined as follows,
<span class="math display">\[
Ax=b
\]</span> where <span class="math inline">\(A\in\mathbb{R}^{n\times
m}\)</span> and <span class="math inline">\(b\in\mathbb{R}^{n}\)</span>
are given by the measurement. Therefore, building a linear regression is
equal to solve the linear system <span
class="math inline">\(Ax=b\)</span>. Here we just consider the over
determined system <span class="math inline">\(n&gt;m\)</span> since it
is often what we have in modern data. We will use the least square
estimate to solve the vector <span class="math inline">\(x\)</span>.</p>
<span id="more"></span>
<p>To explain this linear regression more clearly, let's say that every
rows of equation is based on data from an individual medical history. So
the equation <span
class="math inline">\(a_{i1}x_1+\cdots+a_{im}x_{m}=b_{i}\)</span> means
the medical case of the <span class="math inline">\(i-th\)</span>
person. Every columns of matrix <span class="math inline">\(A\)</span>
represent different kinds of risk factors and the vector <span
class="math inline">\(b\)</span> means the risk of heart disease.
Therefore, we can give a interpretation about linear regression. Linear
regression is a best fit model <span class="math inline">\(x\)</span>
for what combination of those factors describe best or predict best the
future risk of heart disease. Actually it may be driven by the nonlinear
system in real world, here we oversimplify this system by using linear
regression to approximate it.</p>
<p>Since we consider the overdetermined system, it is hard to find a
vector <span class="math inline">\(x\)</span> that exactly solves this
linear model. We want to find the best fit vector <span
class="math inline">\(x\)</span> that minimizes the error norm between
<span class="math inline">\(Ax\)</span> and <span
class="math inline">\(b\)</span>. In other words, the best fit vector
<span class="math inline">\(x\)</span> gives us the best prediction of
<span class="math inline">\(b\)</span> given information in <span
class="math inline">\(A\)</span>. Here we consider a one-dimensional
linear regression model since it can be easily plotted in the following
figure and gives a geometric interpretation of linear regression. Of
course, the conclusion derived in this way is very simple and can be
generalized to much higher-dimensional situations.</p>
<p><img
src="https://cdn.jsdelivr.net/gh/LYD122504/picx-images-hosting@master/2024-06-20_20-56-36_screenshot.7i08u20nua.png" /></p>
<p>As seen in figure, we will solve the <span
class="math inline">\(x\)</span> here that we want is the slope of this
fit line. We generalize this interpretation to much higher dimensional
case instead of having just one factor that I use to predict <span
class="math inline">\(b\)</span>. In general, I will find a best fit
plane instead of best fit line to approximate the high dimensional data.
In one dimensional, <span class="math inline">\(A=\begin{bmatrix} a_1\
a_2\ \cdots\ a_n\end{bmatrix}^{T}\)</span> and <span
class="math inline">\(b=\begin{bmatrix}b_1\ b_2\ \cdots\
b_n\end{bmatrix}^{T}\)</span> will be considered. Based on the SVD, we
can give the decompostion matrices <span
class="math inline">\(U=\frac{A}{\Vert A\Vert_2}\)</span>, <span
class="math inline">\(\Sigma=\Vert A\Vert_2\)</span> and <span
class="math inline">\(V^T=1\)</span> easily. Using Moore-Penrose pseudo
inverse of <span class="math inline">\(A\)</span>, we can solve the best
fit slope <span class="math inline">\(x\)</span> as follows, <span
class="math display">\[
x=A^{\dagger}b=\frac{A^T}{\Vert A\Vert_2^2}b
\]</span> Due to the value of the best fit slope, we find that the
linear regression can be written as <span
class="math inline">\(Ax=AA^{\dagger}b=\frac{AA^T}{\Vert
A\Vert_2^2}b\)</span>. It means that we project <span
class="math inline">\(b\)</span> into the direction of <span
class="math inline">\(A\)</span>. When we think about the projection, we
realize that the normalized matrix <span
class="math inline">\(\tilde{A}=\frac{A}{\Vert A\Vert_2}\)</span>
satisfies <span
class="math inline">\(\tilde{A}^T\tilde{A}=\frac{A^TA}{\Vert
A\Vert_2}=1\)</span>. From this, we can use this normalized matrix <span
class="math inline">\(\tilde{A}\)</span> to derive the projection
operator <span
class="math inline">\(\tilde{A}(\tilde{A}^T\tilde{A})\tilde{A}^T=\tilde{A}\tilde{A}^{T}=\frac{AA^T}{\Vert
A\Vert_2^2}\)</span>. This formula can be generalized into higher
dimensional case easily.</p>
<p>At last, we consider a very common problem that our data has some
outliers in data analysis. As seen in the above figure, these data do
not fully mathch our best fitting model. Since the normal data can have
some variablity, like Gaussian noisy, our linear regression just is a
approximation model to real world. Here we assume that our data has a
outlier which is completely different with others. It will bias my
distrubution and effect the best fit slope, since the purpose of our
derivation is minimizing the sum of the squares of the errors of all of
those points to the line. Therefore, it is a big risk we have when we
have outliers. Regularly square based on the SVD can handle white noise
very well. However, the matrix <span class="math inline">\(A\)</span>
will be sensitive to outliers, we will consider in robust
statistics.</p>
]]></content>
      <categories>
        <category>Reduced Order Model</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>SVD-Linear System and Pseudo Inverse</title>
    <url>/2024/06/18/SVD-Linear%20System%20and%20Pseudo%20Inverse/</url>
    <content><![CDATA[<p>In many physics problem, we always encounter the solution of the
linear equation <span class="math inline">\(Ax=b\)</span>. Classically,
we solve this linear equation in the case that <span
class="math inline">\(A\)</span> is a square and invertible matrix. In
fact, this case is too special to better model the real world. Based on
the singular value decomposition which we have introduced, we can
generalize this linear equation to the case that <span
class="math inline">\(A\)</span> is a non-square matrix. Actually, when
we consider the data analysis and data modeling, the matrix <span
class="math inline">\(A\)</span> is always a non-square matrix.</p>
<span id="more"></span>
<p>Here we introduce few different things happened due to the non-square
matrix <span class="math inline">\(A\in\mathbb{R}^{n\times m}\)</span>.
Since A is a non-square matrix, there are two canonical non-square
matrix <span class="math inline">\(A\)</span>, corresponding to the two
different canonical non-square linear equations.</p>
<ol type="1">
<li>Underdetermined System <span class="math inline">\((n&lt;m)\)</span>
Since <span class="math inline">\(n&lt;m\)</span>, the shape of the
matrix <span class="math inline">\(A\)</span> is short and fat. In this
case, the number of equations is less than the number of unknowns. There
is not enough measurements in vector <span
class="math inline">\(b\)</span> to determine a single unique vector
<span class="math inline">\(x\)</span>. In other words, there is more
degrees of freedom in the unknown vector <span
class="math inline">\(x\)</span> and there is not enough values in the
vector <span class="math inline">\(b\)</span> to determine the unknown
vector <span class="math inline">\(x\)</span>. In general, there are
infinitely many solutions <span class="math inline">\(x\)</span> given
by the underdetermined system. In fact, you can try to construct a very
special underdetermined system such that there is only one solution. But
it is too special that we can ignore it.</li>
<li>Overdetermined System <span class="math inline">\((n&gt;m)\)</span>
Since <span class="math inline">\(n&gt;m\)</span>, the shape of the
matrix <span class="math inline">\(A\)</span> is tall and skinny. In
this case, the number of equations is more than the number of unknowns.
Unfortunately, since there is not enough degrees of freedom in unknown
vector <span class="math inline">\(x\)</span> to fit these equation,
there is no solution <span class="math inline">\(x\)</span> given by the
overdetermined system generally. In fact, you can try to construct a
very special underdetermined system such that there is only one
solution. But it is too special that we can ignore it.</li>
</ol>
<p>The singular value decomposition allows us to approximately invert
this matrix <span class="math inline">\(A\)</span> to compute what is
known as the pseudo inverse and find a best fit solution <span
class="math inline">\(x\)</span> that either comes as close to solving
this equation as possible or solves this equation with the minimum <span
class="math inline">\(L^2\)</span> norm of <span
class="math inline">\(x\)</span>. Here we introduce the Moore-Penrose
pseudo inverse of the matrix <span class="math inline">\(A\)</span>.
Based on the economy singular value decomposition of the matrix <span
class="math inline">\(A\)</span>, we can decompose the matrix <span
class="math inline">\(A\)</span> as <span
class="math inline">\(A=U\Sigma V^T\)</span>, where <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> satisfy <span
class="math inline">\(U^TU=I\)</span> and <span
class="math inline">\(V^TV=VV^T=I\)</span>. The Moore-Penrose pseudo
inverse of the matrix <span class="math inline">\(A\)</span> is defined
as <span class="math display">\[
A^{\dagger}=V\Sigma^{-1}U^T
\]</span> Based on the properties of <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span>, we realize that the Moore-Penrose
pseudo inverse is a left pseudo inverse of the matrix <span
class="math inline">\(A\)</span>. Now we can give a approximate solution
of the linear equation <span class="math inline">\(Ax=b\)</span> by
using the Moore-Penrose pseudo inverse of the matrix <span
class="math inline">\(A\)</span>. <span class="math display">\[
Ax=b\Rightarrow U\Sigma V^Tx=b\Rightarrow
\tilde{x}=V\Sigma^{-1}U^Tb=A^{\dagger}b
\]</span> Here we have given a best fit solution <span
class="math inline">\(x\)</span> which comes as close to solving this
equation as possible. Now we will give out this solution which satisfies
the optimization problems. Now we give the optimization problems that
the best fit solution <span class="math inline">\(x\)</span> satisfies
for underdetermined system and overdetermined system.</p>
<ol type="1">
<li><p>Underdetermined System <span
class="math inline">\((n&lt;m)\)</span> In this case, the best fit
solution <span class="math inline">\(x\)</span> satisfies the
optimization problem <span class="math display">\[
   \min_x\|x\|_2\quad\text{subject to}\quad Ax=b
\]</span> The solution of this optimization problem is the solution of
the linear equation <span class="math inline">\(Ax=b\)</span> with the
minimum <span class="math inline">\(L^2\)</span> norm of <span
class="math inline">\(x\)</span>.</p></li>
<li><p>Overdetermined System In this case, the best fit solution <span
class="math inline">\(x\)</span> satisfies the optimization problem
<span class="math display">\[
   \min_x\|Ax-b\|_2
\]</span> The solution of this optimization problem is the solution of
the linear equation <span class="math inline">\(Ax=b\)</span> that comes
as close to solving this equation as possible.</p>
<p>Finally, we will clarify something that is highly confusing: the
method of obtaining the optimal approximation solution is same as the
normal process,but it is just an approximate solution. Here we
substitute the approximate solution <span
class="math inline">\(\tilde{x}=A^{\dagger}b\)</span> into the linear
equation <span class="math inline">\(Ax=b\)</span>. <span
class="math display">\[
   A\tilde{x}=AA^{\dagger}b=U\Sigma V^TV\Sigma^{-1}U^Tb=UU^Tb
\]</span> where the matrix <span class="math inline">\(UU^T\)</span> is
not a identity matrix and it might not even be close to the identity
matrix. Before we explain the vector <span
class="math inline">\(UU^Tb\)</span>, we should discuss the project
operator. Here we assume that we have a vector <span
class="math inline">\(v\)</span> and a matrix <span
class="math inline">\(P\)</span>. Now we try to project the vector <span
class="math inline">\(v\)</span> onto the column space of the matrix
<span class="math inline">\(P\)</span>. We assume the projection of the
vector <span class="math inline">\(v\)</span> onto the column space of
the matrix <span class="math inline">\(P\)</span> is <span
class="math inline">\(Px\)</span>, where <span
class="math inline">\(x\)</span> is an unknown vector. Based on the
definition of the projection, the vector <span
class="math inline">\(e=v-Px\)</span> and all column vectors of <span
class="math inline">\(P\)</span> are orthogonal. Therefore, we can
derive the following linear equaiton, <span class="math display">\[
   P^Te=P^T(v-Px)=P^Tv-P^TPx=0\Rightarrow x=(P^TP)^{-1}P^Tv
\]</span> From the above result, we can derive the projection vector
<span class="math inline">\(Px=P(P^TP)^{-1}P^Tv\)</span>. Here we call
<span class="math inline">\(P(P^TP)^{-1}P^T\)</span> as the projection
operator. Now we consider the matrix <span
class="math inline">\(U\)</span> which satisfies <span
class="math inline">\(U^TU=I\)</span>, so the projection operator <span
class="math inline">\(U(U^TU)^{-1}U^T=UU^T\)</span>. Above all, the
meaning of <span class="math inline">\(UU^Tb\)</span> is the projection
of the vector <span class="math inline">\(b\)</span> onto the column
space of the matrix <span class="math inline">\(U\)</span>. Furthermore,
the column space of the matrix <span class="math inline">\(U\)</span> is
equal to the column space of the matrix <span
class="math inline">\(A\)</span>. Therefore, the vector <span
class="math inline">\(UU^Tb\)</span> is the projection of the vector
<span class="math inline">\(b\)</span> onto the column space of the
matrix <span class="math inline">\(A\)</span>. The errors crops up since
<span class="math inline">\(UU^Tb\)</span> is an orthogonal projection
of <span class="math inline">\(B\)</span> onto the column space of <span
class="math inline">\(A\)</span>.</p></li>
</ol>
]]></content>
      <categories>
        <category>Reduced Order Model</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>SVD-Matrix Approximation</title>
    <url>/2024/04/19/SVD-Matrix%20Approximation/</url>
    <content><![CDATA[<p>The SVD allows us to decompose data matrix <span
class="math inline">\(X\)</span> as the product of three matrices, <span
class="math inline">\(U,\ V^T,\ \Sigma\)</span>, where essentially <span
class="math inline">\(U\)</span> contains information about the column
space of <span class="math inline">\(X\)</span>, <span
class="math inline">\(V\)</span> contains information about the row
space of <span class="math inline">\(X\)</span> and <span
class="math inline">\(\Sigma\)</span> is a hierarchically ordered
diagonal matrix, which tells you how important the various columns of
<span class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> are.</p>
<span id="more"></span>
<p>In fact, the data matrix <span class="math inline">\(X\)</span> has
only <span class="math inline">\(m\)</span> columns, it means there are
at most <span class="math inline">\(m\)</span> linearly independence
columns in this <span class="math inline">\(n\)</span>-dimensional
vector space.So the first <span class="math inline">\(m\)</span> columns
of <span class="math inline">\(U\)</span> are important in representing
this data matrix. To explain this fact more deeply, we try to represent
expansion as a sum of rank-1 matrices.</p>
<p>Based on the Singular Value Decomposition,we can get the following
equation, <span class="math display">\[
X=U\Sigma V^T=\begin{bmatrix}\vdots&amp;\vdots&amp;\
&amp;\vdots\\u_1&amp;u_2&amp;\cdots&amp;u_m\\\vdots&amp;\vdots&amp;\
&amp;\vdots\end{bmatrix}\begin{bmatrix}\sigma_1&amp;\ &amp;\ &amp;\ \\\
&amp;\sigma_2&amp;\ &amp;\ \\\ &amp;\ &amp;\ddots&amp;\ \\
\ &amp;\ &amp;\ &amp;\sigma_m\\\ &amp;\ &amp;\ &amp;\ \\\ &amp;\ &amp;\
&amp;\ \\\ &amp;\ &amp;\ &amp;\
\end{bmatrix}\begin{bmatrix}\cdots&amp;v_1^{T}&amp;\cdots\\\cdots&amp;v_2^T&amp;\cdots\\\
&amp;\vdots&amp;\ \\\cdots&amp;v_m^T&amp;\cdots\end{bmatrix}^T
\]</span> Since <span class="math inline">\(\Sigma\)</span> is a
diagnoal matrix, we can expand the above equation as follow, <span
class="math display">\[
X=U\Sigma
V^T=\sigma_1u_1v_1^T+\sigma_2u_2v_2^T+\cdots+\sigma_mu_mv_m^T=\begin{bmatrix}\vdots&amp;\vdots&amp;\
&amp;\vdots\\u_1&amp;u_2&amp;\cdots&amp;u_m\\\vdots&amp;\vdots&amp;\
&amp;\vdots\end{bmatrix}\begin{bmatrix}\sigma_1&amp;\ &amp;\ &amp;\ \\\
&amp;\sigma_2&amp;\ &amp;\ \\\ &amp;\ &amp;\ddots&amp;\ \\
\ &amp;\ &amp;\
&amp;\sigma_m\end{bmatrix}\begin{bmatrix}\cdots&amp;v_1^{T}&amp;\cdots\\\cdots&amp;v_2^T&amp;\cdots\\\
&amp;\vdots&amp;\
\\\cdots&amp;v_m^T&amp;\cdots\end{bmatrix}^T=\hat{U}\hat{\Sigma}V^T
\]</span> Even though <span class="math inline">\(U\)</span> is a
massive <span class="math inline">\(n\times n\)</span> matrix, there are
only at most <span class="math inline">\(m\)</span> non-zeros singular
values in <span class="math inline">\(\Sigma\)</span> that means the
rank of data matrix <span class="math inline">\(X\)</span> satisfies
<span class="math inline">\(rank(X)\leq m\)</span>. Actually,I can
selcet the first <span class="math inline">\(m\)</span> columns of <span
class="math inline">\(U\)</span>, the first <span
class="math inline">\(m\times m\)</span> block of <span
class="math inline">\(\Sigma\)</span> and the original <span
class="math inline">\(V\)</span> to represent the data matrix <span
class="math inline">\(X\)</span>. We always call <span
class="math inline">\(X=\hat{U}\hat{\Sigma}V^T\)</span> as the economy
SVD and <span class="math inline">\(X=U\Sigma V^T\)</span> as the full
SVD. Since we consider the case of <span class="math inline">\(n\gg
m\)</span>, <span class="math inline">\(\hat{U}\in \mathbb{R}^{n\times
m}\)</span> and <span class="math inline">\(\hat{\Sigma}\in
\mathbb{R}^{m\times m}\)</span> need lower storage.</p>
<p>Therefore, we can give another explanation about SVD: we can
decompose the data matrix X into the orthogonal basis <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span>, where essentially you can rewrite this
as a sum of rank-1 matrices, which increasingly improve the
approximation of <span class="math inline">\(X\)</span>. According to
this explanation, we can give out some interesting results like, the
best rank-1 matrix that we can make to approximate <span
class="math inline">\(X\)</span> is <span
class="math inline">\(\sigma_1u_1v_1^T\)</span>; the best rank-2 matrix
that we can make to approximate <span class="math inline">\(X\)</span>
is <span
class="math inline">\(\sigma_1u_1v_1^T+\sigma_2u_2v_2^T\)</span> and so
on and so forth.</p>
<p>Since we hope to use less data storage to approximate the real data
as much as possible in practice, we often truncate at rank r. Oftentimes
we have a lot of negligibly small singular values like <span
class="math inline">\(\sigma_{r+1},\
\sigma_{r+2},\cdots,\sigma_m\)</span> , it means that most of the
information of <span class="math inline">\(X\)</span> is captured in the
first <span class="math inline">\(r\)</span> singular values and the
first <span class="math inline">\(r\)</span> singular vectors. So we can
throw away all of low singular values and singular vectors and only keep
the first <span class="math inline">\(r\)</span> columns of <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> and the first <span
class="math inline">\(r\times r\)</span> submatrix in <span
class="math inline">\(\Sigma\)</span>. Then we are going to define this
truncated SVD as follows, <span class="math display">\[
X\approx\begin{bmatrix}\vdots&amp;\vdots&amp;\
&amp;\vdots\\u_1&amp;u_2&amp;\cdots&amp;u_r\\\vdots&amp;\vdots&amp;\
&amp;\vdots\end{bmatrix}\begin{bmatrix}\sigma_1&amp;\ &amp;\ &amp;\ \\\
&amp;\sigma_2&amp;\ &amp;\ \\\ &amp;\ &amp;\ddots&amp;\ \\
\ &amp;\ &amp;\
&amp;\sigma_r\end{bmatrix}\begin{bmatrix}\cdots&amp;v_1^{T}&amp;\cdots\\\cdots&amp;v_2^T&amp;\cdots\\\
&amp;\vdots&amp;\
\\\cdots&amp;v_r^T&amp;\cdots\end{bmatrix}^T=\tilde{U}\tilde{\Sigma}\tilde{V}^T
\]</span> Here we find that the truncated SVD <span
class="math inline">\(\tilde{U}\tilde{\Sigma}\tilde{V}^T\)</span> is the
best rank-<span class="math inline">\(r\)</span> matrix approximating
the data matrix <span class="math inline">\(X\)</span>.Thus,
high-dimensional data may be well described by a few dominant patterns
given by the columns of <span class="math inline">\(\tilde{U}\)</span>
and <span class="math inline">\(\tilde{V}\)</span>.Like the mentioned in
the first section,we realize that the truncated singular vectors <span
class="math inline">\(\tilde{U}\)</span> provides a coordinate
transformation from the high-dimensional measurement space into a
low-dimensional pattern space.</p>
<p>The Eckart-Young theorem states that the absolute best approximation
to the matrix <span class="math inline">\(X\)</span> of rank r,
Theorem(Eckart-Young) The optimal rank-r approximation to <span
class="math inline">\(X\)</span>, in a least-squares sense, is given by
the rank-r SVD truncation <span
class="math inline">\(\tilde{X}\)</span>, <span class="math display">\[
argmin_{\tilde{X},\ s.t.\ rank(\tilde{X})=r}\left\Vert
X-\tilde{X}\right\Vert_F=\tilde{U}\tilde{\Sigma}\tilde{V}^T
\]</span> The Ecakrt-Young theorem guarantees that the best possible
matrix approximation to <span class="math inline">\(X\)</span> of rank
<span class="math inline">\(r\)</span> is given by the firsr <span
class="math inline">\(r\)</span> truncated SVD.</p>
<p>Finally,we should mention an important point. At beginning of
discussing the SVD, we define the matrices <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> are unitary. However,if we truncated at
rank r,the truncated matrices <span
class="math inline">\(\tilde{U}\)</span> and <span
class="math inline">\(\tilde{V}\)</span> are no longer square matrices,
so they are not unitary matrix again. They satisfy that <span
class="math inline">\(\tilde{U}^T\tilde{U}=\tilde{V}^TV=I,\
\tilde{U}\tilde{U}^T\not=I\)</span></p>
]]></content>
      <categories>
        <category>Reduced Order Model</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux-Shell常用指令</title>
    <url>/2024/03/30/linux-1/</url>
    <content><![CDATA[<h1 id="shell-基本命令">Shell 基本命令</h1>
<p>Bash提供了命令行补全的特性.我们可以利用Tab键来完成命令以及文件名的自动补全.如果以已输入的字符开头的文件不止一个,那么连续输入两次Tab键,shell将会以列表的形式给出所有以输入字符开头的文件名.</p>
<p>同样我们可以补全Linux命令,这是因为Linux的命令本质上是一些可执行文件,所以可以认为命令补全和文件名补全其实是同一件事.</p>
<p>此外,Shell在查找文件中存在通配符的专用符号,其为*,?,[].这些通配符可以搜索并匹配文件名的一部分,从而可以做到批量检索文件.</p>
<span id="more"></span>
<ol type="1">
<li><p>*用于匹配文件名中任意长度的字符串.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> *.cpp <span class="comment">## 检索以.cpp结尾的文件</span></span></span><br></pre></td></tr></table></figure></li>
<li><p>?和*类似,但是与*匹配任意长度的字符串不同,?只能匹配一个字符串</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> text? <span class="comment">## 检索文件名中以text开头并且后面接一个字符的文件.</span></span></span><br></pre></td></tr></table></figure></li>
<li><p>[]用于匹配所有出现在方括号内的字符.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> text[1A]  <span class="comment">## 用于列出以text开头而仅以1或者A结束的文件</span></span></span><br></pre></td></tr></table></figure>
<p>如果需要查找多个连续字符范围的,可以利用短线-来指定一个字符集范围.所有包含在上下界之间的字符都会被匹配.同样可对字母操作,如在ASC码中,A-Z可以包含所有的大写字母.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> text[1-3] <span class="comment">## 用于列出以text开头而仅以1-3中字符结束的文件</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> text[A-Z] <span class="comment">## 用于列出以text开头而仅以大写字母结束的文件</span></span></span><br></pre></td></tr></table></figure></li>
</ol>
<p>虽然我们上面的演示都以通配符位于文件末,但实际上通配符的位置是任意的.并且可以随意搭配使用.</p>
<p><a id="org48c8384"></a></p>
<h2 id="pwd-显示当前目录">pwd: 显示当前目录</h2>
<p>pwd命令会显示当前所在的位置,也就是工作目录.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> /usr/local/bin  <span class="comment">## 进入/usr/local/bin目录</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">pwd</span>                <span class="comment">## 显示当前所在位置</span></span></span><br><span class="line">/user/local/bin</span><br></pre></td></tr></table></figure>
<p><a id="org0a74477"></a></p>
<h2 id="cd-改变目录">cd: 改变目录</h2>
<p>cd是Linux文件系统在不同文件夹之间转移的常用指令.其后一般都加上路径名作为参数表示跳转到相应的位置,但其有一些特殊的路径符号来简化跳转特定文件夹的指令.</p>
<ol type="1">
<li><p>/表示根目录,不管在什么目录下,命令</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> / <span class="comment">## 跳转到根目录</span></span></span><br></pre></td></tr></table></figure>
<p>都会快速跳转到根目录下.</p></li>
<li><p>..表示当前目录的上一级目录,例如如果现在工作目录都是/usr/bin</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> .. <span class="comment">## 跳转到上一级目录</span></span></span><br></pre></td></tr></table></figure>
<p>其会跳转到上一级目录即/usr中.</p></li>
<li><p>.表示当前目录,我们如果需要跳转到当前目录的某个文件夹中可以用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> ./xxx  <span class="comment">## 跳转到同级目录中的下级目录</span></span></span><br></pre></td></tr></table></figure></li>
<li><p>~表示用户主目录,其一般是/home/下名称为用户名的文件夹,在其中存储对应用户的数据和设置.同样我们可以用cd命令不加任何参数来跳转到用户主目录.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> ~  <span class="comment">## 跳转到用户主目录</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span>    <span class="comment">## 跳转到用户主目录</span></span></span><br></pre></td></tr></table></figure></li>
</ol>
<p><a id="orge55b477"></a></p>
<h2 id="ls-列出目录内容">ls: 列出目录内容</h2>
<p>ls命令实际上是list的简化形式.list的用法十分多,但其基本语法是</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">ls [OPTION]... [FILE]... ## ls命令的基本形式</span><br></pre></td></tr></table></figure>
<p>最简单的情况,我们直接在任意工作目录下输入ls命令,用于列出当前目录下所有文件和子目录.为了区分文件目录和文件,linux系统会用不同颜色来标注不同的类型.由于不同的系统对于颜色渲染不太一样,甚至可能没有颜色,我们可以用-F参数的ls命令,-F会在每个目录后面加上/,在可执行文件后加*,在链接文件后加@.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -F <span class="comment">##列出文件并且在列出的文件名称后加一符号</span></span></span><br></pre></td></tr></table></figure>
<p>虽然我们可以用ls列出当前目录下的所有内容,但实际上有很多的名称以.开头的文件并未展示,我们称这类文件为隐含文件,在默认情况下是不会展示的.我们可以用参数-a来显示所有的文件.对于命令的参数是可以组合使用的,并且制定多个选项只需要用一个短线即可.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -a  <span class="comment">## 列出所有文件及目录(以.开头的隐含文件也会被列出)</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -aF <span class="comment">## 列出所有文件和目录的同时加上一个后缀符号</span></span></span><br></pre></td></tr></table></figure>
<p>ls另外一个比较常见的参数是-l,这个选项可以用来查看文件的各种属性,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -l  <span class="comment">## 列出所有文件和目录以及其对应的属性</span></span></span><br><span class="line">  总计 152</span><br><span class="line">  drwxr-xr-x  2 lyd  lyd   4096  2月  5 09:49  公共</span><br><span class="line">  drwxr-xr-x  2 lyd  lyd   4096  2月  5 09:49  模板</span><br><span class="line">  drwxr-xr-x  2 lyd  lyd   4096  2月  5 09:49  视频</span><br><span class="line">  drwxr-xr-x  3 lyd  lyd   4096  2月 25 20:55  图片</span><br><span class="line">  drwxr-xr-x  2 lyd  lyd   4096  2月  5 09:49  文档</span><br><span class="line">  drwxr-xr-x  2 lyd  lyd   4096  3月 14 15:39  下载</span><br><span class="line">  drwxr-xr-x  2 lyd  lyd   4096  2月  5 09:49  音乐</span><br><span class="line">  drwxr-xr-x  2 lyd  lyd   4096  2月  5 09:49  桌面</span><br></pre></td></tr></table></figure>
<p>这一共有九个不同的信息栏,从左到右依次表示,</p>
<ol type="1">
<li>文件的权限标志(后续会进一步讨论其含义)</li>
<li>文件连接个数(同上)</li>
<li>文件所有者的用户名</li>
<li>该用户所在的用户组祖名(后续会讨论)</li>
<li>文件的大小</li>
<li>最后一次修改时的月份</li>
<li>最后一次修改的日期</li>
<li>最后一次修改的时间</li>
<li>文件名</li>
</ol>
<p>ls命令后接路径名,可以直接查看子目录的内容,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> /etc/init.d/ <span class="comment">## 用来查看/etc/init.d/的内容</span></span></span><br></pre></td></tr></table></figure>
<p>除了ls命令可以列出目录,我们也可以用windows系统常用的dir命令来列出,但其功能比ls要少.我们也可以用vdir来代替ls
-l来列出目录和文件的完整信息.但由于ls的功能明显更为强大,我们就不再多介绍dir和vdir的用法.</p>
<p><a id="org824b1c7"></a></p>
<h2 id="cat-和-more-查看文本文件">cat 和 more: 查看文本文件</h2>
<p>cat命令用于查看文件内容(一般是文本文件,如果打开的是PDF或者其他形式的文件会以文本文件的形式打开),后跟文件名作为参数.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> xxx <span class="comment">## 查看文件内容</span></span></span><br></pre></td></tr></table></figure>
<p>cat后面的文件名参数可以添加多个文件名,同样也可以用通配符来批量查看文件.</p>
<p>为了调试方便,我们可以用-n参数来在展示的文本文件形式的每行前面显示行号</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> -n xxxx <span class="comment">## 查看文件内容的同时显示行号</span></span></span><br></pre></td></tr></table></figure>
<p>但是当我们打开多个文件的时候,用-n参数并不会智能地区分每个文件的终止,而是连续的对行号进行递增编号.</p>
<p>但由于cat命令会将文件的内容全部展示在shell命令行之上,对于内容较多的文件,这样的打开方式并不合适.因此我们可以用more命令来一页页的显示文章内容,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">more XXX <span class="comment">## 以分页的形式展示文件内容</span></span></span><br><span class="line">....</span><br><span class="line">--More-- (75%) </span><br></pre></td></tr></table></figure>
<p>more命令会在最后显示一个百分比,其表示已显示内容占整个文件的比例.我们可以用空格键向下翻一页,用Enter则可以向下滚动一行,用Q退出文本文件展示.</p>
<p>more命令和cat一样可以添加多个文件名,也可以通配符来批量查看文件.</p>
<p><a id="orge830626"></a></p>
<h2 id="head-和-tail-查看文件的开头和结尾">head 和 tail:
查看文件的开头和结尾</h2>
<p>head和tail用于展示文件的开头和结尾.我们可以用-n参数来修改展示的行数,如果我们不加-n命令手动修改展示行数,那么会默认展示10行的内容.head命令的默认输出是包括输出文件名,但不会计入展示文件开头的行数,我们可以用-q参数来取消输出文件名.不仅如此,head后面还可以接多个文件名,用于批量展示多个文件的开头和结尾.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">head</span> -n k xxx <span class="comment">## 展示文件前k行,如果没有-n参数,默认展示文件的前10行</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">head</span> -q xxx <span class="comment">## 不展示文件名</span></span></span><br></pre></td></tr></table></figure>
<p>tail和head的用法完全一致.</p>
<p><a id="org41780dc"></a></p>
<h2 id="less-更好的文本阅读工具">less: 更好的文本阅读工具</h2>
<p>less和前面提到的more十分类似,都不能对文本文件进行修改,也支持打开多个文本文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">less ~/.bashrc <span class="comment">## 用less查看文件</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">~/.bashrc: executed by bash(1) <span class="keyword">for</span> non-login shells.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">see /usr/share/doc/bash/examples/startup-files (<span class="keyword">in</span> the package bash-doc)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> examples</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">If not running interactively, don<span class="string">&#x27;t do anything</span></span></span><br><span class="line">case $ - in</span><br><span class="line">    *i*) ;;</span><br><span class="line">      *) return;;</span><br><span class="line">esac</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">don&#x27;</span>t put duplicate lines or lines starting with space <span class="keyword">in</span> the <span class="built_in">history</span>.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">See bash(1) <span class="keyword">for</span> more options</span></span><br><span class="line">HISTCONTROL=ignoreboth</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">append to the <span class="built_in">history</span> file, don<span class="string">&#x27;t overwrite it</span></span></span><br><span class="line">shopt -s histappend</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">for setting history length see HISTSIZE and HISTFILESIZE in bash(1)</span></span></span><br><span class="line">HISTSIZE=1000</span><br><span class="line">HISTFILESIZE=2000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">check the window size after each command and, if necessary,</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">update the values of LINES and COLUMNS.</span></span></span><br><span class="line">:</span><br></pre></td></tr></table></figure>
<p>在屏幕的末尾会有一个冒号,其表示等待用户输入命令.在此我们罗列常用的移动命令,</p>
<ul>
<li><SPC> 向下滚动一页</li>
<li><Enter> 向下滚动一行</li>
<li>b 向上翻一页</li>
<li>y 向上滚动一行</li>
<li>d 向下翻半页</li>
<li>u 向上翻半页</li>
</ul>
<p>在less查看的文本文件中,我们还可以用/跟上想要查找的内容,来实现对文本内容的检索,less会把第一个搜索目标高亮显示,如果我们还需要查找相同内容,我们只需要用n来跳转到下一个搜索目标,同样如果我们需要跳转到前一次的搜索结果我们可以用N来跳转.</p>
<p>实际上/并不是全文检索,而是以光标为基准向后搜索字符串,相反我们可以用?来以光标为基准向前搜索字符串.</p>
<p>由于less的用法较多,可以在冒号后接h来打开less自带的帮助文档查看指令及其作用.</p>
<p>为了展示更多的文件信息,我们可以用-M来展示更多的文件信息,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">  $ </span><span class="language-bash">less -m ~/.bashrc</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">~/.bashrc: executed by bash(1) <span class="keyword">for</span> non-login shells.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">see /usr/share/doc/bash/examples/startup-files (<span class="keyword">in</span> the package bash-doc)</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> examples</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">If not running interactively, don<span class="string">&#x27;t do anything</span></span></span><br><span class="line">case $ - in</span><br><span class="line">    *i*) ;;</span><br><span class="line">      *) return;;</span><br><span class="line">esac</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">don&#x27;</span>t put duplicate lines or lines starting with space <span class="keyword">in</span> the <span class="built_in">history</span>.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">See bash(1) <span class="keyword">for</span> more options</span></span><br><span class="line">HISTCONTROL=ignoreboth</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">append to the <span class="built_in">history</span> file, don<span class="string">&#x27;t overwrite it</span></span></span><br><span class="line">shopt -s histappend</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">for setting history length see HISTSIZE and HISTFILESIZE in bash(1)</span></span></span><br><span class="line">HISTSIZE=1000</span><br><span class="line">/home/lyd/.bashrc lines 1-19/114 15%</span><br></pre></td></tr></table></figure>
<p>less在输出的底部显示了文件名,当前行数,总行数以及所占百分比.最后可以用Q推出less程序.</p>
<p><a id="org6891801"></a></p>
<h2 id="grep-查找文件内容">grep: 查找文件内容</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">grep [OPTIONS] PATTERN [FILE...]  ## grep一般形式</span><br></pre></td></tr></table></figure>
<p>如果我们想在文件A中查找包含test的行,可以用如下命令,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">grep <span class="built_in">test</span> A</span></span><br></pre></td></tr></table></figure>
<p>从上面的命令我们可以看出grep后面接两个不同类型的参数,第一个是被搜索的关键词,也称之为模式,第二个则是所搜索的文件.grep会将文件中出现关键词的行输出,并且grep可指定多个文件来搜索.另外我们用grep查找的是关键词,对于查找如Debian
Ubunut这样的关键词,我们需要用单引号来将空格包含在关键词中,不然grep会认为关键词为Debian,在文件名为Ubuntu的文件中查找.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">grep <span class="string">&#x27;Debian Ubuntu&#x27;</span> linux  <span class="comment">##在linux文件中查找关键词 Debian Ubuntu</span></span></span><br></pre></td></tr></table></figure>
<p>我们列出一些常用的grep命令选项,</p>
<ul>
<li>-i 忽略大小写进行匹配</li>
<li>-r 进行递归查找子目录中的文件</li>
<li>-l 仅输出存在匹配的文件名</li>
<li>-c 输出匹配的行数</li>
<li>-v 反向查找,只打印不匹配的行</li>
</ul>
<p>grep实际上是以正则表达式的形式对文件进行查找,针对扩展的正则表达式,我们可以用egrep来查找.</p>
<p><a id="org676ae04"></a></p>
<h2 id="find-文件查找">find: 文件查找</h2>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">find [OPTION] [path...] [expression] ## find命令的基本语法</span><br></pre></td></tr></table></figure>
<p>find命令需要一个路径名作为查找范围,find会深入该路径中的每个子目录中寻找,如果我们指定的路径名为/,那么就会在整个文件系统中搜索.下给出一个find命令的实例,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">find /usr/bin -name zip -<span class="built_in">print</span> <span class="comment">##在/usr/bin的目录下查找zip</span></span></span><br><span class="line">/usr/bin/zip</span><br></pre></td></tr></table></figure>
<p>这里的/usr/bin就是find的查找范围,-name指定了查找条件以文件名为基准,此处支持用通配符*和?.-print表示将结果输出到屏幕,实际上可以不需要加这个动作指令,find会默认将其输出.值得注意的是find输出的结果是文件的绝对路径.</p>
<p>我们在此罗列find常用的查找条件的参数,</p>
<ul>
<li>-name pattern: 按照文件名查找,支持使用通配符</li>
<li>-size [+-]size[cwbkMG]:
按文件大小查找,支持使用+或-来表示大于或小于指定大小,单位可以是c(字节),w(字数),b(块数),k(KB),M(MB)或G(GB).</li>
<li>-mtime days:
按修改时间查找,支持使用+或-表示在指定天数前或后,days是一个整数表示天数</li>
<li>-user username: 按照文件所有者查找</li>
<li>-group groupname: 按文件所属组查找</li>
<li>-type type: 按照文件类型查找,type的参数含义如下表所示</li>
</ul>
<div data-align="center">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col  class="org-left" />
</colgroup>
<colgroup>
<col  class="org-left" />
</colgroup>
<colgroup>
<col  class="org-left" />
</colgroup>
<colgroup>
<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">
参数
</th>
<th scope="col" class="org-left">
含义
</th>
<th scope="col" class="org-left">
参数
</th>
<th scope="col" class="org-left">
含义
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">
b
</td>
<td class="org-left">
块设备文件
</td>
<td class="org-left">
f
</td>
<td class="org-left">
普通文件
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">
c
</td>
<td class="org-left">
字符设备文件
</td>
<td class="org-left">
p
</td>
<td class="org-left">
命名管道
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">
d
</td>
<td class="org-left">
目录文件
</td>
<td class="org-left">
l
</td>
<td class="org-left">
符号链接
</td>
</tr>
</tbody>
</table>
</div>
<p>我们介绍find对匹配到的文件执行的操作,</p>
<ul>
<li>-amin n: 查找在n分钟被访问过的文件</li>
<li>-atime n: 查找在n*24小时内被访问的文件</li>
<li>-cmin n: 查找在n分钟内状态发生变化的文件(例如权限)</li>
<li>-ctime n: 查找在n*24小时内状态发生过变化的文件(例如权限)</li>
<li>-mmin n: 查找在n分钟内被修改过的文件</li>
<li>-mtime n: 查找在n*24小时内被修改过的文件</li>
</ul>
<p>上面的参数n可以是正数,负数亦或是0.正数+n表示查找比指定时间更早的文件或目录,负数-n表示查找在指定时间内的文件或目录,0表示当天的文件或目录</p>
<p><a id="org3ded0d1"></a></p>
<h2 id="locate-快速定位文件">locate: 快速定位文件</h2>
<p>由于find在查找大批量文件中将花费大量的文件,因此我们可以用locate来代替find,在Ubuntu系统中locate并没有内置安装,因此我们先用apt包管理器下载locate,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sudo apt-get install mlocate <span class="comment">## 下载locate命令及相关依赖</span></span></span><br></pre></td></tr></table></figure>
<p>locate的底层逻辑是并不去查找每个子目录,而是在/var/lib/slocate资料库里查找,因此locate的查找并不是及时的,需要对数据库进行更新,一般来说系统每天会自动更新一次,当然也可以用命令自己手动的更新,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sudo updatedb <span class="comment">## 手动更新locate的文件名数据库</span></span></span><br></pre></td></tr></table></figure>
<p>locate查找也可以使用通配符,一些常用的locate命令参数为</p>
<ul>
<li>-c: 只输出找到的数量</li>
<li>-i: 忽略大小写进行查找</li>
<li>-n n: 至多显示n个输出</li>
<li>-h: 显示帮助</li>
</ul>
<p><a id="orgcffd121"></a></p>
<h2 id="whereis-查找特定程序">whereis: 查找特定程序</h2>
<p>whereis命令主要用于查找程序文件并提供该程序的二进制可执行文件,源代码文件和使用手册页存放的位置.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">whereis find <span class="comment">## 寻找find程序文件以及相关的配套文件</span></span></span><br><span class="line">find: /usr/bin/find /usr/share/man/man1/find.1.gz /usr/share/info/find.info-1.gz /usr/share/info/find.info.gz /usr/share/info/find.info-2.gz</span><br></pre></td></tr></table></figure>
<p>我们可以用-b选项来让whereis只返回该程序的二进制可执行文件,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">whereis -b find <span class="comment">## 寻找find命令的二进制可执行文件</span></span></span><br><span class="line">find: /usr/bin/find</span><br></pre></td></tr></table></figure>
<p>同样,可以用-m选项来让whereis只返回该程序的帮助文件,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">whereis -m find <span class="comment">## 寻找find命令的帮助文件</span></span></span><br><span class="line">find: /usr/share/man/man1/find.1.gz /usr/share/info/find.info-1.gz /usr/share/info/find.info.gz /usr/share/info/find.info-2.gz</span><br></pre></td></tr></table></figure>
<p>如果查找的文件不存在,那会返回一个空字符串,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">find xxx</span></span><br><span class="line">xxx:</span><br></pre></td></tr></table></figure>
<p>事实上,whereis命令查找不到文件,不一定是文件不存在,而是有可能不是在whereis的查找目录中,其实whereis并没有查找全文件系统,仅查找了内置的子目录路径,这在查找精度上是一个缺陷,但对于查找的速度来说是一个显著提升.</p>
<p><a id="org99ede03"></a></p>
<h2 id="用户版本信息查看">用户版本信息查看</h2>
<p>who可以查看当前系统有哪些人登陆使用,以及他们使用的工作台,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"> <span class="built_in">who</span> <span class="comment">## 查看当前使用系统的用户</span></span></span><br></pre></td></tr></table></figure>
<p>whoami用来查看自己是以什么身份进入系统的</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">whoami</span> <span class="comment">## 查看自己进入系统的身份</span></span></span><br></pre></td></tr></table></figure>
<p>上面这两个命令对于PC系统的应用不大,主要是对于服务器系统的查看,因为不同的用户会有不同的权限,因此需要用这个来查看用户身份.</p>
<p>uname是用来显示当前系统的版本信息.-a选项会显示当前的操作系统的所有有用的信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">uname</span> -a <span class="comment">## 显示当前的操作系统的所有有用的信息</span></span></span><br><span class="line">Linux lyd-Lenovo-330S-14IKB 6.5.0-25-generic #25~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Feb 20 16:09:15 UTC 2 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>
<p>如果只需要查看处理器类型,-m选项可以只输出这个结果,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">uname</span> -m</span></span><br><span class="line">x86_64</span><br></pre></td></tr></table></figure>
<h2 id="man-寻求帮助">man: 寻求帮助</h2>
<p>linux可以用man命令接某个命令的名称来获取该命令的帮助文档,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">man find <span class="comment">## 查看find的帮助文档</span></span></span><br></pre></td></tr></table></figure>
<p>man命令显示手册页用的是less程序.对于其上下移动和vim的操作一样,不再赘述.</p>
<h2 id="whatis-和-apropos-获取命令简介">whatis 和 apropos:
获取命令简介</h2>
<p>由于man查找的是帮助文档,因此太过冗长.我们可以用whatis命令快速得到命令的简要介绍,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">whatis find  <span class="comment">## 获取命令的简要介绍</span></span></span><br><span class="line">find (1)             - search for files in a directory hierarchy</span><br></pre></td></tr></table></figure>
<p>但如果我们想要实现某个功能但不知道用什么命令时,我们可以选用apropos来反向查找,如</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">apropos search  <span class="comment">##反向搜索使用的命令</span></span></span><br><span class="line">apropos (1)          - search the manual page names and descriptions</span><br><span class="line">apt-patterns (7)     - Syntax and semantics of apt search patterns</span><br><span class="line">badblocks (8)        - search a device for bad blocks</span><br><span class="line">bsearch (3)          - binary search of a sorted array</span><br><span class="line">bzegrep (1)          - search possibly bzip2 compressed files for a regular e...</span><br><span class="line">bzfgrep (1)          - search possibly bzip2 compressed files for a regular e...</span><br><span class="line">bzgrep (1)           - search possibly bzip2 compressed files for a regular e...</span><br><span class="line">Data::DPath::Context (3pm) - Abstraction for a current context that enables i...</span><br><span class="line">find (1)             - search for files in a directory hierarchy</span><br><span class="line">git-bisect (1)       - Use binary search to find the commit that introduced a...</span><br><span class="line">hsearch (3)          - hash table management</span><br><span class="line">hsearch_r (3)        - hash table management</span><br><span class="line">lfind (3)            - linear search of an array</span><br></pre></td></tr></table></figure>
<p>可以发现apropos实际上是检索命令简介中包含关键词的条目罗列出,让用户选择.</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>SVD-Unitary Transformation and Geometry</title>
    <url>/2024/06/16/SVD-Unitary%20Transformation/</url>
    <content><![CDATA[<p>In the singular value decomposition of the data matrix <span
class="math inline">\(X\)</span>, we have the two unitary matrices <span
class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span>. We have introduced that the unitary
matrices satisfy <span class="math inline">\(U^TU=UU^T=I\)</span> and
<span class="math inline">\(VV^T=V^TV=I\)</span>. Here these identity
matrices have different size.</p>
<span id="more"></span>
<p>Actually, unitary matrices preserve the angle between any two vectors
in the vector space. Since the inverse of the unitary matrix is its own
transpose, it can not only preserve the angles between vectors, but also
preserve the length of the vectors. Essentially, unitary transformation
is a coordinate transformation into a new representation. It just takes
all of those vectors and rotates them into a new representation. It will
not change the length of any vector and angles between any two
vectors.</p>
<p>Here you will find that we just consider the real-valued case.
However, when we consider the data matrix <span
class="math inline">\(X\)</span> is a complex-valued matrix, the
singular vectors <span class="math inline">\(U\)</span> and <span
class="math inline">\(V\)</span> are complex-valued matrices. Compared
with the real-valued matrix, the most important difference is that the
transpose of a complex-valued matrix is actually a conjugate
transpose.</p>
<p>To sum up, we give the mathematical form of unitary transformation
here. For any two vectors <span class="math inline">\(x\)</span> and
<span class="math inline">\(y\)</span> in vector space, then we have
<span class="math display">\[
(x,y)=(Ux,Uy)
\]</span> where <span class="math inline">\((\cdot,\cdot)\)</span> is
the inner product of two vectors and the operator <span
class="math inline">\(U\)</span> means the unitary transformation, which
in a unitary matrix in discrete case. Based on this mathematical form,
we realize that the inner product of <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span> is unchanged when we transform <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span> by unitary transformation.</p>
<p>Since we have introduced the unitary transform in the vector space,
we interpret singular vector decomposition geometrically. Here we
consider the size of data matrix is <span class="math inline">\(n\times
m\)</span>. Now we consider any vector <span
class="math inline">\(\vec{v}\)</span> in space <span
class="math inline">\(\mathbb{R}^m\)</span> and vector <span
class="math inline">\(\vec{v}\)</span> is palced on the unit sphere. As
we all know, the matrix is a linear transformation about the vector.
Therefore, we consider the transformed vector <span
class="math inline">\(X\vec{v}\)</span>, which is placed on the
ellipsoid in the vector space <span
class="math inline">\(\mathbb{R}^n\)</span>. Here the gemoetric
interpretation of the singular value decomposition can be given. The
length of these principal axes are specially given by the singular
values of the data matrix <span class="math inline">\(X\)</span>. The
orientation of this ellipsoid is somehow given by these left singular
vectors in <span class="math inline">\(U\)</span>. Like the
interpretation of linear transformation, <span
class="math inline">\(X\)</span> multiplies the vector on the left side
or on the right side, which corresponds to a change to a rotation of
space. Finally, <span class="math inline">\(X\)</span> is not a square
matrix, so the rotation of the corresponding space is actually a change
in dimensionality.</p>
]]></content>
      <categories>
        <category>Reduced Order Model</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux-文件目录管理</title>
    <url>/2024/04/16/linux-2/</url>
    <content><![CDATA[<h1 id="文件目录管理">文件目录管理</h1>
<p>下表罗列了Linux文件系统中主要目录的内容</p>
<div data-align="center">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col  class="org-center" />
</colgroup>
<colgroup>
<col  class="org-center" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-center">
目 录
</th>
<th scope="col" class="org-center">
内 容
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-center">
/bin
</td>
<td class="org-center">
构建最小系统所需要的命令(最常用的命令)
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/boot
</td>
<td class="org-center">
内核和启动文件
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/dev
</td>
<td class="org-center">
各种设备文件
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/etc
</td>
<td ·class="org-center">系统软件的启动和配置文件</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/home
</td>
<td class="org-center">
用户的主目录(用户所有数据,极其重要)
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/lib
</td>
<td class="org-center">
C编译器的库
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/media
</td>
<td class="org-center">
可移动介质的安装点
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/opt
</td>
<td class="org-center">
可选的应用软件包(用deb包安装的部分软件在这)
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/proc
</td>
<td class="org-center">
进程的映像
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/root
</td>
<td class="org-center">
根用户root的主目录
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/sbin
</td>
<td class="org-center">
和系统操作有关的命令
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/tmp
</td>
<td class="org-center">
临时文件存放点
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/usr
</td>
<td class="org-center">
非系统的程序和命令(apt安装的位置)
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-center">
/var
</td>
<td class="org-center">
系统专用的数据和配置文件
</td>
</tr>
</tbody>
</table>
</div>
<p><a id="org8d61c7d"></a></p>
<span id="more"></span>
<h2 id="mkdir-建立目录">mkdir: 建立目录</h2>
<p>mkdir命令可以一次建立一个或几个目录.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> document picture <span class="comment">## 新建两个目录</span></span></span><br></pre></td></tr></table></figure>
<p>这样的创建是先利用cd到你想创建的位置,再创建目录.用户也可以用绝对路径来新建目录,这样的话不需要用cd来跳转到创建的位置,在任意位置用mkdir加绝对路径即可创建目录,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> ~/picture/temp <span class="comment">## 在主目录的picture目录下创建temp子目录</span></span></span><br></pre></td></tr></table></figure>
<p>这个命令合法是因为picture目录存在主目录下,如果用户试图在一个不存在的目录下创建新子目录,那命令会报错.为了避免这个报错,我们可以用-p选项来创建目录.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> -p /home/username <span class="comment">## 在home目录下创建username目录,如果他存在那就不去创建</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> -p ~/test1/test2 <span class="comment">## 如果主目录下没有test1,那么不会报错,而是先创建test1再在其中创建test2</span></span></span><br></pre></td></tr></table></figure>
<p><a id="orgfef9599"></a></p>
<h2 id="touch-建立一个空文件">touch: 建立一个空文件</h2>
<p>touch后面接文件名作为参数,可以在当前目录创建一个或多个新文件,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">touch</span> hello <span class="comment">## 在当前目录创建文件名为hello的文件</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">touch</span> <span class="string">&#x27;hello world&#x27;</span> <span class="comment">## 在当前目录创建文件名为 &#x27;hello world&#x27; 的文件</span></span></span><br></pre></td></tr></table></figure>
<p>如果我们在上面的'hello
world'中不加引号,那touch会认为创建hello和world两个文件.用touch命令创建的文件是空文件,其内不包含任何内容.</p>
<p>touch除了创建空文件的作用,其更重要的用途是更新一个文件的建立日期和时间.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -l init.el~ <span class="comment">## 查看文件的时间属性</span></span></span><br><span class="line">-rw-r--r-- 1 lyd lyd 997  2月  5 21:54 init.el~ ## 原来文件时间属性是2024/2/5-21:54</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">touch</span> init.el~ <span class="comment">## 修改文件的时间属性</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -l init.el~ <span class="comment">## 查看改过的文件时间属性</span></span></span><br><span class="line">-rw-r--r-- 1 lyd lyd 997  3月 20 20:52 init.el~ ## 改过以后文件时间变成了2024/3/20-20:52</span><br></pre></td></tr></table></figure>
<p>touch命令在自动备份和整理文件文件时非常有用.</p>
<p><a id="org0ba94bd"></a></p>
<h2 id="mv-移动和重命名">mv: 移动和重命名</h2>
<p>mv其实是move的缩写形式.这个命令可以用来移动文件或目录到另一个路径.这里移动到的文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mv</span> hello bin/ <span class="comment">## 把hello文件移动到bin目录下</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mv</span> Photos/ 桌面/ <span class="comment">## 把Photos目录移动到桌面目录下</span></span></span><br></pre></td></tr></table></figure>
<p>由于执行mv命令的时候并不会有任何信息显示,那么如果目标目录中有一个同名文件,不加任何选项的mv命令会强制替换文件.这一行为极其危险,容易强制替换掉一些关键文件.为了避免这种情况出现,我们可以用-i选项来控制覆盖文件与否,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mv</span> -i hello <span class="built_in">test</span>/ <span class="comment">## 出现同名文件询问是否覆盖</span></span></span><br></pre></td></tr></table></figure>
<p>其会先询问用户是否覆盖旧文件,输入y表示直接覆盖,输入n表示取消移动操作.</p>
<p>另外一个避免强制覆盖的选项是-b,其与-i的询问不同,如果出现同名文件,那么他会在转移之前将目标目录中的同名文件的文件名后面加一个~,形成一个备份,从而避免文件覆盖情况的发生.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mv</span> -b hello <span class="built_in">test</span>/ <span class="comment">## 出现同名文件则对其备份</span></span></span><br></pre></td></tr></table></figure>
<p>Linux中不存在重命名的操作,我们可以认为重命名只不过是在同一级目录下的移动而已.因此我们可以用mv命令来实现对文件或目录的重命名.
如</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mv</span> hello world <span class="comment">## 在同级目录下进行重命名</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mv</span> /home/lyd/hello /home/lyd/learning/world <span class="comment">## 移动到其他目录并且重命名</span></span></span><br></pre></td></tr></table></figure>
<p>使用mv命令可能出现的一些简单情况,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mv</span> file1 file2 <span class="comment">## 移动file1到file2</span></span></span><br></pre></td></tr></table></figure>
<p>如果file2存在,他的内容会被file1的内容覆盖.如果file2不存在,那就会创建file2.但这两种情况下,file1都不会存在.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mv</span> file1 file2 dir1/ <span class="comment">## 移动file1和file2到目录dir1中</span></span></span><br></pre></td></tr></table></figure>
<p>这个命令的前提是目录dir1必须已经存在.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mv</span> dir1/ dir2/ <span class="comment">## 移动目录dir1到目录dir2</span></span></span><br></pre></td></tr></table></figure>
<p>如果目录dir2不存在,创建目录dir2,并且移动目录dir1的内容到目录dir2中,同时删除目录dir1.如果dir2存在,移动目录dir1及他的内容到目录dir2.</p>
<p><a id="org8b9b032"></a></p>
<h2 id="cp-复制文件和目录">cp: 复制文件和目录</h2>
<p>cp命令用来复制文件和目录.下面的指令用来将test.php复制到test目录下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cp</span> test.php <span class="built_in">test</span>/ <span class="comment">## 复制文件到指定路径</span></span></span><br></pre></td></tr></table></figure>
<p>和mv命令一样,cp在默认情况下也会强制覆盖目标目录中的同名文件.同样我们可以用-i选项来对这种覆盖文件的情况进行提示,我们也可以用-b选项来避免询问,通过对同名文件进行一个修改命名再进行复制.这两个选项的使用和mv命令的一样,因此不再赘述.</p>
<p>如果我们希望复制一个目录到另一个目录下,而直接用cp命令进行复制,则会有如下结果</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cp</span> learning/ Downloads/ <span class="comment">## 报错的复制目录</span></span></span><br><span class="line">cp: 未指定 -r；略过目录 &#x27;learning/&#x27;</span><br></pre></td></tr></table></figure>
<p>这是因为cp命令在执行复制任务的时候会自动跳过目录.如果我们一定要连同目录及其内部的文件一块复制到另一个目录,我们需要用-r选项来实现这一操作.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cp</span> -r learning/ Downloads/ <span class="comment">##  复制目录到指定目录</span></span></span><br></pre></td></tr></table></figure>
<p>使用cp可能出现的一些简单情况,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cp</span> file1 file2 <span class="comment">## 复制文件file1内容到文件file2</span></span></span><br></pre></td></tr></table></figure>
<p>如果file2存在,那么file2的内容会被file1的内容覆盖.如果file2不存在,那就会创建一个file2.但和mv删除file1不同,cp命令会保留file1.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cp</span> file1 file2 dir1/ <span class="comment">## 复制文件file1和file2到目录dir1</span></span></span><br></pre></td></tr></table></figure>
<p>但这里必须要求目录dir1存在.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cp</span> dir1/* dir2/ <span class="comment">## 用通配符移动文件</span></span></span><br></pre></td></tr></table></figure>
<p>与-r选项不同,用通配符来批量移动dir1中的文件,要求dir2存在并且移动只会复制dir1中的文件和子目录,不会在dir2中复制dir1这个目录.</p>
<p><a id="org9b7d339"></a></p>
<h2 id="rmdir-和-rm-删除目录和文件">rmdir 和 rm: 删除目录和文件</h2>
<p>rmdir命令可以用来删除目录.其只需要在rmdir命令后面接要删除的文件名即可.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> remove <span class="comment">##创建一个空目录</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">rmdir</span> remove <span class="comment">##删除指定的空目录</span></span></span><br></pre></td></tr></table></figure>
<p>但rmdir只能用于删除空目录,一旦试图用其删除非空目录,那么会有如下报错</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">rmdir</span> learning</span></span><br><span class="line">rmdir: 删除 &#x27;learning&#x27; 失败: 目录非空</span><br></pre></td></tr></table></figure>
<p>因此,如果我们想要用rmdir删除非空目录,我们需要先删除该目录下的所有子目录及文件.</p>
<p>对于文件的删除可以用rm命令,其实rm命令也可以针对目录,此处rm命令删除的目录是允许出现非空目录,因此rm的应用远比rmdir更为广泛.rm命令可以一次性删除单个或多个文件,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">rm</span> <span class="built_in">test</span>/* <span class="comment">## 删除test目录下的所有文件</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">rm</span> halo <span class="comment">## 删除halo文件</span></span></span><br></pre></td></tr></table></figure>
<p>和前面的mv和cp命令一样,rm不会在运行过程中出现任何提示,利用rm命令删除的文件并不会被移入回收站,而是直接从系统中删除.因此为了安全的使用rm,我们一般用-i选项来在删除过程之前给出提示等待用户确认,与前面的mv和cp一样.但有个例外,如果我们删除只读文件,那么即便我们不使用-i选项,rm命令也会对这一操作进行询问</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">rm</span> <span class="built_in">test</span> <span class="comment">## 删除test只读文件</span></span></span><br><span class="line">rm: 是否删除有写保护的普通空文件 &#x27;test&#x27;？ </span><br></pre></td></tr></table></figure>
<p>如果对这个交互操作默认是y,且不想多次交互,我们可以用-f选项来跳过这些交互操作,rm会自动对这些交互操作回答y.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">rm</span> -f <span class="built_in">test</span> <span class="comment">## -f选项跳过交互操作</span></span> </span><br></pre></td></tr></table></figure>
<p>我们可以用带-r选项的rm命令会递归的删除指定目录下所有文件和子目录.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">rm</span> -r Photos/ <span class="comment">## 递归删除Photos目录</span></span></span><br></pre></td></tr></table></figure>
<p>关键在于使用rm
-rf命令一定要注意评估删除的后果,不然可能会破坏系统的稳定性,故慎用rm
-r命令.</p>
<p><a id="org1f6941e"></a></p>
<h2 id="linux文件目录权限">Linux文件目录权限</h2>
<p>Linux的文件目录权限针对三类人群:文件所有者(属主),文件属组用户,其他人.关键在于其他人的多样性,可能会对文件目录内容进行修改,从而有机会造成不可预料的信息损坏.root用户虽然应该归属于上面的其他人,但root用户显然具有对系统任意文件的查看,修改,执行权利,因为root用户拥有控制一台计算机的所有权限.</p>
<p>文件所有者一般是文件的创建者.但这并不是绝对的,root用户可以修改一个文件的属主用户.换言之,在某个用户在linux系统创建了某个文件,此时文件所有者(属主)自动是文件创建者,但后续过程中root用户可以将文件所有者进行转让,这个转让过程也仅能用root用户进行.</p>
<p>当然文件的权限也会被给予一个用户组,我们称这个用户组为文件的属组.组是一群用户组成的一个集合.文件属组中的用户按照设置对文件享有特定的权限.通常而言,当一个用户创建了一个文件,那么该文件的属主就是这个用户,而文件的属组则是有且仅有该用户的用户组.当然我们也可以设置这个文件属组是一个不包括文件属主的用户组.如果文件属主执行文件操作的时候,系统只会关注于文件属主的权限,而文件属组的权限并不会对文件属主的权限造成影响.</p>
<p>其他人则是不包括文件所有者,文件属组用户和root用户以外的其他用户.通常其他人的权限十分低,甚至于无法对文件有任何权限.</p>
<p>可以用来赋予用户的文件和目录的权限为读取(r),写入(w)和执行(x).对于文件而言,读取权限意味着可以打开并查看文件的内容,写入权限控制着对文件的修改权限,至于是否能呢更狗删除和重命名一个文件则是由其父目录的权限设置所控制.要让一个文件可执行,必须设置其执行权限.可执行文件有两类,一类是可以直接由CPU执行的二进制代码,另一类则是Shell脚本程序.</p>
<p>对目录而言,目录的执行权限其实是控制用户是否能够进入该目录,因此目录的执行权限其实上是目录的最基本的权限.而读取权限负责确定能否列出该目录的内容,写入权限则控制在目录中创建,删除和重命名文件.</p>
<p><a id="org3d640d7"></a></p>
<h2 id="ls--l-查看文件类型">ls -l: 查看文件类型</h2>
<p>前面介绍ls命令时,提到了ls
-l来查看文件的属性,此处进一步解释展示的文件属性的参数</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -l  <span class="comment">## 列出所有文件和目录以及其对应的属性</span></span></span><br><span class="line">  总计 152</span><br><span class="line">  drwxr-xr-x  2 lyd  lyd   4096  2月  5 09:49  公共</span><br></pre></td></tr></table></figure>
<ul>
<li>第一个字段中的第一个字符表示文件类型,这个例子是d,其表示这是一个目录,具体的字符和文件类型的对应下表所示,</li>
</ul>
<div data-align="center">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<colgroup>
<col  class="org-left" />
</colgroup>
<colgroup>
<col  class="org-left" />
</colgroup>
<colgroup>
<col  class="org-left" />
</colgroup>
<colgroup>
<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">
文件类型
</th>
<th scope="col" class="org-left">
符号
</th>
<th scope="col" class="org-left">
文件类型
</th>
<th scope="col" class="org-left">
符号
</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">
普通文件
</td>
<td class="org-left">
-
</td>
<td class="org-left">
本地域套接口
</td>
<td class="org-left">
s
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">
目录
</td>
<td class="org-left">
d
</td>
<td class="org-left">
有名管道
</td>
<td class="org-left">
p
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">
字符设备文件
</td>
<td class="org-left">
c
</td>
<td class="org-left">
符号链接
</td>
<td class="org-left">
l
</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">
块设备文件
</td>
<td class="org-left">
b
</td>
<td class="org-left">
 
</td>
<td class="org-left">
 
</td>
</tr>
</tbody>
</table>
</div>
<p>其中具体的含义大概解释如下,Linux一般用设备文件来表示一个特定的硬件设备.Linux中有两类设备文件:字符设备文件和块设备文件.其中字符设备指的是能从他那读取成字符序列的设备,如磁带和串行设备;块设备则是指用来存储数据并对其各部分内容提供同等访问权的设备,如磁盘.一般我们可以称字符设备为顺序访问设备,块设备则为随机访问设备.这是因为块设备可以从硬盘的任何随机位置获取数据,而字符设备则必须按照数据发送的顺序从串行线路上获得.但是系统中存在设备文件,并不代表着他一定链接着相应的硬件设备,而是表示其具有处理对应硬件设备的能力.</p>
<p>关于本地域套接口和有名管道这两个文件涉及到了进程间通信,日常使用并不常见.</p>
<p>符号链接会在后续的ln里介绍,类似于windows系统的快捷方式.</p>
<ul>
<li>接下来的rwxr-xr-x则是三组权限位,断句如下rwx,r-x,r-x,分别代表着属主,属组,其他用户的权限.r表示可读取,w表示可写入,x表示可执行,如果某个权限被禁用,那么其会用短横线-取代.</li>
<li>紧跟着三组权限位的数字表示文件的链接树木.此处是2.表示该目录存在两个链接,关于链接后续会给出介绍.</li>
<li>后面第三个字段和第四个字段表示文件的属主和属组.</li>
</ul>
<p>ls
-l可以用来查看某个特定文件的属性,但是如果我们需要查看目录的属性则需要用ls
-ld命令来查看.</p>
<p><a id="orgaef0396"></a></p>
<h2 id="chown-和-chgrp-改变文件所有权">chown 和 chgrp:
改变文件所有权</h2>
<p>chown命令用于改变文件的所有权.chown命令的基本语法为:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">chown [OPTION] ... [OWNER][:[GROUP]] FILE... ## chown语法基本结构</span><br></pre></td></tr></table></figure>
<p>这条命令可以将文件FILE的属主更改为OWNER,属组更改为GROUP.
下面命令给出了一个示例,其将文件的属主更改为LYD,同时将文件属组更改为root组,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sudo <span class="built_in">chown</span> LYD:root file <span class="comment">## 修改文件file的属主为LYD,同时修改属组为root组</span></span></span><br></pre></td></tr></table></figure>
<p>如果我们只需要修改文件的属主,那么我们只需要输入OWNER,不需要输入:GROUP.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sudo <span class="built_in">chown</span> LYD file <span class="comment">## 修改文件file的属主为LYD,但不对属组进行修改</span></span></span><br></pre></td></tr></table></figure>
<p>同样,我们只修改文件属组的话,只需要输入:GROUP,但这里要注意的是冒号:并不是可省略的,省略参数OWNER即可,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sudo <span class="built_in">chown</span> :root file <span class="comment">## 修改文件file的属组为root,但不对属主进行操作</span></span></span><br></pre></td></tr></table></figure>
<p>我们在前面用chown修改单个文件的属组和属组.实际上我们可以用chown来修改目录文件的属主和属组,但是显然如果我们只修改目录文件的属主和属组的话,并没有什么意义.因此我们可以利用chown命令的-R选项,用于改变一个目录及其下所有文件(包括子目录)的所有权设置.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sudo <span class="built_in">chown</span> -R LYD:root iso/ <span class="comment">## 递归修改iso文件及其下的所有文件(包括子目录)的所有权</span></span></span><br></pre></td></tr></table></figure>
<p>由于chown可以更改文件的属主和属组属性,但实际上Linux系统提供了另一个命令chgrp,其专门用来修改文件的属组.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chgrp</span> nogroup days <span class="comment">## 更改文件属组</span></span></span><br></pre></td></tr></table></figure>
<p>同样和chown一样,我们也可以用-R选项来递归的更改目录及其下所有文件和子目录的属组.但是这里chgrp和chown有一个关键区别在于,chown修改文件属组需要用root用户权限,然而对于chgrp,其只需要修改的用户在文件的属组就可以修改文件的属组.</p>
<p><a id="orgad5e953"></a></p>
<h2 id="chmod-改变文件权限">chmod: 改变文件权限</h2>
<p>chmod用来改变一个文件的权限.其修改删除权限的模式是用户组+/-权限的表达式.具体而言,用户组分为文件属主(u),文件属组(g),其他人(o),以及所有人(a).权限则表示读取(r),写入(w)和执行(x).</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> u+x days <span class="comment">## 为days文件属主提供文件执行权限</span></span></span><br></pre></td></tr></table></figure>
<p>chmod可以用a来同时指定包括文件属主,文件属组和其他人的三类人,我们可以利用以此同时给全部人员增加或删除权限.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> a+w days <span class="comment">## 为days文件的所有人同时提供文件的写入权限</span></span></span><br></pre></td></tr></table></figure>
<p>如果我们需要同时给三类人增删权限,除了用a来表示所有人以外,实际上我们可以不加任何用户组标记,直接用+/-权限的形式,同样可以给全部人员增加或删除权限</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> +rw file <span class="comment">## 为文件file的所有用户增加读取权限和写入权限</span></span></span><br></pre></td></tr></table></figure>
<p>这个命令中我们通过+rw同时给所有用户添加了读取权限和写入权限,这也就是说我们可以在一个命令中同时给用户添加或删除一个或多个权限.进一步我们可以利用逗号来分隔不同的两个修改权限操作</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> u+r,go-w file <span class="comment">## 为文件file的属主添加读取权限,属组和其他人则删除写入权限</span></span></span><br></pre></td></tr></table></figure>
<p>在这里,我们用了一个go来同时对属组和其他人进行修改权限操作,因此实际上我们可以看出来前面用的所有人标识符a和ugo等价.上面我们用逗号分隔符来分隔对不同用户的修改权限操作,实际上我们也可以用逗号分隔符来分隔对同一用户做的不同修改操作.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> u+r,u-w file <span class="comment">## 为文件file的属主添加读取权限的同时,删除文件属主的写入权限</span></span></span><br></pre></td></tr></table></figure>
<p>如果用上面的添加或删除操作修改用户的权限来达到预期会极其复杂,中间也可能需要多次用ls
-l查看文件属性.因此chmod还存在一个规则:用户组=权限来直接设置文件权限.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> ug=rw,o=r file <span class="comment">## 文件的属主和属组权限更改为读取权限和写入权限,而其他人的权限更改为读取权限</span></span></span><br></pre></td></tr></table></figure>
<p>如果文件本身就有一个默认设定的权限,=这种规则会先重置需要修改的用户组的默认规则,然后按照=设定的权限重新赋予.虽然我们举了对所有用户用=修改权限的例子,但实际上,=规则是可以只对其中部分进行修改的.</p>
<p>最后最为常用的是用户组1=用户组2,其的作用在于将用户组1的权限和用户组2的权限设置成一样.明确来说,他的作用是将用户组2的权限覆盖用户组1的权限.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> o=u file <span class="comment">## 将文件file的其他人权限设置成文件属主的权限</span></span></span><br></pre></td></tr></table></figure>
<p>最为关键的一点是只有文件的属主和root用户才可以修改文件的权限.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> u+r file1 file2 <span class="comment">## 为文件file1和file2的属主增加读取权限</span></span></span><br></pre></td></tr></table></figure>
<p>但这里要注意这样的修改要么在root用户下进行,要么两个文件的属主是同一个用户</p>
<p>虽然我们前面已经将权限用三个字母来表示,但这对批量修改文件的权限十分麻烦,因此我们可以用八进制来代表设置权限.由于对于任意的权限而言,其只存在两种状态:设置(1)和不设置(0).所以我们可以得到任意一组权限设置的八进制表示,如'rwx'的二进制为111,八进制为7;'r-x'的二进制为101,八进制为5.这样我们就可以将完整的9位权限位用3个八进制数来表示,例如'rwxr-x-w-'分别对应的三个三位二进制为111/101/010,其对应的三个八进制数752.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> 752 file <span class="comment">## 将文件权限设置为 rwx/ r-x/ -w-</span></span> </span><br></pre></td></tr></table></figure>
<p><a id="orgece5a30"></a></p>
<h2 id="ln-建立链接">ln: 建立链接</h2>
<p>ln命令是用来为某个文件在另一个位置创建一个同步的链接.链接主要分为符号链接(软链接)和硬链接两大类.无论硬链接还是软链接,他们都不会重新复制原来的文件,他们只会占用非常少量的磁盘空间.</p>
<p>符号链接,也被称为软链接,其用ln
-s命令来创建生成.软链接具有如下的特点,</p>
<ol type="1">
<li><p>软链接以路径的形式存在,类似于windows系统的快捷方式</p></li>
<li><p>软链接可以跨越不同的文件系统</p></li>
<li><p>软链接可以对一个不存在的文件名进行链接</p></li>
<li><p>软链接还可以对目录文件进行链接7</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ln</span> -s TARGET LINKNAME <span class="comment">## 为文件TARGET创建了一个别名LINKNAME</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -l LINKNAME <span class="comment">## 查看LINK_NAME的属性</span></span></span><br><span class="line">lrwxrwxrwx 1 lyd lyd 6  4月 15 11:31 LINK_NAME -&gt; TARGET</span><br></pre></td></tr></table></figure></li>
</ol>
<p>从这里我们可以看出这个文件被指向TARGET文件,因此访问LINK_NAME就相当于访问TARGET.需要注意的是,这里的LINKNAME只是提供了访问TARGET的一个路径,因此我们删除LINKNAME并不会影响TARGET的正常运行,反之我们删除TARGET,虽然不会同时删除LINKNAME,但是其存在已经没有任何意义了.</p>
<p>同样符号链接还可用于目录,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ln</span> -s learning/ LINK <span class="comment">## 为目录learning创建一个别名LINK</span></span></span><br></pre></td></tr></table></figure>
<p>此外,对于软链接而言,其还存在删除和修改操作如下,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">rm</span> -rf LINKNAME <span class="comment">## 删除软链接</span></span></span><br></pre></td></tr></table></figure>
<p>切记不要在软链接后面加/,不然如果软链接对应的是目录文件,可能会直接把目录文件里的内容全部删除.</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ln</span> -snf NEWTARGET LINK <span class="comment">## 将LINK链接的文件从原文件更改到新文件NEWTARGET</span></span></span><br></pre></td></tr></table></figure>
<p>这里的修改操作,不止允许同一类型之间文件的链接相互更改,还可以把普通文件的链接文件链接到目录文件,亦或反之.</p>
<p>Linux中的另一种链接称为硬链接.其将两个独立的文件联系在一起,因此硬链接和软链接的本质区别是,硬链接是直接引用,而软链接是通过名称进行引用.</p>
<p>硬链接具有的特性如下,</p>
<ol type="1">
<li>以文件副本的形式存在,但不占用实际空间</li>
<li>不允许给目录创建硬链接</li>
<li>硬链接不能跨越文件系统创建,只能在同一个文件系统里创建</li>
</ol>
<p>硬链接和软链接不同,硬链接用不带-s选项的ln命令直接创建,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ln</span> world worldlink <span class="comment">## 创建world的硬链接文件worldlink</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> -l worldlink <span class="comment">## 查看硬链接文件的属性</span></span></span><br><span class="line">-r----xrw- 2 lyd lyd 4  4月  8 07:39 world_link</span><br></pre></td></tr></table></figure>
<p>根据上述查看的文件属性,我们得知这两个文件是独立的,但是会被联系在一起而已.</p>
<p>无论软链接还是硬链接,只是对目标文件的一个访问渠道而已,因此我们在任何一类链接文件上修改都会导致目标文件的变化.</p>
<p><a id="org898ec68"></a></p>
<h2 id="输入输出重定向和管道">输入输出重定向和管道</h2>
<p>重定向和管道操作是Linux的Shell命令里的一种高级特性,其允许用户修改程序获取输入或者生成输出的位置.</p>
<p>在默认情况下程序输出结果的位置称之为标准输出.通常来说,标准输出位都是显示器.输出重定向的作用是将程序的输出转移到另一个地方去,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> &gt; ~/ls_out <span class="comment">## 将输出结果定向到ls_out文件</span></span></span><br></pre></td></tr></table></figure>
<p>如果指定文件不存在,那么这个命令会创建这个文件,反之如果这个文件是存在的,那么他会直接覆盖文件原有的内容.如果我们希望能够保留原来文件中的内容,那么我们可以用输出重定向符号&gt;&gt;,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">date</span> &gt; date_out <span class="comment">## 将date命令的输出重定向到date_out文件</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> date_out <span class="comment">## 查看date_out文件</span></span></span><br><span class="line">2024年 04月 16日 星期二 21:15:39 CST</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">uname</span> -r &gt;&gt; date_out <span class="comment">## 将输出结果重定向至date_out文件,并且仅在末尾输出,不覆盖原文件</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> date_out <span class="comment">## 再次查看date_out文件</span></span></span><br><span class="line">2024年 04月 16日 星期二 21:15:39 CST</span><br><span class="line">6.5.0-26-generic</span><br></pre></td></tr></table></figure>
<p>类似于标准输出的定义,程序默认情况下接受输入的地方称之为标准输入.通常来说,标准输入指向键盘.如果使用不带任何参数的cat命令,cat不会执行而是等待从标准输入来获取数据,用户可以输入一行会直接输出在屏幕上,直到Ctrl+D用来给cat命令输入一个文件结束符.
在此,我们罗列一下基于shell的常用快捷键及其含义</p>
<ol type="1">
<li>Ctrl+c 终止当前正在执行的程序</li>
<li>Ctrl+z
中断当前进程,但不是终止进程,只是将进程挂起,我们可以用fg命令来重新调用进程(后续介绍)</li>
<li>Ctrl+l 清空屏幕</li>
<li>Ctrl+d 输入特殊的二进制值,表示EOF,作为文件的结束</li>
</ol>
<p>通过使用输入重定向符号&lt;可以让程序从一个文件中获取输入,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> &lt; file <span class="comment">## 输出file里的内容</span></span></span><br></pre></td></tr></table></figure>
<p>显然cat可以直接接受参数来显示文件内容,因此输入重定向对这类命令并没有优势,故其使用比较少.</p>
<p>类似,其也存在另一种重定向符号&lt;&lt;,被称之为立即文档.立即文档告诉shell从键盘接受输入,并传递给程序,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF ## 立即文档作为输入,EOF</span>用来输入文档终止</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">Hello</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">,</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">world</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">!</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">EOF</span></span><br><span class="line">Hello</span><br><span class="line">,</span><br><span class="line">world</span><br><span class="line">!</span><br></pre></td></tr></table></figure>
<p>这里的EOF只是代用常见的终止缩写,实际上是可以随意使用的,只需要保证输入的内容中不会出终止代号即可.进一步,上面提到的两个重定向符号是可以在同一个命令中出现的.</p>
<p>管道符号|的出现则进一步使输出重定向的功能变灵活.通过一条竖线,将一个命令的输出作为下一个命令的输入,</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">ls</span> | grep ld <span class="comment">##罗列文件列表以后查找文件名包含ld的文件</span></span></span><br><span class="line">world</span><br><span class="line">worldlink</span><br></pre></td></tr></table></figure>
<p>当然管道可以继续叠加使用,虽然会十分复杂但是运行起来会很高效.</p>
<p><a id="org86fc6aa"></a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Computer Science</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
</search>
